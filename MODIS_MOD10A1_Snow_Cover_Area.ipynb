{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2022/blob/main/MODIS_MOD10A1_Snow_Cover_Area.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng_wcRuL252M"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NbFxO-hu1Tx"
      },
      "source": [
        "## **RUNNING CODE NDSI *MOD10A1.061* DAILY**  \n",
        "## ðŸŒ¨ï¸â„ï¸ðŸ”ï¸  **SNOW COVER AREA**  ðŸ”ï¸â„ï¸ðŸŒ¨ï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre requisites please run this section >>>>"
      ],
      "metadata": {
        "id": "E8YMNHt3soFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1oyqXeHZgaTOjLod-Vqz-VAzS88O13JDr\n",
        "!unzip /content/beasBasinShapeFile.zip -d /content\n"
      ],
      "metadata": {
        "id": "79zaMlOGsmAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Data Here"
      ],
      "metadata": {
        "id": "PmqyTwzIyPI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD10A1v6daily/'\n",
        "fileExt = r'.hdf'\n",
        "temp_dir = r'/content/'\n",
        "imgs_list = [f for f in os.listdir(image_dir) if f.endswith(fileExt)]\n",
        "imgs_list.sort()\n",
        "imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n",
        "#creating a loop to run for all files in the directory having extension \".hdf\"\n",
        "for name, image in enumerate(imgs_path):\n",
        "  print('ok')\n",
        "  print(name)\n",
        "  print(image)\n",
        "  path = imgs_list[name]\n",
        "  #converting to sinusodial coordinate system to wgs 84 utm 43\n",
        "  #import pymodis\n",
        "  subset = [0,0,0,1,0,0,0]\n",
        "  pymodis.convertmodis_gdal.convertModisGDAL( image_dir + path, temp_dir + path[:-4], subset, 2400,outformat=\"GTiff\",epsg=32643).run()\n",
        "  \n",
        "  #cutting into Area Of interest using shape file\n",
        "  !gdalwarp \\\n",
        "  -ot Float32 \\\n",
        "  -cutline /content/beasBasinShapeFile.shp  -crop_to_cutline \\\n",
        "  {temp_dir}{path[:-4]}\"_ndsi.tif\" \\\n",
        "  {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\"\n",
        "\n",
        "  !rm -r {temp_dir}{path[:-4]}\"_ndsi.tif\"\n",
        "  #deleting file\n",
        "\n",
        "  #creating file NDSI>=0.4\n",
        "  !gdal_calc.py \\\n",
        "  --overwrite \\\n",
        "  --type=Float32 \\\n",
        "  -A {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\" \\\n",
        "  --A_band 1 \\\n",
        "  --outfile={temp_dir}{path[:-4]}\"_result.tif\" \\\n",
        "  --calc=\"A.astype(float)>=400\"\n",
        "\n",
        "  !rm -r {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\"\n",
        "  #deleting file\n",
        "\n",
        "  #counting Snow and Non-Snowpixels  \n",
        "  #import gdalnumeric\n",
        "  raster_file = gdalnumeric.LoadFile(temp_dir + path[:-4]+\"_result.tif\")\n",
        "  pixel_count_snow = (raster_file ==0).sum()\n",
        "  pixel_count_notsnow = (raster_file ==1).sum()\n",
        "  print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)\n",
        "\n",
        "  #creating constant for multiplication at pixel size with count of pixels\n",
        "  m1 = !gdalinfo -json {temp_dir}{path[:-4]}\"_result.tif\" | jq -r .geoTransform \n",
        "  coonstant_c = int(m1[2][9:9+4])*int(m1[6][9+1:9+4+1])/1000000\n",
        "\n",
        "  !rm -r {temp_dir}{path[:-4]}\"_result.tif\"\n",
        "  #deleting file\n",
        "\n",
        "  #combining name of the  file , SnowCoverArea , NonSnowCoverArea to a text file format\n",
        "  lines = str(path[8:8+8] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow))\n",
        "  with open(temp_dir+\"out.txt\",\"a+\", encoding=\"utf-8\") as f:\n",
        "    f.writelines('\\n'+ lines)\n",
        "    output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "ioDVU1ltWkZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-xNwbraGnKj"
      },
      "source": [
        "## DOWNLOADING MOD10A.061 daily"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SET FILE LOCATIONS AND CREATIONS"
      ],
      "metadata": {
        "id": "OGzGYkPlL_sa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWBdIR_BG4eC"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/OUT/data/MOD10A1v6daily\n",
        "!ls /content/drive/MyDrive/OUT/\n",
        "%cd /content/drive/MyDrive/OUT/data/MOD10A1v6daily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OjcfAthIdSi"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/OUT/data/MOD10A1v6daily"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEIFN0qOLXui"
      },
      "source": [
        "just change >>>\n",
        "```\n",
        "time_start = '2018-03-01T00:00:00Z'\n",
        "time_end = '2022-08-05T17:40:42Z'\n",
        "bounding_box = '75.65,31,78.69,32.74'\n",
        "```\n",
        "as per requirement IF download gets interrupted then check in google drive the\n",
        "```\n",
        "MOD10A1.A2018108.h24v05.061.2021324032344.hdf\n",
        "```\n",
        "> `A2018108`  is \"2018\" year and \"108\" th day of the year i.e. \"2018-04-18\" << that means it have successfully downloaded tillthen now change \n",
        ">`time_start` to `2018-04-18T00:00:00Z `\n",
        ">`time_end` to `2022-08-05T17:40:42Z`\n",
        ">>even the bounding box is updatable to user required co-ordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDCH3QPNGrDU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title DOWNLOADING MOD10A1.061 DAILY DATA >>> { vertical-output: true }\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import base64\n",
        "import getopt\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import netrc\n",
        "import os.path\n",
        "import ssl\n",
        "import sys\n",
        "import time\n",
        "from getpass import getpass\n",
        "\n",
        "try:\n",
        "    from urllib.parse import urlparse\n",
        "    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n",
        "    from urllib.error import HTTPError, URLError\n",
        "except ImportError:\n",
        "    from urlparse import urlparse\n",
        "    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n",
        "\n",
        "short_name = 'MOD10A1'\n",
        "version = '61'\n",
        "time_start = '2015-05-26T00:00:00Z'\n",
        "time_end = '2022-08-05T17:40:42Z'\n",
        "bounding_box = '75.65,31,78.69,32.74'\n",
        "polygon = ''\n",
        "filename_filter = ''\n",
        "url_list = []\n",
        "\n",
        "CMR_URL = 'https://cmr.earthdata.nasa.gov'\n",
        "URS_URL = 'https://urs.earthdata.nasa.gov'\n",
        "CMR_PAGE_SIZE = 2000\n",
        "CMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n",
        "                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n",
        "                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n",
        "\n",
        "\n",
        "def get_username():\n",
        "    username = 'kroy0001'\n",
        "    while not username:\n",
        "      # For Python 2/3 compatibility:\n",
        "      try:\n",
        "          do_input = raw_input  # noqa\n",
        "      except NameError:\n",
        "          do_input = input\n",
        "      username = do_input('Earthdata username (or press Return to use a bearer token): ')\n",
        "    return username\n",
        "\n",
        "\n",
        "def get_password():\n",
        "    password = '/#j%kWrPA,8.HRe'\n",
        "    while not password:\n",
        "        password = getpass('password: ')\n",
        "    return password\n",
        "\n",
        "def get_token():\n",
        "    token = ''\n",
        "    while not token:\n",
        "        token = getpass('bearer token: ')\n",
        "    return token\n",
        "\n",
        "\n",
        "def get_login_credentials():\n",
        "    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n",
        "    credentials = None\n",
        "    token = None\n",
        "\n",
        "    try:\n",
        "        info = netrc.netrc()\n",
        "        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n",
        "        if username == 'token':\n",
        "            token = password\n",
        "        else:\n",
        "            credentials = '{0}:{1}'.format(username, password)\n",
        "            credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n",
        "    except Exception:\n",
        "        username = None\n",
        "        password = None\n",
        "\n",
        "    if not username:\n",
        "        username = get_username()\n",
        "        if len(username):\n",
        "            password = get_password()\n",
        "            credentials = '{0}:{1}'.format(username, password)\n",
        "            credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n",
        "        else:\n",
        "            token = get_token()\n",
        "\n",
        "    return credentials, token\n",
        "\n",
        "\n",
        "def build_version_query_params(version):\n",
        "    desired_pad_length = 3\n",
        "    if len(version) > desired_pad_length:\n",
        "        print('Version string too long: \"{0}\"'.format(version))\n",
        "        quit()\n",
        "\n",
        "    version = str(int(version))  # Strip off any leading zeros\n",
        "    query_params = ''\n",
        "\n",
        "    while len(version) <= desired_pad_length:\n",
        "        padded_version = version.zfill(desired_pad_length)\n",
        "        query_params += '&version={0}'.format(padded_version)\n",
        "        desired_pad_length -= 1\n",
        "    return query_params\n",
        "\n",
        "\n",
        "def filter_add_wildcards(filter):\n",
        "    if not filter.startswith('*'):\n",
        "        filter = '*' + filter\n",
        "    if not filter.endswith('*'):\n",
        "        filter = filter + '*'\n",
        "    return filter\n",
        "\n",
        "\n",
        "def build_filename_filter(filename_filter):\n",
        "    filters = filename_filter.split(',')\n",
        "    result = '&options[producer_granule_id][pattern]=true'\n",
        "    for filter in filters:\n",
        "        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n",
        "    return result\n",
        "\n",
        "\n",
        "def build_cmr_query_url(short_name, version, time_start, time_end,\n",
        "                        bounding_box=None, polygon=None,\n",
        "                        filename_filter=None):\n",
        "    params = '&short_name={0}'.format(short_name)\n",
        "    params += build_version_query_params(version)\n",
        "    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n",
        "    if polygon:\n",
        "        params += '&polygon={0}'.format(polygon)\n",
        "    elif bounding_box:\n",
        "        params += '&bounding_box={0}'.format(bounding_box)\n",
        "    if filename_filter:\n",
        "        params += build_filename_filter(filename_filter)\n",
        "    return CMR_FILE_URL + params\n",
        "\n",
        "\n",
        "def get_speed(time_elapsed, chunk_size):\n",
        "    if time_elapsed <= 0:\n",
        "        return ''\n",
        "    speed = chunk_size / time_elapsed\n",
        "    if speed <= 0:\n",
        "        speed = 1\n",
        "    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n",
        "    i = int(math.floor(math.log(speed, 1000)))\n",
        "    p = math.pow(1000, i)\n",
        "    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n",
        "\n",
        "\n",
        "def output_progress(count, total, status='', bar_len=60):\n",
        "    if total <= 0:\n",
        "        return\n",
        "    fraction = min(max(count / float(total), 0), 1)\n",
        "    filled_len = int(round(bar_len * fraction))\n",
        "    percents = int(round(100.0 * fraction))\n",
        "    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n",
        "    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n",
        "    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n",
        "    sys.stdout.write(fmt)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n",
        "    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n",
        "    while True:\n",
        "        data = file_object.read(chunk_size)\n",
        "        if not data:\n",
        "            break\n",
        "        yield data\n",
        "\n",
        "\n",
        "def get_login_response(url, credentials, token):\n",
        "    opener = build_opener(HTTPCookieProcessor())\n",
        "\n",
        "    req = Request(url)\n",
        "    if token:\n",
        "        req.add_header('Authorization', 'Bearer {0}'.format(token))\n",
        "    elif credentials:\n",
        "        try:\n",
        "            response = opener.open(req)\n",
        "            # We have a redirect URL - try again with authorization.\n",
        "            url = response.url\n",
        "        except HTTPError:\n",
        "            # No redirect - just try again with authorization.\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print('Error{0}: {1}'.format(type(e), str(e)))\n",
        "            sys.exit(1)\n",
        "\n",
        "        req = Request(url)\n",
        "        req.add_header('Authorization', 'Basic {0}'.format(credentials))\n",
        "\n",
        "    try:\n",
        "        response = opener.open(req)\n",
        "    except HTTPError as e:\n",
        "        err = 'HTTP error {0}, {1}'.format(e.code, e.reason)\n",
        "        if 'Unauthorized' in e.reason:\n",
        "            if token:\n",
        "                err += ': Check your bearer token'\n",
        "            else:\n",
        "                err += ': Check your username and password'\n",
        "        print(err)\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print('Error{0}: {1}'.format(type(e), str(e)))\n",
        "        sys.exit(1)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "def cmr_download(urls, force=False, quiet=False):\n",
        "    \"\"\"Download files from list of urls.\"\"\"\n",
        "    if not urls:\n",
        "        return\n",
        "\n",
        "    url_count = len(urls)\n",
        "    if not quiet:\n",
        "        print('Downloading {0} files...'.format(url_count))\n",
        "    credentials = None\n",
        "    token = None\n",
        "\n",
        "    for index, url in enumerate(urls, start=1):\n",
        "        if not credentials and not token:\n",
        "            p = urlparse(url)\n",
        "            if p.scheme == 'https':\n",
        "                credentials, token = get_login_credentials()\n",
        "\n",
        "        filename = url.split('/')[-1]\n",
        "        if not quiet:\n",
        "            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n",
        "                                        url_count, filename))\n",
        "\n",
        "        try:\n",
        "            response = get_login_response(url, credentials, token)\n",
        "            length = int(response.headers['content-length'])\n",
        "            try:\n",
        "                if not force and length == os.path.getsize(filename):\n",
        "                    if not quiet:\n",
        "                        print('  File exists, skipping')\n",
        "                    continue\n",
        "            except OSError:\n",
        "                pass\n",
        "            count = 0\n",
        "            chunk_size = min(max(length, 1), 1024 * 1024)\n",
        "            max_chunks = int(math.ceil(length / chunk_size))\n",
        "            time_initial = time.time()\n",
        "            with open(filename, 'wb') as out_file:\n",
        "                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n",
        "                    out_file.write(data)\n",
        "                    if not quiet:\n",
        "                        count = count + 1\n",
        "                        time_elapsed = time.time() - time_initial\n",
        "                        download_speed = get_speed(time_elapsed, count * chunk_size)\n",
        "                        output_progress(count, max_chunks, status=download_speed)\n",
        "            if not quiet:\n",
        "                print()\n",
        "        except HTTPError as e:\n",
        "            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n",
        "        except URLError as e:\n",
        "            print('URL error: {0}'.format(e.reason))\n",
        "        except IOError:\n",
        "            raise\n",
        "\n",
        "\n",
        "def cmr_filter_urls(search_results):\n",
        "    \"\"\"Select only the desired data files from CMR response.\"\"\"\n",
        "    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n",
        "        return []\n",
        "\n",
        "    entries = [e['links']\n",
        "               for e in search_results['feed']['entry']\n",
        "               if 'links' in e]\n",
        "    # Flatten \"entries\" to a simple list of links\n",
        "    links = list(itertools.chain(*entries))\n",
        "\n",
        "    urls = []\n",
        "    unique_filenames = set()\n",
        "    for link in links:\n",
        "        if 'href' not in link:\n",
        "            # Exclude links with nothing to download\n",
        "            continue\n",
        "        if 'inherited' in link and link['inherited'] is True:\n",
        "            # Why are we excluding these links?\n",
        "            continue\n",
        "        if 'rel' in link and 'data#' not in link['rel']:\n",
        "            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n",
        "            continue\n",
        "\n",
        "        if 'title' in link and 'opendap' in link['title'].lower():\n",
        "            # Exclude OPeNDAP links--they are responsible for many duplicates\n",
        "            # This is a hack; when the metadata is updated to properly identify\n",
        "            # non-datapool links, we should be able to do this in a non-hack way\n",
        "            continue\n",
        "\n",
        "        filename = link['href'].split('/')[-1]\n",
        "        if filename in unique_filenames:\n",
        "            # Exclude links with duplicate filenames (they would overwrite)\n",
        "            continue\n",
        "        unique_filenames.add(filename)\n",
        "\n",
        "        urls.append(link['href'])\n",
        "\n",
        "    return urls\n",
        "\n",
        "\n",
        "def cmr_search(short_name, version, time_start, time_end,\n",
        "               bounding_box='', polygon='', filename_filter='', quiet=False):\n",
        "    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n",
        "    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n",
        "                                        time_start=time_start, time_end=time_end,\n",
        "                                        bounding_box=bounding_box,\n",
        "                                        polygon=polygon, filename_filter=filename_filter)\n",
        "    if not quiet:\n",
        "        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n",
        "\n",
        "    cmr_scroll_id = None\n",
        "    ctx = ssl.create_default_context()\n",
        "    ctx.check_hostname = False\n",
        "    ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    urls = []\n",
        "    hits = 0\n",
        "    while True:\n",
        "        req = Request(cmr_query_url)\n",
        "        if cmr_scroll_id:\n",
        "            req.add_header('cmr-scroll-id', cmr_scroll_id)\n",
        "        try:\n",
        "            response = urlopen(req, context=ctx)\n",
        "        except Exception as e:\n",
        "            print('Error: ' + str(e))\n",
        "            sys.exit(1)\n",
        "        if not cmr_scroll_id:\n",
        "            # Python 2 and 3 have different case for the http headers\n",
        "            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n",
        "            cmr_scroll_id = headers['cmr-scroll-id']\n",
        "            hits = int(headers['cmr-hits'])\n",
        "            if not quiet:\n",
        "                if hits > 0:\n",
        "                    print('Found {0} matches.'.format(hits))\n",
        "                else:\n",
        "                    print('Found no matches.')\n",
        "        search_page = response.read()\n",
        "        search_page = json.loads(search_page.decode('utf-8'))\n",
        "        url_scroll_results = cmr_filter_urls(search_page)\n",
        "        if not url_scroll_results:\n",
        "            break\n",
        "        if not quiet and hits > CMR_PAGE_SIZE:\n",
        "            print('.', end='')\n",
        "            sys.stdout.flush()\n",
        "        urls += url_scroll_results\n",
        "\n",
        "    if not quiet and hits > CMR_PAGE_SIZE:\n",
        "        print()\n",
        "    return urls\n",
        "\n",
        "\n",
        "def main(argv=None):\n",
        "    global short_name, version, time_start, time_end, bounding_box, \\\n",
        "        polygon, filename_filter, url_list\n",
        "\n",
        "    if argv is None:\n",
        "        argv = sys.argv[1:]\n",
        "\n",
        "    force = False\n",
        "    quiet = False\n",
        "    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n",
        "\n",
        "    try:\n",
        "        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n",
        "        for opt, _arg in opts:\n",
        "            if opt in ('-f', '--force'):\n",
        "                force = True\n",
        "            elif opt in ('-q', '--quiet'):\n",
        "                quiet = True\n",
        "            elif opt in ('-h', '--help'):\n",
        "                print(usage)\n",
        "                sys.exit(0)\n",
        "    except getopt.GetoptError as e:\n",
        "        print(e.args[0])\n",
        "        print(usage)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Supply some default search parameters, just for testing purposes.\n",
        "    # These are only used if the parameters aren't filled in up above.\n",
        "    if 'short_name' in short_name:\n",
        "        short_name = 'ATL06'\n",
        "        version = '003'\n",
        "        time_start = '2018-10-14T00:00:00Z'\n",
        "        time_end = '2021-01-08T21:48:13Z'\n",
        "        bounding_box = ''\n",
        "        polygon = ''\n",
        "        filename_filter = '*ATL06_2020111121*'\n",
        "        url_list = []\n",
        "\n",
        "    try:\n",
        "        if not url_list:\n",
        "            url_list = cmr_search(short_name, version, time_start, time_end,\n",
        "                                  bounding_box=bounding_box, polygon=polygon,\n",
        "                                  filename_filter=filename_filter, quiet=quiet)\n",
        "\n",
        "        cmr_download(url_list, force=force, quiet=quiet)\n",
        "    except KeyboardInterrupt:\n",
        "        quit()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing slope map"
      ],
      "metadata": {
        "id": "d3rwvgNgqaoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir -p /content/dem /content/out_dem /content/out_aspect_dem"
      ],
      "metadata": {
        "id": "7tJ4Pik8EdMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/dem\n",
        "!gdown https://drive.google.com/uc?id=1-8x01glg16zLwy3NO08oukZMs4Ms2lgP\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "nS1_B3EKHxZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/out_aspect_dem /content/out_dem"
      ],
      "metadata": {
        "id": "sm8X_-gXFrAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…Slope_calculations\n",
        "import os\n",
        "image_dir = r'/content/dem/'\n",
        "fileExt = r'.tif'\n",
        "temp_dir = r'/content/out_dem/'\n",
        "imgs_list = [f for f in os.listdir(image_dir) if f.endswith(fileExt)]\n",
        "imgs_list.sort()\n",
        "imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n",
        "for i, name in enumerate(imgs_path):\n",
        "  path = imgs_list[i]\n",
        "  !gdaldem slope {image_dir}{path} {temp_dir}{path[:-4]}\"_slope.tif\" -compute_edges -of GTiff -b 1 -p "
      ],
      "metadata": {
        "id": "kGFhGhZ9DuIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…aspect_calculations\n",
        "import os\n",
        "image_dir = r'/content/dem/'\n",
        "fileExt = r'.tif'\n",
        "temp_dir = r'/content/out_aspect_dem/'\n",
        "imgs_list = [f for f in os.listdir(image_dir) if f.endswith(fileExt)]\n",
        "imgs_list.sort()\n",
        "imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n",
        "for i, name in enumerate(imgs_path):\n",
        "  path = imgs_list[i]\n",
        "  !gdaldem aspect {image_dir}{path} {temp_dir}{path[:-4]}\"_aspect.tif\" -compute_edges -of GTiff -b 1"
      ],
      "metadata": {
        "id": "yhU4IjZSqfNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#os.system (cmd)\n",
        "slp1=gdal.Open(\"/content/out_aspect_dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001_aspect.tif\")\n",
        "slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "plt.figure()\n",
        "plt.imshow(slp1Array)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TDCikz2DzCk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdalinfo /content/out_aspect_dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001_slope_aspect.tif"
      ],
      "metadata": {
        "id": "f6mtw4_jhpmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdal_calc.py \\\n",
        "  --overwrite \\\n",
        "  --type=Float32 \\\n",
        "  -A \"/content/dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001.tif\" \\\n",
        "  --A_band 1 \\\n",
        "  --outfile=\"/content/check.tif\" \\\n",
        "  --calc=\"(A.astype(float)>=1000)*(A.astype(float)<=3000)\"\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#os.system (cmd)\n",
        "slp1=gdal.Open(\"/content/check.tif\")\n",
        "slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "plt.figure()\n",
        "plt.imshow(slp1Array)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_uTpcDCOG7Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…Separate_by_elevations\n",
        "elev = [100, 3000, 5000]\n",
        "temp_dir = \"/content/elevations/\"\n",
        "!mkdir -p {temp_dir}\n",
        "for i, elevations in enumerate(elev):\n",
        "  print(elev[i])\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A /content/dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001.tif --outfile={temp_dir}result_\"{elev[i]}\".tif --calc=\"A>={elev[i]}\" --NoDataValue=0\n",
        "\n",
        "\n",
        "\n",
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A /content/dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001.tif --outfile=result_1000.tif --calc=\"A>=1000\" --NoDataValue=0\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for i, elevations in enumerate(elev):\n",
        "  print(elev[i])\n",
        "  print(temp_dir+\"result_\"+str(elev[i])+\".tif\")\n",
        "  #os.system (cmd)\n",
        "  slp1=gdal.Open(temp_dir+\"result_\"+str(elev[i])+\".tif\")\n",
        "  slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "  plt.figure()\n",
        "  plt.imshow(slp1Array)\n",
        "  plt.colorbar()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "rufEF0QNMekI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…Polygonoze_each_extracted_elevation_raster_as_shape_file\n",
        "!mkdir -p \"/content/polygon_shp/\"\n",
        "for i, elevations in enumerate(elev):\n",
        "  print(elev[i])\n",
        "  dir_to_store = \"/content/polygon_shp/\"+str(elev[i])+\"_shpFile/\"\n",
        "  !mkdir -p {dir_to_store}\n",
        "  !gdal_polygonize.py {temp_dir}result_{elev[i]}.tif -f \"ESRI Shapefile\" {dir_to_store}result_{elev[i]}.shp OUTPUT DN  #âœ…"
      ],
      "metadata": {
        "id": "K0eTuu0kZOJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/elevations_aspects"
      ],
      "metadata": {
        "id": "NL2curIyTy4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1pQH1VU_KX1TIbJLHvptxAFup10EUj6nX"
      ],
      "metadata": {
        "id": "17z3TX-o8S4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/correct.zip"
      ],
      "metadata": {
        "id": "tHff4TSQHYb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…Separate_by_elevations_THEN_aspects\n",
        "elev = [ 1001000, 10002000, 20003000, 30004000, 40005000, 50006000, 60007000]\n",
        "\n",
        "aspects = [0, 45, 90, 135, 180, 225, 270, 315]\n",
        "temp_dir = \"/content/elevations_aspects/\"\n",
        "!mkdir -p {temp_dir}\n",
        "temp_dir_elevations = \"/content/correct/\"\n",
        "for j, elevations in enumerate(elev):\n",
        "  !gdalwarp \\\n",
        "    -ot Float32 \\\n",
        "    -cutline {temp_dir_elevations}E{elev[j]}.shp  -crop_to_cutline \\\n",
        "    /content/out_aspect_dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001_aspect.tif \\\n",
        "    {temp_dir}result_\"{elev[j]}\".tif\n"
      ],
      "metadata": {
        "id": "eZDV-P9e7Nac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…Separate_by_elevations_aspects\n",
        "elev = [ 1001000, 10002000, 20003000, 30004000, 40005000, 50006000, 60007000]\n",
        "\n",
        "aspects = [0, 45, 90, 135, 180, 225, 270, 315]\n",
        "temp_dir = \"/content/elevations_aspects/\"\n",
        "!mkdir -p {temp_dir}\n",
        "temp_dir_elevations = \"/content/correct/\"\n",
        "for j, elevations in enumerate(elev):\n",
        "  !gdalwarp \\\n",
        "    -ot Float32 \\\n",
        "    -cutline {temp_dir_elevations}E{elev[j]}.shp  -crop_to_cutline \\\n",
        "    /content/out_aspect_dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001_aspect.tif \\\n",
        "    {temp_dir}result_\"{elev[j]}\".tif\n",
        "\n",
        "for j, elevations in enumerate(elev):\n",
        "  !mkdir -p {temp_dir}\"A\"{elev[j]}\n",
        "  for i, aspect in enumerate(aspects):\n",
        "      print(aspects[i])\n",
        "      !gdal_calc.py \\\n",
        "        --overwrite \\\n",
        "        --type=Float32 \\\n",
        "        -A {temp_dir}result_\"{elev[j]}\".tif --outfile={temp_dir}\"A\"{elev[j]}\"/\"result_\"{aspects[i]}\".tif --calc=\"A>={aspects[i]}\" --NoDataValue=0\n",
        "\n",
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A /content/dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001.tif --outfile=result_1000.tif --calc=\"A>=1000\" --NoDataValue=0\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for j, elevations in enumerate(elev):\n",
        "  for i, aspect in enumerate(aspects):\n",
        "    print(aspects[i])\n",
        "    print(temp_dir+\"A\"+str(elev[j])+\"/result_\"+str(aspects[i])+\".tif\")\n",
        "    #os.system (cmd)\n",
        "    slp1=gdal.Open(temp_dir+\"A\"+str(elev[j])+\"/result_\"+str(aspects[i])+\".tif\")\n",
        "    slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "    plt.figure()\n",
        "    plt.imshow(slp1Array)\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CYiR765es5hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…Polygonoze_each_extracted_elevation&aspects[8]_raster_as_shape_file\n",
        "\n",
        "elev = [ 1001000, 10002000, 20003000, 30004000, 40005000, 50006000, 60007000]\n",
        "aspects = [0, 45, 90, 135, 180, 225, 270, 315]\n",
        "temp_dir = \"/content/elevations_aspects/\"\n",
        "!mkdir -p \"/content/polygon_shp/\"\n",
        "for j, elevations in enumerate(elev):\n",
        "  for i, aspect in enumerate(aspects):\n",
        "    print(elev[j])\n",
        "    dir_to_store = \"/content/polygon_shp/A\"+str(elev[j])+\"_shpFile/\"\n",
        "    !mkdir -p {dir_to_store}\n",
        "    !gdal_polygonize.py {temp_dir}A{elev[j]}/result_{aspects[i]}.tif -f \"ESRI Shapefile\" {dir_to_store}result_{aspects[i]}.shp OUTPUT DN  #âœ…"
      ],
      "metadata": {
        "id": "dnOpeeHiYDuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/polygon_shp"
      ],
      "metadata": {
        "id": "AX1baXclUP7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2rU8zB757kMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEST 2 FOR ASPECTS POLYGONIZATION >>> AS EACH ELEVATION IS CUT INTO [8] ASPECTS"
      ],
      "metadata": {
        "id": "3N6B1l__7lzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK"
      ],
      "metadata": {
        "id": "lFAA9g9x7kaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/elev_correct.zip"
      ],
      "metadata": {
        "id": "DVo3bDmy7kaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…Separate_by_elevations_THEN_aspects\n",
        "elev = [1003000, 30005000, 50007000]\n",
        "\n",
        "aspects = [0, 45, 90, 135, 180, 225, 270, 315]\n",
        "temp_dir = \"/content/elevations_aspects/\"\n",
        "!mkdir -p {temp_dir}\n",
        "temp_dir_elevations = \"/content/elev_correct/\"\n",
        "for j, elevations in enumerate(elev):\n",
        "  !gdalwarp \\\n",
        "    -ot Float32 \\\n",
        "    -cutline {temp_dir_elevations}E{elev[j]}.shp  -crop_to_cutline \\\n",
        "    /content/out_aspect_dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001_aspect.tif \\\n",
        "    {temp_dir}result_\"{elev[j]}\".tif\n"
      ],
      "metadata": {
        "id": "nqkLGGic7kaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/polygon_shp"
      ],
      "metadata": {
        "id": "oVUOnFx5CeCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…Separate_by_aspect_each_elevations_THEN_polygonize_each_vectors\n",
        "formulas = [\"(A<=45)*1\", \"(A>46)*(A<=90)*2\", \"(A>90)*(A<=135)*3\", \"(A>136)*(A<=180)*4\", \"(A>181)*(A<=225)*5\", \"(A>226)*(A<=270)*6\", \"(A>271)*(A<=315)*7\", \"(A>316)*8\"]\n",
        "print(len(formulas))\n",
        "CLIPPED_ELEVATIONS = \"/content/elevations_aspects/\"\n",
        "elev = [1003000, 30005000, 50007000]\n",
        "aspects = [0, 45, 90, 135, 180, 225, 270, 315]\n",
        "temp_dir = \"/content/elevations_aspects/\"\n",
        "!mkdir -p \"/content/polygon_shp/\"\n",
        "for j, elevations in enumerate(elev):\n",
        "  in_image = str(CLIPPED_ELEVATIONS)+\"result_\"+str(elev[j])+\".tif\"\n",
        "  print(in_image)\n",
        "  dir_to_store = \"/content/polygon_shp/A\"+str(elev[j])+\"_shpFile/\"\n",
        "  !mkdir -p {dir_to_store}\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {in_image} \\\n",
        "    --outfile={dir_to_store}SmallerFileB1.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A<=45)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store}SmallerFileB1.tif -f \"ESRI Shapefile\" {dir_to_store}result_B1.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {in_image} \\\n",
        "    --outfile={dir_to_store}SmallerFileB2.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>46)*(A<=90)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store}SmallerFileB2.tif -f \"ESRI Shapefile\" {dir_to_store}result_B2.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {in_image} \\\n",
        "    --outfile={dir_to_store}SmallerFileB3.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>90)*(A<=135)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store}SmallerFileB3.tif -f \"ESRI Shapefile\" {dir_to_store}result_B3.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {in_image} \\\n",
        "    --outfile={dir_to_store}SmallerFileB4.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>136)*(A<=180)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store}SmallerFileB4.tif -f \"ESRI Shapefile\" {dir_to_store}result_B4.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {in_image} \\\n",
        "    --outfile={dir_to_store}SmallerFileB5.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>181)*(A<=225)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store}SmallerFileB5.tif -f \"ESRI Shapefile\" {dir_to_store}result_B5.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {in_image} \\\n",
        "    --outfile={dir_to_store}SmallerFileB6.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>226)*(A<=270)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store}SmallerFileB6.tif -f \"ESRI Shapefile\" {dir_to_store}result_B6.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {in_image} \\\n",
        "    --outfile={dir_to_store}SmallerFileB7.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>271)*(A<=315)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store}SmallerFileB7.tif -f \"ESRI Shapefile\" {dir_to_store}result_B7.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {in_image} \\\n",
        "    --outfile={dir_to_store}SmallerFileB8.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>316)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store}SmallerFileB8.tif -f \"ESRI Shapefile\" {dir_to_store}result_B8.shp OUTPUT DN  #âœ…\n",
        "\n",
        "\n",
        "# import os\n",
        "# import subprocess\n",
        "# from osgeo import gdal\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for i, formula in enumerate(formulas):\n",
        "#   #os.system (cmd)\n",
        "#   slp1=gdal.Open(\"/content/SmallerFileB\"+str(i+1)+\".tif\")\n",
        "#   slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "#   plt.figure()\n",
        "#   plt.imshow(slp1Array)\n",
        "#   plt.colorbar()\n",
        "#   plt.show()"
      ],
      "metadata": {
        "id": "4qxWcG9VwjNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "download the polygon shape files for aspects[8] for each elevations use \"FIX GEOMETRY\" using qgis then re ipload for computations"
      ],
      "metadata": {
        "id": "wKDa41MODEus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/polygon_shp\n",
        "!zip -r /content/poly_shp_filesX.zip /content/polygon_shp/*\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "ohSAP166DDkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TESTING DEM CUT BY RASTER CALCULATION USING `gdal_calc`"
      ],
      "metadata": {
        "id": "nLcfZ9S47y90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#os.system (cmd)\n",
        "slp1=gdal.Open(\"/content/SmallerFileB.tif\")\n",
        "slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "plt.figure()\n",
        "plt.imshow(slp1Array)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H4u0UT3LT9C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cutting into Area Of interest using shape file\n",
        "!gdalwarp \\\n",
        "  -ot Float32 \\\n",
        "  -cutline /content/correct/E0400_1000.shp \\\n",
        "  /content/dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001.tif \\\n",
        "  /content/clipped.tif"
      ],
      "metadata": {
        "id": "9cCGtF_LaHGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ogr2ogr filtered_result.shp /content/result_1000.shp -where \"DN=1\"\n"
      ],
      "metadata": {
        "id": "mcNmMtAiZvyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A /content/dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001.tif --outfile=result_1000.tif --calc=\"A>=100\" --NoDataValue=0\n",
        "# !gdal_polygonize.py result_1000.tif  -f \"ESRI Shapefile\" /content/xyz.shp OUTPUT DN  #âœ…"
      ],
      "metadata": {
        "id": "A5BDsrTMKVqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clipping by raster to raster"
      ],
      "metadata": {
        "id": "n57Oqkv3lLbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#os.system (cmd)\n",
        "slp1=gdal.Open(\"/content/clipped.tif\")\n",
        "slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "plt.figure()\n",
        "plt.imshow(slp1Array)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "djwLNSXJZF9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "!gdal_calc.py \\\n",
        "  --overwrite \\\n",
        "  --type=Float32 \\\n",
        "  -A /content/result_4000.tif -B /content/dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001.tif --outfile=SmallerFileA.tif --NoDataValue=0 --calc=\"B.astype(float)*(A>=0)\" \n",
        "\n",
        "\n",
        "#os.system (cmd)\n",
        "slp1=gdal.Open(\"/content/SmallerFileA.tif\")\n",
        "slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "plt.figure()\n",
        "plt.imshow(slp1Array)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "!gdal_calc.py \\\n",
        "  --overwrite \\\n",
        "  --type=Float32 \\\n",
        "  -A /content/result_2000.tif -B /content/dem/SRTMGL1_NC.003_SRTMGL1_DEM_doy2000042_aid0001.tif --outfile=SmallerFileB.tif --NoDataValue=0 --calc=\"B.astype(float)*(A>=0)\" \n",
        "\n",
        "\n",
        "#os.system (cmd)\n",
        "slp1=gdal.Open(\"/content/SmallerFileB.tif\")\n",
        "slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "plt.figure()\n",
        "plt.imshow(slp1Array)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "!gdal_calc.py \\\n",
        "  --overwrite \\\n",
        "  --type=Float32 \\\n",
        "  -A /content/SmallerFileA.tif -B /content/SmallerFileB.tif --outfile=SmallerFileC.tif --NoDataValue=0 --calc=\"(B-A)\" \n",
        "\n",
        "\n",
        "#os.system (cmd)\n",
        "slp1=gdal.Open(\"/content/SmallerFileC.tif\")\n",
        "slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "plt.figure()\n",
        "plt.imshow(slp1Array)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r1rNvT52hAy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/polygon_shp\n",
        "!zip -r /content/poly_shp_files.zip /content/polygon_shp/*\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "v6SMkv-9NHVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clipping slope"
      ],
      "metadata": {
        "id": "b7i6ZB1ZF1uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading slope map & elevation shape files"
      ],
      "metadata": {
        "id": "ilh0woBzRi2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content\n",
        "!gdown https://drive.google.com/uc?id=1hJqzhX9smqsTlMvWyT3U2vc9X52rLbmI #downloading SLOPE-MAP\n",
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "0yv_sfC7KAxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#os.system (cmd)\n",
        "slp1=gdal.Open(\"/content/slope_polygon_shp_files/A1003000_shpFile/SmallerFileB1.tif\")\n",
        "slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "plt.figure()\n",
        "plt.imshow(slp1Array)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TmapNyNdZvLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "#####################important import libraries\n",
        "CLIPPED_ELEVATIONS = \"/content/elev_correct/\"\n",
        "elev = [1003000, 30005000, 50007000]\n",
        "save_dir = \"/content/slope_polygon_shp_files/\"\n",
        "!mkdir -p {save_dir}\n",
        "for j, elevations in enumerate(elev):\n",
        "  in_image = \"/content/Reclass_Slop21.tif\" #enter path to SLOPE-MAP\n",
        "  print(in_image)\n",
        "  ################################CUTTING SLOPE-MAP BY ELEVATION\n",
        "  !gdalwarp \\\n",
        "    -ot Float32 \\\n",
        "    -cutline {CLIPPED_ELEVATIONS}E{elev[j]}.shp  -crop_to_cutline \\\n",
        "    {in_image} \\\n",
        "    {save_dir}result_\"{elev[j]}\".tif\n",
        "  ################################CUTTING SLOPE-MAP BY ELEVATION\n",
        "  dir_to_store_slope_shapeFiles = str(save_dir)+\"A\"+str(elev[j])+\"_shpFile/\"\n",
        "  !mkdir -p {dir_to_store_slope_shapeFiles}\n",
        "\n",
        "  ################################RASTERIZING TO DIFFERENT CLASSES\n",
        "  INPUT_SLOPE_IN_IMAGE = str(save_dir)+\"result_\"+str(elev[j])+\".tif\"\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {INPUT_SLOPE_IN_IMAGE} \\\n",
        "    --outfile={dir_to_store_slope_shapeFiles}SmallerFileB1.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A<=1)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store_slope_shapeFiles}SmallerFileB1.tif -f \"ESRI Shapefile\" {dir_to_store_slope_shapeFiles}result_B1.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {INPUT_SLOPE_IN_IMAGE} \\\n",
        "    --outfile={dir_to_store_slope_shapeFiles}SmallerFileB2.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>1)*(A<=2)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store_slope_shapeFiles}SmallerFileB2.tif -f \"ESRI Shapefile\" {dir_to_store_slope_shapeFiles}result_B2.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {INPUT_SLOPE_IN_IMAGE} \\\n",
        "    --outfile={dir_to_store_slope_shapeFiles}SmallerFileB3.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>2)*(A<=3)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store_slope_shapeFiles}SmallerFileB3.tif -f \"ESRI Shapefile\" {dir_to_store_slope_shapeFiles}result_B3.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {INPUT_SLOPE_IN_IMAGE} \\\n",
        "    --outfile={dir_to_store_slope_shapeFiles}SmallerFileB4.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>3)*(A<=4)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store_slope_shapeFiles}SmallerFileB4.tif -f \"ESRI Shapefile\" {dir_to_store_slope_shapeFiles}result_B4.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {INPUT_SLOPE_IN_IMAGE} \\\n",
        "    --outfile={dir_to_store_slope_shapeFiles}SmallerFileB5.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>4)*(A<=5)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store_slope_shapeFiles}SmallerFileB5.tif -f \"ESRI Shapefile\" {dir_to_store_slope_shapeFiles}result_B5.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {INPUT_SLOPE_IN_IMAGE} \\\n",
        "    --outfile={dir_to_store_slope_shapeFiles}SmallerFileB6.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>5)*(A<=6)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store_slope_shapeFiles}SmallerFileB6.tif -f \"ESRI Shapefile\" {dir_to_store_slope_shapeFiles}result_B6.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {INPUT_SLOPE_IN_IMAGE} \\\n",
        "    --outfile={dir_to_store_slope_shapeFiles}SmallerFileB7.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>6)*(A<=7)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store_slope_shapeFiles}SmallerFileB7.tif -f \"ESRI Shapefile\" {dir_to_store_slope_shapeFiles}result_B7.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {INPUT_SLOPE_IN_IMAGE} \\\n",
        "    --outfile={dir_to_store_slope_shapeFiles}SmallerFileB8.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>7)*(A<=8)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store_slope_shapeFiles}SmallerFileB8.tif -f \"ESRI Shapefile\" {dir_to_store_slope_shapeFiles}result_B8.shp OUTPUT DN  #âœ…\n",
        "\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {INPUT_SLOPE_IN_IMAGE} \\\n",
        "    --outfile={dir_to_store_slope_shapeFiles}SmallerFileB9.tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"(A>8)*(A<=9)*1\"\n",
        "  !gdal_polygonize.py {dir_to_store_slope_shapeFiles}SmallerFileB9.tif -f \"ESRI Shapefile\" {dir_to_store_slope_shapeFiles}result_B9.shp OUTPUT DN  #âœ…"
      ],
      "metadata": {
        "id": "3Zb0DihaMBX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/slope_polygon_shp_files\n",
        "!zip -r /content/SlopePpoly_shp_filesX.zip /content/slope_polygon_shp_files*\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "oXoAOUi0dPjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test raster calculation instead of vector calculation (as vector creation and calculation takes more time)"
      ],
      "metadata": {
        "id": "ZtusSQU1umhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal"
      ],
      "metadata": {
        "id": "xh7FbvwFjlxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A {image_dir}{pathb4} \\\n",
        "#   --A_band 1 \\\n",
        "#   -B {image_dir}{pathb6} \\\n",
        "#   --B_band 1 \\\n",
        "#   --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "#   --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A {image_dir}{pathb2} \\\n",
        "#   --A_band 1 \\\n",
        "#   -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "#   --B_band 1 \\\n",
        "#   --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "#   --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "!rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "INPUT_SLOPE_IN_IMAGE = \"/content/Reclass_Slop21.tif\"\n",
        "!gdal_translate -outsize 454 233 {INPUT_SLOPE_IN_IMAGE} /content/output.tif\n",
        "!gdal_calc.py \\\n",
        "  --overwrite \\\n",
        "  --type=Float32 \\\n",
        "  -A /content/output.tif \\\n",
        "  --A_band 1 \\\n",
        "  -B /content/NDSI_resultnnn.tif \\\n",
        "  --B_band 1 \\\n",
        "  --outfile=SmallerFileB2.tif \\\n",
        "  --NoDataValue=0 \\\n",
        "  --calc=\"((A>3)*(A<=4)*1)&(B.astype(float)>=0.4)\"\n",
        "import gdalnumeric\n",
        "raster_file = gdalnumeric.LoadFile(\"/content/SmallerFileB2.tif\")\n",
        "pixel_count_snow = (raster_file ==1).sum()\n",
        "pixel_count_notsnow = (raster_file ==0).sum()\n",
        "print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)"
      ],
      "metadata": {
        "id": "c6QSt-7-jqDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#os.system (cmd)\n",
        "slp1=gdal.Open(\"/content/SmallerFileB2.tif\")\n",
        "slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "plt.figure()\n",
        "plt.imshow(slp1Array)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JOGCzK-6kgpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nS9OyXsBE1cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸªœSLOPEðŸªœ DATA SNOW COVER CALCULATOR >>>"
      ],
      "metadata": {
        "id": "m7fIE_mGhWmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "nEHp5mXEhuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq"
      ],
      "metadata": {
        "id": "OEuDDcBlhuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1hJqzhX9smqsTlMvWyT3U2vc9X52rLbmI #downloading SLOPE-MAP\n",
        "!gdown https://drive.google.com/uc?id=1ISVpFRmCNlRDHvIyVlYAgLMe2uZVaFou #downloading SLOPE ELEVATION SHAPE FILES\n",
        "!unzip /content/slope_polygon_shp_files.zip\n",
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "QNNIqyxOhuFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://edcintl.cr.usgs.gov/downloads/sciweb1/shared/fews/download_tool/FEWS_CiDB8Dk8lDwYiE1dXDYH.zip"
      ],
      "metadata": {
        "id": "Xs27mxZxfHHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_dir = r'/content/'\n",
        "######################################READING SLOPE MAP HAVING 9 CLASSES##################\n",
        "INPUT_SLOPE_IN_IMAGE = \"/content/Reclass_Slop21.tif\"\n",
        "!gdal_translate -outsize 454 233 {INPUT_SLOPE_IN_IMAGE} {temp_dir}output.tif   #<<<< RESIZING SLOPE MAP for size of NDSI result\n",
        "##############################RASTER CALCULATION OF SLOPEMAP & NDSI RESULT#########\n",
        "######################################CREATING SCA ON 9 SLOPES##################\n",
        "for i in range(1,10):\n",
        "  !gdal_calc.py \\\n",
        "    --overwrite \\\n",
        "    --type=Float32 \\\n",
        "    -A {temp_dir}output.tif \\\n",
        "    --A_band 1 \\\n",
        "    -B {temp_dir}BothCheck_result.tif \\\n",
        "    --B_band 1 \\\n",
        "    --outfile=SmallerFileB\"{str(i)}\".tif \\\n",
        "    --NoDataValue=0 \\\n",
        "    --calc=\"((A=={i})*1)*(B.astype(float))\"\n",
        "######################################CREATING SCA ON 9 SLOPES##################\n",
        "!rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "import gdalnumeric\n",
        "SC_Slope_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "elev = [ 1003000, 30005000, 50007000]\n",
        "for j, elevations in enumerate(elev):\n",
        "  for i in range(1,10):\n",
        "    !gdalwarp \\\n",
        "      --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "      -overwrite -of GTiff -ot Float32 \\\n",
        "      -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "      {temp_dir}SmallerFileB\"{str(i)}\".tif \\\n",
        "      {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "\n",
        "    #counting Snow and Non-Snowpixels\n",
        "    raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "    pixel_nos_snow = (raster_file ==1).sum(); pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "    SC_Slope_values.append(pixel_nos_snow); SC_Slope_values.append(pixel_nos_notsnow)\n",
        "    m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "    coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "\n",
        "    print(j,\",\",pixel_nos_snow * coonstant_c,\",\", pixel_nos_notsnow * coonstant_c)\n",
        "    !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "print(len(SC_Slope_values))\n",
        "######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "from osgeo import gdal\n",
        "\n",
        "#os.system (cmd)\n",
        "slp1=gdal.Open(\"/content/SmallerFileB2.tif\")\n",
        "slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "plt.figure()\n",
        "plt.imshow(slp1Array)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7jfXlYf1KsFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "save_dir = \"/content/slope_polygon_shp_files/\"\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "imgs_list_b4 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b4)]\n",
        "imgs_list_b6 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b6)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_list_b4.sort(reverse=True)\n",
        "imgs_list_b6.sort(reverse=True)\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "imgs_path_b4 = [os.path.join(image_dir, i) for i in imgs_list_b4 if i != 'outputs']\n",
        "imgs_path_b6 = [os.path.join(image_dir, i) for i in imgs_list_b6 if i != 'outputs']\n",
        "print(len(imgs_path_b2),len(imgs_path_b4),len(imgs_path_b6))\n",
        "\n",
        "data_output = image_dir[:-6] + \"DATA/SlopeOutfinalTest.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "if(path.exists(data_output) == False):\n",
        "  f = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      pathb4 = imgs_list_b4[i]\n",
        "      pathb6 = imgs_list_b6[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[25:25+10])\n",
        "      f = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "      f.writelines(\"0,\" + lines + '\\n')\n",
        "  f.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    pathb4 = imgs_list_b4[i]\n",
        "    pathb6 = imgs_list_b6[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[25:25+10])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[25:25+10]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[25:25+10]), \"1,\" + str(pathb2[25:25+10]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[25:25+10]))\n",
        "        ########################################################################\n",
        "        #creating file NDSI\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb4} \\\n",
        "          --A_band 1 \\\n",
        "          -B {image_dir}{pathb6} \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "          --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb2} \\\n",
        "          --A_band 1 \\\n",
        "          -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "          --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "\n",
        "        !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "        #deleting file\n",
        "        ########################################################################\n",
        "        #counting Snow and Non-Snowpixels  \n",
        "        raster_file = gdalnumeric.LoadFile(temp_dir+\"BothCheck_result.tif\")\n",
        "        pixel_count_snow = (raster_file ==1).sum()\n",
        "        pixel_count_notsnow = (raster_file ==0).sum()\n",
        "        print(\"snow:\", pixel_count_snow,\" not snow:\", pixel_count_notsnow)\n",
        "\n",
        "        ######################################READING SLOPE MAP HAVING 9 CLASSES##################\n",
        "        INPUT_SLOPE_IN_IMAGE = \"/content/Reclass_Slop21.tif\"\n",
        "        !gdal_translate -outsize 454 233 {INPUT_SLOPE_IN_IMAGE} {temp_dir}output.tif   #<<<< RESIZING SLOPE MAP for size of NDSI result\n",
        "        ##############################RASTER CALCULATION OF SLOPEMAP & NDSI RESULT#########\n",
        "        ######################################CREATING SCA ON 9 SLOPES##################\n",
        "        for i in range(1,10):\n",
        "          !gdal_calc.py \\\n",
        "            --overwrite \\\n",
        "            --type=Float32 \\\n",
        "            -A {temp_dir}output.tif \\\n",
        "            --A_band 1 \\\n",
        "            -B {temp_dir}BothCheck_result.tif \\\n",
        "            --B_band 1 \\\n",
        "            --outfile=SmallerFileB\"{str(i)}\".tif \\\n",
        "            --NoDataValue=0 \\\n",
        "            --calc=\"((A=={i})*1)*(B.astype(float))\"\n",
        "        ######################################CREATING SCA ON 9 SLOPES##################\n",
        "        ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        import gdalnumeric\n",
        "        SC_Slope_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          for i in range(1,10):\n",
        "            !gdalwarp \\\n",
        "              --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "              -overwrite -of GTiff -ot Float32 \\\n",
        "              -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "              {temp_dir}SmallerFileB\"{str(i)}\".tif \\\n",
        "              {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "\n",
        "            #counting Snow and Non-Snowpixels\n",
        "            raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "            pixel_nos_snow = (raster_file ==1).sum(); pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "            SC_Slope_values.append(pixel_nos_snow) #; SC_Slope_values.append(pixel_nos_notsnow)\n",
        "            print(j,\",\",pixel_nos_snow * coonstant_c,\",\", pixel_nos_notsnow * coonstant_c)\n",
        "            !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "        print(len(SC_Slope_values))\n",
        "        ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        ########################################################################\n",
        "        m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "        ########################################################################\n",
        "        !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "        #######################################################################\n",
        "        area_slope_sc_values = [number * coonstant_c for number in SC_Slope_values]\n",
        "        print(str(coonstant_c*pixel_count_snow),str(coonstant_c*pixel_count_notsnow),str(coonstant_c*pixel_count_snow+coonstant_c*pixel_count_notsnow))\n",
        "        #######################################################################\n",
        "        lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow) + \",\" + str(area_slope_sc_values)[1:-1])\n",
        "        # lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow))\n",
        "\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[25:25+10]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n"
      ],
      "metadata": {
        "id": "WKgCghZLi9SJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9150cbaa-64ab-453c-ebb9-665c255ba2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB6.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB6.tif.\n",
            "Copying nodata values from source /content/SmallerFileB6.tif to destination /content/Xresult_6.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 , 0.0 , 14934.027775077708\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB7.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB7.tif.\n",
            "Copying nodata values from source /content/SmallerFileB7.tif to destination /content/Xresult_7.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 , 0.0 , 14934.027775077708\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB8.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB8.tif.\n",
            "Copying nodata values from source /content/SmallerFileB8.tif to destination /content/Xresult_8.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 , 0.0 , 14934.027775077708\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB9.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB9.tif.\n",
            "Copying nodata values from source /content/SmallerFileB9.tif to destination /content/Xresult_9.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 , 0.0 , 14934.027775077708\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB1.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB1.tif.\n",
            "Copying nodata values from source /content/SmallerFileB1.tif to destination /content/Xresult_1.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "1 , 174.65277774620057 , 14255.034719644913\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB2.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB2.tif.\n",
            "Copying nodata values from source /content/SmallerFileB2.tif to destination /content/Xresult_2.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "1 , 242.7083332894517 , 14183.159719657908\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB3.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB3.tif.\n",
            "Copying nodata values from source /content/SmallerFileB3.tif to destination /content/Xresult_3.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "1 , 85.76388887338278 , 14344.44444185097\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MODIS_MOD10A1_Snow_Cover_Area.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}