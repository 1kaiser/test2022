{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y9ZP-oZjoATD",
        "SI1aeBA-dCqC",
        "DllYcUtgovO5",
        "4o02pjAJqdjz",
        "PddyjIzYzu9D",
        "LvNe_bDIz0Wy",
        "w5h1ioy9Rbtl",
        "FzqGlqVxz5mJ",
        "CbgbyxgY0AaN",
        "T4WTGhNs0rep",
        "cq1KuyqhumRh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2022/blob/main/MLPCopy_of_Untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  run from here"
      ],
      "metadata": {
        "id": "Y9ZP-oZjoATD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI1aeBA-dCqC"
      },
      "source": [
        "### Model and training code\n",
        "\n",
        "Our model is a coordinate-based multilayer perceptron. In this example, for each input image coordinate $(x,y)$, the model predicts the associated color $(r,g,b)$ or any $(gray)$.\n",
        "\n",
        "![Network diagram](https://user-images.githubusercontent.com/3310961/85066930-ad444580-b164-11ea-9cc0-17494679e71f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[--xla_force_host_platform_device_count](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html#:~:text=When%20running%20on-,CPU,-you%20can%20always)"
      ],
      "metadata": {
        "id": "Zehuof-K_cZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#✅\n",
        "import os\n",
        "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
        "import jax\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "rUeQSSu5_Jwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b4b7f6e-6a79-420e-e1b8-f1745e727211"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[CpuDevice(id=0),\n",
              " CpuDevice(id=1),\n",
              " CpuDevice(id=2),\n",
              " CpuDevice(id=3),\n",
              " CpuDevice(id=4),\n",
              " CpuDevice(id=5),\n",
              " CpuDevice(id=6),\n",
              " CpuDevice(id=7)]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#✅\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(inputs):\n",
        "    print(\"positional_encoding start\")\n",
        "    batch_size, _ = inputs.shape;print(inputs.shape)\n",
        "    inputs_freq = jax.vmap(lambda x: inputs * 2.0 ** x)(jnp.arange(positional_encoding_dims));print(inputs_freq.shape)\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)]);print(x.shape)\n",
        "    x = x.swapaxes(0, 2);print(x.shape)\n",
        "    x = x.reshape([batch_size, -1]);print(x.shape)\n",
        "    x = jnp.concatenate([inputs, x], axis=-1);print(x.shape)\n",
        "    print(\"positional_encoding end\")\n",
        "    return x\n",
        "\n",
        "# y = np.ones((256, 256, 3))\n",
        "# print(y.shape)\n",
        "# image_height, image_width, cha = y.shape\n",
        "# size = image_height * image_width\n",
        "# yt = np.ones((size, cha))\n",
        "# print(yt.shape)\n",
        "# positional_encoding(yt)\n"
      ],
      "metadata": {
        "id": "z9aPWpu5iJ1I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# positional_encoding_vmap = jax.vmap(positional_encoding)\n",
        "# ######################################\n",
        "# y = jnp.ones((8, 256, 256, 3))\n",
        "# print(y.shape)\n",
        "# batchsize, image_height, image_width, cha = y.shape\n",
        "# size = image_height * image_width\n",
        "# yt = jnp.ones((batchsize, size, cha))\n",
        "# ######################################\n",
        "# print(\"vmap >>>\")\n",
        "# print(positional_encoding_vmap(yt).shape)\n",
        "\n",
        "# positional_encoding_pmap = jax.pmap(positional_encoding)\n",
        "# print(\"pmap >>>\")\n",
        "# print(positional_encoding_pmap(yt).shape)"
      ],
      "metadata": {
        "id": "YEAZBplw6bKG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP MODEL\n",
        "Basically, passing input points through a simple Fourier Feature Mapping enables an MLP to learn high-frequency functions (such as an RGB image) in low-dimensional problem domains (such as a 2D coordinate of pixels)."
      ],
      "metadata": {
        "id": "DllYcUtgovO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#✅\n",
        "!python -m pip install -q -U flax\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import flax\n",
        "import optax\n",
        "from typing import Any\n",
        "\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "num_dense_layers = 8 # Number of dense layers in MLP\n",
        "dense_layer_width = 256 # Dimentionality of dense layers' output space \n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "        x = positional_encoding(input_points) if self.apply_positional_encoding else input_points\n",
        "        print(\"network model start\")\n",
        "        print(x.shape)\n",
        "        for i in range(num_dense_layers):\n",
        "            x = nn.Dense(\n",
        "                dense_layer_width,\n",
        "                dtype=self.dtype,\n",
        "                precision=self.precision\n",
        "                )(x)\n",
        "            x = nn.relu(x)\n",
        "            x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "            print(x.shape)\n",
        "  \n",
        "        x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "        print(x.shape)\n",
        "        print(\"network model end\")\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "VRkotxnvvHrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9914986d-e821-47e9-d1ce-491cef46ca4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 185 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 57.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 53.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 5.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#✅\n",
        "# !wget https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz\n",
        "# from google.colab import output\n",
        "# output.clear() #to_clear_the_output_console_everytime\n",
        "# import jax.numpy as jnp\n",
        "\n",
        "# data = jnp.load(\"tiny_nerf_data.npz\")\n",
        "# images = data[\"images\"]\n",
        "# poses = data[\"poses\"]\n",
        "# focal = float(data[\"focal\"])\n",
        "# _, image_height, image_width, _ = images.shape\n",
        "# train_images, train_poses = images[:100], poses[:100]\n",
        "# val_image, val_pose = images[101], poses[101]\n",
        "# ############################################\n",
        "# def initialize_model(key, input_pts_shape):\n",
        "#     model = MLPModel()\n",
        "#     initial_params = jax.jit(model.init)({\"params\": key},jnp.ones(input_pts_shape),)\n",
        "#     return model, initial_params[\"params\"]\n",
        "# #############################################\n",
        "# n_devices = jax.local_device_count()\n",
        "# key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "# model, params = initialize_model(key, (image_height * image_width, 3))\n"
      ],
      "metadata": {
        "id": "liiEWyFxuwkj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #✅\n",
        "# import jax.numpy as jnp\n",
        "# import jax\n",
        "# key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "# batch_size_no = 64\n",
        "# x = jnp.ones(shape=(batch_size_no, 32, 32, 3)) # Dummy Input\n",
        "# BATCH, image_height, image_width, channel = x.shape\n",
        "# size = image_height * image_width\n",
        "# yt = jnp.ones((size, channel))\n",
        "# model = MLPModel() # Instantiate the Model\n",
        "\n",
        "# params = model.init(rng, yt) # Initialize the parameters\n",
        "# print(type(params))\n",
        "\n",
        "# params1 = model.apply # Initialize the parameters\n",
        "# print(type(params1))\n",
        "\n",
        "# jax.tree_map(lambda x: x.shape, params) # Check the parameters\n"
      ],
      "metadata": {
        "id": "rg6S9PI0vwV_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MODEL SUMMARY { vertical-output: true }\n",
        "#✅\n",
        "# import flax.linen as nn\n",
        "# nn.tabulate(model, rng)(jnp.ones((image_height * image_width, channels)))"
      ],
      "metadata": {
        "id": "hC5jFSB-_P4u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### initialize the module"
      ],
      "metadata": {
        "id": "4o02pjAJqdjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#✅\n",
        "!python -m pip install -q -U flax\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "\n",
        "def init_train_state(model, r_key, shape, learning_rate ) -> train_state.TrainState:\n",
        "    print(shape)\n",
        "    # BATCH, image_height, image_width, cha = shape\n",
        "    # size = image_height * image_width\n",
        "    # yt = jnp.ones((size, cha))\n",
        "    init_variables = model.init(r_key, jnp.ones(shape))  # Initialize the Model\n",
        "    optimizer = optax.adam(learning_rate) # Create the optimizer\n",
        "    # Create a State\n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=init_variables['params']\n",
        "    )\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size_no = 64\n",
        "model = MLPModel() # Instantiate the Model\n",
        "key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "x = jnp.ones(shape=(batch_size_no, 28, 28, 1)) # Dummy Input\n",
        "_, image_height, image_width, channels = x.shape\n",
        "state = init_train_state( model, rng, (image_height * image_width, channels), learning_rate )\n"
      ],
      "metadata": {
        "id": "rJEuhCl5xuR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2738db1-bd43-4ff3-b612-ef8db00803c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 1)\n",
            "positional_encoding start\n",
            "(784, 1)\n",
            "(6, 784, 1)\n",
            "(2, 6, 784, 1)\n",
            "(784, 6, 2, 1)\n",
            "(784, 12)\n",
            "(784, 13)\n",
            "positional_encoding end\n",
            "network model start\n",
            "(784, 13)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 257)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 1)\n",
            "network model end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(*, logits, labels):\n",
        "    one_hot_encoded_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "    return optax.softmax_cross_entropy(\n",
        "        logits=logits, labels=one_hot_encoded_labels\n",
        "    ).mean()\n",
        "def compute_metrics(*, logits, labels):\n",
        "  loss = .5 * jnp.mean((logits - labels) ** 2)\n",
        "  loss = lax.pmean(loss, axis_name=\"batch\");print(\"ok4\")\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "  }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "f-6Gf-ee-9Jh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "@jax.jit\n",
        "def train_step(state: train_state.TrainState, batch: jnp.ndarray, rng):\n",
        "    print(batch)\n",
        "    image, label = batch\n",
        "    print(image,\"<<<image\")\n",
        "    print(label,\"<<<label\")    \n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({'params': params}, image);print(\"done1\",logits.shape)\n",
        "        loss =  .5 * jnp.mean((logits - label) ** 2);print(\"done2\",loss.shape)\n",
        "        return loss, logits\n",
        "\n",
        "    # def loss_fn(params):\n",
        "    #     model_fn = lambda x: state.apply_fn({\"params\": params}, x)\n",
        "    #     ray_origins, ray_directions = inputs\n",
        "    #     print(ray_origins)\n",
        "    #     print(ray_directions)\n",
        "    #     rgb, *_ = perform_volume_rendering(\n",
        "    #         model_fn, ray_origins, ray_directions, rng\n",
        "    #     )\n",
        "    #     return jnp.mean((rgb - targets) ** 2)  \n",
        "    print(\"ok1really\")\n",
        "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True);print(\"ok1\")\n",
        "    (_, logits), grads = gradient_fn(state.params);print(\"ok2\")\n",
        "    #train_loss, gradients_each = jax.value_and_grad(loss_fn)(state.params);print(\"ok3\")\n",
        "    grads = lax.pmean(grads,\"batch\");print(\"ok4\")\n",
        "    # grads = jnp.mean(grads);print(\"ok4\")\n",
        "    state = state.apply_gradients(grads=grads);print(\"ok5\")\n",
        "    # train_loss = jnp.mean(train_loss);print(\"ok6\")\n",
        "    metrics = compute_metrics(logits=logits, labels=label);print(\"ok7\")\n",
        "    return state, metrics\n",
        "\n",
        "parallel_train_step = jax.pmap(train_step, \"batch\")\n",
        "# parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", in_axes = (0, 0, 0))\n",
        "\n",
        "import jax\n",
        "@jax.jit\n",
        "def eval_step(state, batch):\n",
        "    image, label = batch\n",
        "    logits = state.apply_fn({'params': state.params}, image)\n",
        "    return compute_metrics(logits=logits, labels=label)\n"
      ],
      "metadata": {
        "id": "wskhkyy49MIC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### checkpoints management"
      ],
      "metadata": {
        "id": "PddyjIzYzu9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(ckpt_path, state):\n",
        "    with open(ckpt_path, \"wb\") as outfile:\n",
        "        outfile.write(msgpack_serialize(to_state_dict(state)))\n",
        "    \n",
        "\n",
        "\n",
        "def load_checkpoint(ckpt_path, ckpt_file, state):\n",
        "    ckpt_path = os.path.join(ckpt_path, ckpt_file)\n",
        "    with open(ckpt_path, \"rb\") as data_file:\n",
        "        byte_data = data_file.read()\n",
        "    return from_bytes(state, byte_data)\n",
        "\n",
        "\n",
        "def accumulate_metrics(metrics):\n",
        "    metrics = jax.device_get(metrics)\n",
        "    return {\n",
        "        k: np.mean([metric[k] for metric in metrics])\n",
        "        for k in metrics[0]\n",
        "    }"
      ],
      "metadata": {
        "id": "6e5D7TaR8os4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train & evaluation function"
      ],
      "metadata": {
        "id": "LvNe_bDIz0Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1UgWEotThxnP-Vh-h83-VcTPMkKWmgCDe #downloading MAP-DEM "
      ],
      "metadata": {
        "id": "BoQeqWgTkX3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f1cc7a-29bb-415c-f96e-34014fb3cea3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UgWEotThxnP-Vh-h83-VcTPMkKWmgCDe\n",
            "To: /content/HLSL30.020_B04_doy2021057_aid0001_43N.tif\n",
            "\r  0% 0.00/10.6M [00:00<?, ?B/s]\r100% 10.6M/10.6M [00:00<00:00, 141MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -U tifffile imagecodecs matplotlib lxml zarr fsspec\n"
      ],
      "metadata": {
        "id": "Sh6EJ9Wbsb3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19da4372-e003-4964-ea6c-1dd72cf2ea58"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.7/dist-packages (2021.11.2)\n",
            "Collecting imagecodecs\n",
            "  Downloading imagecodecs-2021.11.20-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.0 MB 79.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 43.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Collecting zarr\n",
            "  Downloading zarr-2.12.0-py3-none-any.whl (185 kB)\n",
            "\u001b[K     |████████████████████████████████| 185 kB 61.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (2022.10.0)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from tifffile) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 56.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Collecting numcodecs>=0.6.4\n",
            "  Downloading numcodecs-0.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 39.4 MB/s \n",
            "\u001b[?25hCollecting asciitree\n",
            "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
            "Collecting fasteners\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from numcodecs>=0.6.4->zarr) (0.4)\n",
            "Building wheels for collected packages: asciitree\n",
            "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5050 sha256=076d43a95f3fce3d1446e5fd87e8576a6a0cd61de873932b69e7947bd3ad1236\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/38/0def51e15add93bff3f4bf9c248b94db0839b980b8535e72a0\n",
            "Successfully built asciitree\n",
            "Installing collected packages: numcodecs, fonttools, fasteners, asciitree, zarr, matplotlib, imagecodecs\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed asciitree-0.3.3 fasteners-0.18 fonttools-4.38.0 imagecodecs-2021.11.20 matplotlib-3.5.3 numcodecs-0.10.2 zarr-2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/HLSL30.020_B04_doy2021057_aid0001_43N.tif /content/a.tif #renaming file"
      ],
      "metadata": {
        "id": "yKer8jJlBMPd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tifffile\n",
        "import imagecodecs\n",
        "from imagecodecs import imread, imwrite\n",
        "fp = r'/content/a.tif'\n",
        "image = imread(\"/content/a.tif\")\n",
        "b = image.reshape(-1,1)\n",
        "b.shape\n",
        "newsize = (28, 28)\n",
        "c = jnp.asarray(image.resize(newsize, refcheck=False)).reshape(-1,1)\n",
        "c.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFJvGVWttn1l",
        "outputId": "27eb5fef-a53f-4e24-846b-c66f4d7fbe12"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install rasterio\n"
      ],
      "metadata": {
        "id": "E7ILCMFnxUVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c016e5d3-e951-4552-8138-98df1e4ce1db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.3 MB 11.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rasterio) (57.4.0)\n",
            "Collecting affine\n",
            "  Downloading affine-2.3.1-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.21.6)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from rasterio) (2022.9.24)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio) (22.1.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio) (7.1.2)\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Collecting click-plugins\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n",
            "Installing collected packages: snuggs, cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.3.1 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.2.10 snuggs-1.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "# fp = r'/content/a.tif'\n",
        "# img = rasterio.open(fp)\n",
        "# show(image)\n",
        "# print(img.count) #to print number of bands\n",
        "newsize = (28, 28)\n",
        "def imageGRAY(argv):\n",
        "    im = imread(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "x, x1 = imageGRAY(\"/content/a.tif\")\n",
        "print(x.shape, x1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJJCBhyEmIWy",
        "outputId": "b2bc00e5-ec1c-45dc-b064-f00a7d635911"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "() (1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://live.staticflickr.com/7492/15677707699_d9d67acf9d_b.jpg -O a.jpg\n",
        "\n",
        "newsize = (28, 28)\n",
        "from PIL import Image, ImageFilter\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def imageGRAY(argv):\n",
        "    im = Image.open(argv).convert('L')\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu\n",
        "x, x1 = imageGRAY(\"/content/a.jpg\")\n",
        "print(x.shape)# mnist IMAGES are 28x28=784 pixels\n",
        "y, y1 = imageRGB(\"/content/a.jpg\")\n",
        "print(y.shape,y1.shape,\"<< y shape\",x.shape,x1.shape,\"<< y shape\")# mnist IMAGES are 28x28=784 pixels\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(x);plt.show()\n",
        "plt.imshow(y);plt.show()\n",
        "plt.imshow((y1[0]-x1).reshape(28,-1));plt.show()\n",
        "plt.imshow((y1[1]-x1).reshape(28,-1));plt.show()\n",
        "plt.imshow((y1[2]-x1).reshape(28,-1));plt.show()\n",
        "\n",
        "\n",
        "# batch = y1, x1  # jnp.ones((28*28,1)),jnp.ones((28*28,1)) OR jnp.ones((2, 28*28, 1))\n",
        "# shapea, channels = y1.shape\n",
        "# state = init_train_state( model, rng, (shapea, channels), learning_rate )\n",
        "# for e in range(150):\n",
        "#   state, metrics = train_step(state, batch, rng)\n",
        "#   print(metrics)"
      ],
      "metadata": {
        "id": "QjJ1VIiKgs0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #✅\n",
        "# import os\n",
        "# os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
        "# import jax\n",
        "# jax.devices()\n",
        "# import jax.numpy as jnp\n",
        "# from jax import pmap\n",
        "# a = jnp.arange(8*10).reshape((8, 2,5))\n",
        "# b = 2\n",
        "# print(type(a));print(a)\n",
        "# def f(x,y):a = x**2+y**2;return a\n",
        "# ff = pmap(f, in_axes=(0,None))\n",
        "# result = ff(a,b)\n",
        "# print(type(result));print(result)\n",
        "# num_devices = jax.device_count()\n",
        "\n",
        "# shape_prefix = (num_devices, 1);print(shape_prefix)"
      ],
      "metadata": {
        "id": "gcib9OwKfNO0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "download [flower dataset](https://www.kaggle.com/datasets/alxmamaev/flowers-recognition?resource=download) from kaggle."
      ],
      "metadata": {
        "id": "go6vwNDjvoSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1SynswUkxdl6B3c6Uc7Q6MhLG3XirrHd9 # downloading from google drive saved location..\n",
        "!unzip /content/archive.zip #unzipping the flower images from archive.."
      ],
      "metadata": {
        "id": "F_FW-EGUd5EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing batching of the dataset frfom the total dataset, by assuming batch size as 8 then running 50 epochs over the batch , then moving processing onto next batch>>>\n",
        "batch_size = 8\n",
        "import os\n",
        "image_dir = r'/content/flowers/daisy/'\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"c\",\".jpg\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.jpg'\n",
        "expression_b2 = bandend[1]\n",
        "total_images =  [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "total_images.sort()\n",
        "total_images_path = [os.path.join(image_dir, i) for i in total_images if i != 'outputs']\n",
        "print(len(total_images_path))\n",
        "\n"
      ],
      "metadata": {
        "id": "o724RVHZweXt",
        "outputId": "86e5a596-0a4b-49af-904c-142ae51a65d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_devices = jax.device_count()\n",
        "ddy = jnp.asarray((y1,y1,y1,y1,y1,y1,y1,y1))       # rgb images (width * height, 3)\n",
        "print(ddy.shape,\"<<< ddy shape\")\n",
        "ddx = jnp.asarray((x1,x1,x1,x1,x1,x1,x1,x1))    #gray images (width * height, 1)\n",
        "shape_prefix = (num_devices, 1);print(shape_prefix);print(ddy.shape,\"<<<< ???\")\n",
        "# ddy = ddy.reshape(shape_prefix + ddy.shape[1:]);print(ddy.shape,\"<< train_images_incorrect\")\n",
        "# ddx = ddx.reshape(shape_prefix + ddx.shape[1:]);print(ddx.shape,\"<< train_images_incorrect\")\n",
        "batch = ddy, ddx  # jnp.ones((28*28,1)),jnp.ones((28*28,1)) OR jnp.ones((2, 28*28, 1))\n",
        "print(len(batch),\"<<< batch\")\n",
        "vv, shapea, channels = ddy.shape\n",
        "######################\n",
        "rng = jax.random.PRNGKey(0)\n",
        "# dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "######################\n",
        "state = init_train_state( model, rng, (shapea, channels), learning_rate ) \n",
        "state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "\n",
        "for e in range(50):   # EPOCHS for training & updating the initiated state, metrics may show the loss in each epochs or iteration\n",
        "  dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "  state, metrics = parallel_train_step(state, batch, dropout_rngs)\n",
        "  print(\"<<✅✅✅epoc : \",e,\" complete✅✅✅>>\\n\",metrics)"
      ],
      "metadata": {
        "id": "qUpVOU52BWfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84724c0b-b52e-47f7-e862-4eef4bacc67e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 784, 3) <<< ddy shape\n",
            "(8, 1)\n",
            "(8, 784, 3) <<<< ???\n",
            "2 <<< batch\n",
            "(784, 3)\n",
            "positional_encoding start\n",
            "(784, 3)\n",
            "(6, 784, 3)\n",
            "(2, 6, 784, 3)\n",
            "(784, 6, 2, 3)\n",
            "(784, 36)\n",
            "(784, 39)\n",
            "positional_encoding end\n",
            "network model start\n",
            "(784, 39)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 259)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 1)\n",
            "network model end\n",
            "(Traced<ShapedArray(uint8[784,3])>with<DynamicJaxprTrace(level=0/2)>, Traced<ShapedArray(uint8[784,1])>with<DynamicJaxprTrace(level=0/2)>)\n",
            "Traced<ShapedArray(uint8[784,3])>with<DynamicJaxprTrace(level=0/2)> <<<image\n",
            "Traced<ShapedArray(uint8[784,1])>with<DynamicJaxprTrace(level=0/2)> <<<label\n",
            "ok1really\n",
            "ok1\n",
            "positional_encoding start\n",
            "(784, 3)\n",
            "(6, 784, 3)\n",
            "(2, 6, 784, 3)\n",
            "(784, 6, 2, 3)\n",
            "(784, 36)\n",
            "(784, 39)\n",
            "positional_encoding end\n",
            "network model start\n",
            "(784, 39)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 259)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 1)\n",
            "network model end\n",
            "done1 (784, 1)\n",
            "done2 ()\n",
            "ok2\n",
            "ok4\n",
            "ok5\n",
            "ok4\n",
            "ok7\n",
            "<<✅✅✅epoc :  0  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([6655.0317, 6655.0317, 6655.0317, 6655.0317, 6655.0317,\n",
            "                    6655.0317, 6655.0317, 6655.0317], dtype=float32)}\n",
            "<<✅✅✅epoc :  1  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([6471.528, 6471.528, 6471.528, 6471.528, 6471.528,\n",
            "                    6471.528, 6471.528, 6471.528], dtype=float32)}\n",
            "<<✅✅✅epoc :  2  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([6302.406, 6302.406, 6302.406, 6302.406, 6302.406,\n",
            "                    6302.406, 6302.406, 6302.406], dtype=float32)}\n",
            "<<✅✅✅epoc :  3  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([6138.948, 6138.948, 6138.948, 6138.948, 6138.948,\n",
            "                    6138.948, 6138.948, 6138.948], dtype=float32)}\n",
            "<<✅✅✅epoc :  4  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([5979.1836, 5979.1836, 5979.1836, 5979.1836, 5979.1836,\n",
            "                    5979.1836, 5979.1836, 5979.1836], dtype=float32)}\n",
            "<<✅✅✅epoc :  5  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([5825.593, 5825.593, 5825.593, 5825.593, 5825.593,\n",
            "                    5825.593, 5825.593, 5825.593], dtype=float32)}\n",
            "<<✅✅✅epoc :  6  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([5672.367, 5672.367, 5672.367, 5672.367, 5672.367,\n",
            "                    5672.367, 5672.367, 5672.367], dtype=float32)}\n",
            "<<✅✅✅epoc :  7  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([5518.6177, 5518.6177, 5518.6177, 5518.6177, 5518.6177,\n",
            "                    5518.6177, 5518.6177, 5518.6177], dtype=float32)}\n",
            "<<✅✅✅epoc :  8  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([5362.6875, 5362.6875, 5362.6875, 5362.6875, 5362.6875,\n",
            "                    5362.6875, 5362.6875, 5362.6875], dtype=float32)}\n",
            "<<✅✅✅epoc :  9  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([5200.558, 5200.558, 5200.558, 5200.558, 5200.558,\n",
            "                    5200.558, 5200.558, 5200.558], dtype=float32)}\n",
            "<<✅✅✅epoc :  10  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([5029.395, 5029.395, 5029.395, 5029.395, 5029.395,\n",
            "                    5029.395, 5029.395, 5029.395], dtype=float32)}\n",
            "<<✅✅✅epoc :  11  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([4853.2734, 4853.2734, 4853.2734, 4853.2734, 4853.2734,\n",
            "                    4853.2734, 4853.2734, 4853.2734], dtype=float32)}\n",
            "<<✅✅✅epoc :  12  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([4678.2954, 4678.2954, 4678.2954, 4678.2954, 4678.2954,\n",
            "                    4678.2954, 4678.2954, 4678.2954], dtype=float32)}\n",
            "<<✅✅✅epoc :  13  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([4501.109, 4501.109, 4501.109, 4501.109, 4501.109,\n",
            "                    4501.109, 4501.109, 4501.109], dtype=float32)}\n",
            "<<✅✅✅epoc :  14  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([4317.4136, 4317.4136, 4317.4136, 4317.4136, 4317.4136,\n",
            "                    4317.4136, 4317.4136, 4317.4136], dtype=float32)}\n",
            "<<✅✅✅epoc :  15  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([4129.0884, 4129.0884, 4129.0884, 4129.0884, 4129.0884,\n",
            "                    4129.0884, 4129.0884, 4129.0884], dtype=float32)}\n",
            "<<✅✅✅epoc :  16  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([3934.7065, 3934.7065, 3934.7065, 3934.7065, 3934.7065,\n",
            "                    3934.7065, 3934.7065, 3934.7065], dtype=float32)}\n",
            "<<✅✅✅epoc :  17  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([3728.352, 3728.352, 3728.352, 3728.352, 3728.352,\n",
            "                    3728.352, 3728.352, 3728.352], dtype=float32)}\n",
            "<<✅✅✅epoc :  18  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([3506.6248, 3506.6248, 3506.6248, 3506.6248, 3506.6248,\n",
            "                    3506.6248, 3506.6248, 3506.6248], dtype=float32)}\n",
            "<<✅✅✅epoc :  19  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([3269.2935, 3269.2935, 3269.2935, 3269.2935, 3269.2935,\n",
            "                    3269.2935, 3269.2935, 3269.2935], dtype=float32)}\n",
            "<<✅✅✅epoc :  20  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([3018.828, 3018.828, 3018.828, 3018.828, 3018.828,\n",
            "                    3018.828, 3018.828, 3018.828], dtype=float32)}\n",
            "<<✅✅✅epoc :  21  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([2758.9185, 2758.9185, 2758.9185, 2758.9185, 2758.9185,\n",
            "                    2758.9185, 2758.9185, 2758.9185], dtype=float32)}\n",
            "<<✅✅✅epoc :  22  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([2493.4668, 2493.4668, 2493.4668, 2493.4668, 2493.4668,\n",
            "                    2493.4668, 2493.4668, 2493.4668], dtype=float32)}\n",
            "<<✅✅✅epoc :  23  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([2225.4578, 2225.4578, 2225.4578, 2225.4578, 2225.4578,\n",
            "                    2225.4578, 2225.4578, 2225.4578], dtype=float32)}\n",
            "<<✅✅✅epoc :  24  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([1955.7684, 1955.7684, 1955.7684, 1955.7684, 1955.7684,\n",
            "                    1955.7684, 1955.7684, 1955.7684], dtype=float32)}\n",
            "<<✅✅✅epoc :  25  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([1686.9192, 1686.9192, 1686.9192, 1686.9192, 1686.9192,\n",
            "                    1686.9192, 1686.9192, 1686.9192], dtype=float32)}\n",
            "<<✅✅✅epoc :  26  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([1420.6642, 1420.6642, 1420.6642, 1420.6642, 1420.6642,\n",
            "                    1420.6642, 1420.6642, 1420.6642], dtype=float32)}\n",
            "<<✅✅✅epoc :  27  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([1158.729, 1158.729, 1158.729, 1158.729, 1158.729,\n",
            "                    1158.729, 1158.729, 1158.729], dtype=float32)}\n",
            "<<✅✅✅epoc :  28  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([905.6315, 905.6315, 905.6315, 905.6315, 905.6315,\n",
            "                    905.6315, 905.6315, 905.6315], dtype=float32)}\n",
            "<<✅✅✅epoc :  29  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([666.8211, 666.8211, 666.8211, 666.8211, 666.8211,\n",
            "                    666.8211, 666.8211, 666.8211], dtype=float32)}\n",
            "<<✅✅✅epoc :  30  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([451.3486, 451.3486, 451.3486, 451.3486, 451.3486,\n",
            "                    451.3486, 451.3486, 451.3486], dtype=float32)}\n",
            "<<✅✅✅epoc :  31  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([267.46298, 267.46298, 267.46298, 267.46298, 267.46298,\n",
            "                    267.46298, 267.46298, 267.46298], dtype=float32)}\n",
            "<<✅✅✅epoc :  32  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([125.50929, 125.50929, 125.50929, 125.50929, 125.50929,\n",
            "                    125.50929, 125.50929, 125.50929], dtype=float32)}\n",
            "<<✅✅✅epoc :  33  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([35.962612, 35.962612, 35.962612, 35.962612, 35.962612,\n",
            "                    35.962612, 35.962612, 35.962612], dtype=float32)}\n",
            "<<✅✅✅epoc :  34  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([6.4524302, 6.4524302, 6.4524302, 6.4524302, 6.4524302,\n",
            "                    6.4524302, 6.4524302, 6.4524302], dtype=float32)}\n",
            "<<✅✅✅epoc :  35  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([36.788937, 36.788937, 36.788937, 36.788937, 36.788937,\n",
            "                    36.788937, 36.788937, 36.788937], dtype=float32)}\n",
            "<<✅✅✅epoc :  36  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([111.97163, 111.97163, 111.97163, 111.97163, 111.97163,\n",
            "                    111.97163, 111.97163, 111.97163], dtype=float32)}\n",
            "<<✅✅✅epoc :  37  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([203.62277, 203.62277, 203.62277, 203.62277, 203.62277,\n",
            "                    203.62277, 203.62277, 203.62277], dtype=float32)}\n",
            "<<✅✅✅epoc :  38  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([281.33237, 281.33237, 281.33237, 281.33237, 281.33237,\n",
            "                    281.33237, 281.33237, 281.33237], dtype=float32)}\n",
            "<<✅✅✅epoc :  39  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([324.09778, 324.09778, 324.09778, 324.09778, 324.09778,\n",
            "                    324.09778, 324.09778, 324.09778], dtype=float32)}\n",
            "<<✅✅✅epoc :  40  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([325.843, 325.843, 325.843, 325.843, 325.843, 325.843,\n",
            "                    325.843, 325.843], dtype=float32)}\n",
            "<<✅✅✅epoc :  41  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([292.47073, 292.47073, 292.47073, 292.47073, 292.47073,\n",
            "                    292.47073, 292.47073, 292.47073], dtype=float32)}\n",
            "<<✅✅✅epoc :  42  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([236.7151, 236.7151, 236.7151, 236.7151, 236.7151,\n",
            "                    236.7151, 236.7151, 236.7151], dtype=float32)}\n",
            "<<✅✅✅epoc :  43  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([173.23904, 173.23904, 173.23904, 173.23904, 173.23904,\n",
            "                    173.23904, 173.23904, 173.23904], dtype=float32)}\n",
            "<<✅✅✅epoc :  44  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([113.726395, 113.726395, 113.726395, 113.726395,\n",
            "                    113.726395, 113.726395, 113.726395, 113.726395],                   dtype=float32)}\n",
            "<<✅✅✅epoc :  45  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([65.0054, 65.0054, 65.0054, 65.0054, 65.0054, 65.0054,\n",
            "                    65.0054, 65.0054], dtype=float32)}\n",
            "<<✅✅✅epoc :  46  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([31.085829, 31.085829, 31.085829, 31.085829, 31.085829,\n",
            "                    31.085829, 31.085829, 31.085829], dtype=float32)}\n",
            "<<✅✅✅epoc :  47  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([11.862814, 11.862814, 11.862814, 11.862814, 11.862814,\n",
            "                    11.862814, 11.862814, 11.862814], dtype=float32)}\n",
            "<<✅✅✅epoc :  48  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([5.331434, 5.331434, 5.331434, 5.331434, 5.331434,\n",
            "                    5.331434, 5.331434, 5.331434], dtype=float32)}\n",
            "<<✅✅✅epoc :  49  complete✅✅✅>>\n",
            " {'loss': ShardedDeviceArray([8.445503, 8.445503, 8.445503, 8.445503, 8.445503,\n",
            "                    8.445503, 8.445503, 8.445503], dtype=float32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❌❌❌❌❌❌❌doesnot work >>>"
      ],
      "metadata": {
        "id": "w5h1ioy9Rbtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 32\n",
        "import datasets\n",
        "train_images, train_labels, test_images, test_labels = datasets.mnist()\n",
        "num_train = train_images.shape[0]\n",
        "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
        "num_batches = num_complete_batches + bool(leftover)\n",
        "import jax\n",
        "num_devices = jax.device_count()\n",
        "def data_stream():\n",
        "  key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "  while True:\n",
        "    perm = jax.random.permutation(rng, num_train); print(perm.shape)\n",
        "    for i in range(num_batches):\n",
        "      batch_idx = perm[i * batch_size:(i + 1) * batch_size]; print(batch_idx)\n",
        "      images, labels = train_images[batch_idx], train_labels[batch_idx]; print(images.shape,\"<< train_images_incorrect\");print(labels.shape,\"<< train_images_correct\")\n",
        "      # For this SPMD example, we reshape the data batch dimension into two\n",
        "      # batch dimensions, one of which is mapped over parallel devices.\n",
        "      batch_size_per_device, ragged = divmod(images.shape[0], num_devices);print(batch_size_per_device,\"<<< batch_size_per_device\")\n",
        "      if ragged:\n",
        "        msg = \"batch size must be divisible by device count, got {} and {}.\"\n",
        "        raise ValueError(msg.format(batch_size, num_devices))\n",
        "      shape_prefix = (num_devices, batch_size_per_device);print(shape_prefix)\n",
        "      images = images.reshape(shape_prefix + images.shape[1:]);print(images.shape,\"<< train_images_incorrect\")\n",
        "      labels = labels.reshape(shape_prefix + labels.shape[1:]);print(labels.shape,\"<< train_images_correct\")\n",
        "      return images, labels\n",
        "batches = data_stream()\n"
      ],
      "metadata": {
        "id": "i7oLNzL4U5m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_batches = tf.data.experimental.cardinality(train_dataset)\n",
        "train_datagen = iter(tfds.as_numpy(train_dataset))\n",
        "batch = next(train_datagen)\n",
        "batch = jnp.ones((28*28,1)),jnp.ones((28*28,1))  # jnp.ones((28*28,1)),jnp.ones((28*28,1)) OR jnp.ones((2, 28*28, 1))\n",
        "# state = flax.jax_utils.replicate(state)\n",
        "state = init_train_state( model, rng, (image_height * image_width, channels), learning_rate )\n",
        "for e in range(50):\n",
        "  state, metrics = train_step(state, batch, rng)\n",
        "  print(metrics)"
      ],
      "metadata": {
        "id": "_W-fMd-P07bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_and_evaluate(train_dataset, eval_dataset, test_dataset, state: train_state.TrainState, epochs: int,):\n",
        "    num_train_batches = tf.data.experimental.cardinality(train_dataset)\n",
        "    num_eval_batches = tf.data.experimental.cardinality(eval_dataset)\n",
        "    num_test_batches = tf.data.experimental.cardinality(test_dataset)\n",
        "   \n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        best_eval_loss = 1e6\n",
        "        # ============== Training ============== #\n",
        "        train_batch_metrics = []\n",
        "        train_datagen = iter(tfds.as_numpy(train_dataset))\n",
        "        for batch_idx in range(num_train_batches):\n",
        "            batch = next(train_datagen)\n",
        "            state, metrics = train_step(state, batch, rng)\n",
        "            train_batch_metrics.append(metrics)\n",
        "        train_batch_metrics = accumulate_metrics(train_batch_metrics)\n",
        "        print('TRAIN (%d/%d): Loss: %.4f' % (\n",
        "                epoch, epochs, train_batch_metrics['loss'],\n",
        "              ))\n",
        "        # ============== Validation ============= #\n",
        "        eval_batch_metrics = []\n",
        "        eval_datagen = iter(tfds.as_numpy(eval_dataset))\n",
        "        for batch_idx in range(num_eval_batches):\n",
        "            batch = next(eval_datagen)\n",
        "            metrics = eval_step(state, batch)\n",
        "            eval_batch_metrics.append(metrics)\n",
        "        eval_batch_metrics = accumulate_metrics(eval_batch_metrics)\n",
        "        print('EVAL (%d/%d):  Loss: %.4f\\n' % (\n",
        "                epoch, epochs, eval_batch_metrics['loss'],\n",
        "              ))    \n",
        "\n",
        "        if eval_batch_metrics['loss'] < best_eval_loss:\n",
        "            save_checkpoint(\"checkpoint.msgpack\", state)\n",
        "\n",
        "    restored_state = load_checkpoint(\"checkpoint.msgpack\", state)\n",
        "    test_batch_metrics = []\n",
        "    test_datagen = iter(tfds.as_numpy(test_dataset))\n",
        "    for batch_idx in range(num_test_batches):\n",
        "        batch = next(test_datagen)\n",
        "        metrics = eval_step(restored_state, batch)\n",
        "        test_batch_metrics.append(metrics)\n",
        "    \n",
        "    test_batch_metrics = accumulate_metrics(test_batch_metrics)\n",
        "    print(\n",
        "        'Test: Loss: %.4f,' % (\n",
        "            test_batch_metrics['loss'],\n",
        "        )\n",
        "    )\n",
        "    # Log Metrics to Weights & Biases\n",
        "    history = {\n",
        "        \"Train Loss\": train_batch_metrics['loss'],\n",
        "        \"Validation Loss\": eval_batch_metrics['loss'],\n",
        "    }\n",
        "    return state, restored_state, history\n",
        "\n"
      ],
      "metadata": {
        "id": "3wWR27rO-yGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loading 'mnist' data"
      ],
      "metadata": {
        "id": "FzqGlqVxz5mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title @inproceedings{zhou2017scene, title={Scene Parsing through ADE20K Dataset}, author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio}, booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, year={2017} }\n",
        "#✅✅\n",
        "import tensorflow_datasets as tfds\n",
        "(full_train_set, test_dataset), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "validation_split = 0.2\n",
        "def normalize_img(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.\n",
        "    return image, label\n",
        "\n"
      ],
      "metadata": {
        "id": "DNQMoyp_HPM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#✅✅\n",
        "import tensorflow as tf\n",
        "batch_size = 64\n",
        "full_train_set = full_train_set.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "num_data = tf.data.experimental.cardinality(full_train_set).numpy()\n",
        "print(\"Total number of data points:\", num_data)\n",
        "train_dataset = full_train_set.take(num_data * (1 - validation_split))\n",
        "val_dataset = full_train_set.take(num_data * (validation_split))\n",
        "print(\"Number of train data points:\",tf.data.experimental.cardinality(train_dataset).numpy())\n",
        "print(\"Number of val data points:\",tf.data.experimental.cardinality(val_dataset).numpy())\n",
        "#############TRAIN##################\n",
        "train_dataset = train_dataset.cache();print(len(train_dataset))\n",
        "train_dataset = train_dataset.shuffle(tf.data.experimental.cardinality(train_dataset).numpy());print(train_dataset)\n",
        "train_dataset = train_dataset.batch(batch_size);print(train_dataset)\n",
        "#############TRAIN(EVALUATE)##################\n",
        "val_dataset = val_dataset.cache()\n",
        "val_dataset = val_dataset.shuffle(tf.data.experimental.cardinality(val_dataset).numpy())\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "#############TEST##################\n",
        "test_dataset = test_dataset.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "print(\"Number of test data points:\",tf.data.experimental.cardinality(test_dataset).numpy())\n",
        "test_dataset = test_dataset.cache()\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "g7_nuCh5b12e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_batches = tf.data.experimental.cardinality(train_dataset)\n",
        "num_train_batches"
      ],
      "metadata": {
        "id": "atLCxN738JSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run inferences"
      ],
      "metadata": {
        "id": "CbgbyxgY0AaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# state, inference_state, history = train_and_evaluate(state, parallelized_train_step, eval_step)\n",
        "\n",
        "\n",
        "# train_dataset, eval_dataset, test_dataset, state, epochs\n",
        "from tqdm.notebook import tqdm\n",
        "epochs = 15\n",
        "state, best_state, history = train_and_evaluate(\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    test_dataset,\n",
        "    state,\n",
        "    epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "ABcUcSG65REH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "_ULguI_OmYwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# go"
      ],
      "metadata": {
        "id": "T4WTGhNs0rep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import imageio\n",
        "import requests\n",
        "from typing import Any\n",
        "import ipywidgets as widgets\n",
        "from functools import partial\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "!pip3 install -q -U flax\n",
        "\n",
        "import jax\n",
        "import flax\n",
        "import optax\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n"
      ],
      "metadata": {
        "id": "_pHA9kuNvNf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://www.kaggle.com/code/odins0n/jax-flax-tf-data-vision-transformers-tutorial\n",
        "\n",
        "\n",
        "# Detect if Kaggle Notebook has access to TPUs or not\n",
        "if 'TPU_NAME' in os.environ:\n",
        "    import requests\n",
        "    if 'TPU_DRIVER_MODE' not in globals():\n",
        "        url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475/requestversion/tpu_driver_nightly'\n",
        "        resp = requests.post(url)\n",
        "        TPU_DRIVER_MODE = 1\n",
        "    from jax.config import config\n",
        "    jax_xla_backend = \"tpu_driver\"\n",
        "    jax_backend_target = os.environ['TPU_NAME']\n",
        "    print(\"TPU DETECTED!\")\n",
        "    print('Registered TPU:', jax_backend_target)\n",
        "\n",
        "\n",
        "# Detect if Google Colab Notebook has access to TPUs or not\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ:\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "\n",
        "else:\n",
        "    print('No TPU detected.')\n",
        "\n",
        "\n",
        "DEVICE_COUNT = len(jax.local_devices())\n",
        "TPU = DEVICE_COUNT==8\n",
        "\n",
        "\n",
        "if TPU:\n",
        "    print(\"8 cores of TPU ( Local devices in Jax ):\")\n",
        "    print('\\n'.join(map(str,jax.local_devices())))\n"
      ],
      "metadata": {
        "id": "eaTPv-kSvMA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# sync all experiment configs with Weights and Biases\n",
        "\n",
        "near_bound = 2. # Near Bound of sample space for 3d points\n",
        "far_bound = 6.  # Far Bound of sample space for 3d points\n",
        "batch_size = int(1e4) # Batch Size\n",
        "num_sample_points = 256 # Number of points to be sampled across the volume\n",
        "epsilon = 1e10 # Hyperparameter for volume rendering\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "positional_encoding_dims = 3 # Number of positional encodings applied\n",
        "num_dense_layers = 5 # Number of dense layers in MLP\n",
        "dense_layer_width = 256 # Dimentionality of dense layers' output space \n",
        "learning_rate = 5e-4 # Learning Rate\n",
        "train_epochs = 1000 # Number of training epochs\n",
        "plot_interval = 100 # Epoch interval for plotting results during training\n"
      ],
      "metadata": {
        "id": "Dj-uVwfWvJmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encoding_dims = 3 # Number of positional encodings applied\n",
        "def positional_encoding(inputs):\n",
        "    batch_size, _ = inputs.shape;                                                   print(inputs.shape)\n",
        "    # Applying vmap transform to vectorize the multiplication operation\n",
        "    f = jax.vmap(lambda x: inputs * 2.0 ** x)(jnp.arange(positional_encoding_dims));print(f.shape)\n",
        "    fy = jnp.stack([jnp.sin(f), jnp.cos(f)]);                                       print(fy)\n",
        "    fy = fy.swapaxes(0, 2).reshape([batch_size, -1]);                               print(fy)\n",
        "    fy = jnp.concatenate([inputs, fy], axis=-1);                                    print(fy)\n",
        "    return fy\n"
      ],
      "metadata": {
        "id": "tqiAfLsYvFdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(10000, 3)\n",
        "\n",
        "\n",
        "(3, 10000, 3)\n",
        "\n",
        "Traced<ShapedArray(float32[2,3,10000,3])>with<DynamicJaxprTrace(level=0/1)>\n",
        "\n",
        "Traced<ShapedArray(float32[10000,18])>with<DynamicJaxprTrace(level=0/1)>\n",
        "\n",
        "Traced<ShapedArray(float32[10000,21])>with<DynamicJaxprTrace(level=0/1)>"
      ],
      "metadata": {
        "id": "uR_Bu2DpbChW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test mnist pmap gpu 1000 epocs runtime 9 minutes"
      ],
      "metadata": {
        "id": "cq1KuyqhumRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile datasets.py\n",
        "# Copyright 2018 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Datasets used in examples.\"\"\"\n",
        "\n",
        "\n",
        "import array\n",
        "import gzip\n",
        "import os\n",
        "from os import path\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "_DATA = \"/tmp/jax_example_data/\"\n",
        "\n",
        "\n",
        "def _download(url, filename):\n",
        "  \"\"\"Download a url to a file in the JAX data temp directory.\"\"\"\n",
        "  if not path.exists(_DATA):\n",
        "    os.makedirs(_DATA)\n",
        "  out_file = path.join(_DATA, filename)\n",
        "  if not path.isfile(out_file):\n",
        "    urllib.request.urlretrieve(url, out_file)\n",
        "    print(f\"downloaded {url} to {_DATA}\")\n",
        "\n",
        "\n",
        "def _partial_flatten(x):\n",
        "  \"\"\"Flatten all but the first dimension of an ndarray.\"\"\"\n",
        "  return np.reshape(x, (x.shape[0], -1))\n",
        "\n",
        "\n",
        "def _one_hot(x, k, dtype=np.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return np.array(x[:, None] == np.arange(k), dtype)\n",
        "\n",
        "\n",
        "def mnist_raw():\n",
        "  \"\"\"Download and parse the raw MNIST dataset.\"\"\"\n",
        "  # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
        "  base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
        "\n",
        "  def parse_labels(filename):\n",
        "    with gzip.open(filename, \"rb\") as fh:\n",
        "      _ = struct.unpack(\">II\", fh.read(8))\n",
        "      return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
        "\n",
        "  def parse_images(filename):\n",
        "    with gzip.open(filename, \"rb\") as fh:\n",
        "      _, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "      return np.array(array.array(\"B\", fh.read()),\n",
        "                      dtype=np.uint8).reshape(num_data, rows, cols)\n",
        "\n",
        "  for filename in [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n",
        "                   \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]:\n",
        "    _download(base_url + filename, filename)\n",
        "\n",
        "  train_images = parse_images(path.join(_DATA, \"train-images-idx3-ubyte.gz\"))\n",
        "  train_labels = parse_labels(path.join(_DATA, \"train-labels-idx1-ubyte.gz\"))\n",
        "  test_images = parse_images(path.join(_DATA, \"t10k-images-idx3-ubyte.gz\"))\n",
        "  test_labels = parse_labels(path.join(_DATA, \"t10k-labels-idx1-ubyte.gz\"))\n",
        "\n",
        "  return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "def mnist(permute_train=False):\n",
        "  \"\"\"Download, parse and process MNIST data to unit scale and one-hot labels.\"\"\"\n",
        "  train_images, train_labels, test_images, test_labels = mnist_raw()\n",
        "\n",
        "  train_images = _partial_flatten(train_images) / np.float32(255.)\n",
        "  test_images = _partial_flatten(test_images) / np.float32(255.)\n",
        "  train_labels = _one_hot(train_labels, 10)\n",
        "  test_labels = _one_hot(test_labels, 10)\n",
        "\n",
        "  if permute_train:\n",
        "    perm = np.random.RandomState(0).permutation(train_images.shape[0])\n",
        "    train_images = train_images[perm]\n",
        "    train_labels = train_labels[perm]\n",
        "\n",
        "  return train_images, train_labels, test_images, test_labels"
      ],
      "metadata": {
        "id": "pn2wusArvMza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[datasets](https://github.com/google/jax/blob/main/examples/datasets.py) link\n",
        "\n",
        "[spmd_mnist_classifier_fromscratch](https://github.com/google/jax/blob/main/examples/spmd_mnist_classifier_fromscratch.py) link"
      ],
      "metadata": {
        "id": "mBuFyBnWy9S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2018 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"An MNIST example with single-program multiple-data (SPMD) data parallelism.\n",
        "\n",
        "The aim here is to illustrate how to use JAX's `pmap` to express and execute\n",
        "SPMD programs for data parallelism along a batch dimension, while also\n",
        "minimizing dependencies by avoiding the use of higher-level layers and\n",
        "optimizers libraries.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "\n",
        "import jax\n",
        "from jax import jit, grad, pmap\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax.tree_util import tree_map\n",
        "from jax import lax\n",
        "import jax.numpy as jnp\n",
        "import datasets\n",
        "from google.colab import output\n",
        "\n",
        "\n",
        "def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):\n",
        "  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
        "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
        "\n",
        "def predict(params, inputs):\n",
        "  activations = inputs\n",
        "  for w, b in params[:-1]:\n",
        "    outputs = jnp.dot(activations, w) + b\n",
        "    activations = jnp.tanh(outputs)\n",
        "\n",
        "  final_w, final_b = params[-1]\n",
        "  logits = jnp.dot(activations, final_w) + final_b\n",
        "  return logits - logsumexp(logits, axis=1, keepdims=True)\n",
        "\n",
        "def loss(params, batch):\n",
        "  inputs, targets = batch\n",
        "  preds = predict(params, inputs)\n",
        "  return -jnp.mean(jnp.sum(preds * targets, axis=1))\n",
        "\n",
        "@jit\n",
        "def accuracy(params, batch):\n",
        "  inputs, targets = batch\n",
        "  target_class = jnp.argmax(targets, axis=1)\n",
        "  predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n",
        "  return jnp.mean(predicted_class == target_class)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  layer_sizes = [784, 1024, 1024, 10]\n",
        "  param_scale = 0.1\n",
        "  step_size = 0.001\n",
        "  num_epochs = 1000\n",
        "  batch_size = 128\n",
        "\n",
        "  train_images, train_labels, test_images, test_labels = datasets.mnist()\n",
        "  num_train = train_images.shape[0]\n",
        "  num_complete_batches, leftover = divmod(num_train, batch_size)\n",
        "  num_batches = num_complete_batches + bool(leftover)\n",
        "\n",
        "  # For this manual SPMD example, we get the number of devices (e.g. GPUs or\n",
        "  # TPU cores) that we're using, and use it to reshape data minibatches.\n",
        "  num_devices = jax.device_count()\n",
        "  def data_stream():\n",
        "    rng = npr.RandomState(0)\n",
        "    while True:\n",
        "      perm = rng.permutation(num_train)\n",
        "      for i in range(num_batches):\n",
        "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "        images, labels = train_images[batch_idx], train_labels[batch_idx]\n",
        "        # For this SPMD example, we reshape the data batch dimension into two\n",
        "        # batch dimensions, one of which is mapped over parallel devices.\n",
        "        batch_size_per_device, ragged = divmod(images.shape[0], num_devices)\n",
        "        if ragged:\n",
        "          msg = \"batch size must be divisible by device count, got {} and {}.\"\n",
        "          raise ValueError(msg.format(batch_size, num_devices))\n",
        "        shape_prefix = (num_devices, batch_size_per_device)\n",
        "        images = images.reshape(shape_prefix + images.shape[1:])\n",
        "        labels = labels.reshape(shape_prefix + labels.shape[1:])\n",
        "        yield images, labels\n",
        "  batches = data_stream()\n",
        "\n",
        "  @partial(pmap, axis_name='batch')\n",
        "  def spmd_update(params, batch):\n",
        "    grads = grad(loss)(params, batch)\n",
        "    # We compute the total gradients, summing across the device-mapped axis,\n",
        "    # using the `lax.psum` SPMD primitive, which does a fast all-reduce-sum.\n",
        "    grads = [(lax.psum(dw, 'batch'), lax.psum(db, 'batch')) for dw, db in grads]\n",
        "    return [(w - step_size * dw, b - step_size * db)\n",
        "            for (w, b), (dw, db) in zip(params, grads)]\n",
        "\n",
        "  # We replicate the parameters so that the constituent arrays have a leading\n",
        "  # dimension of size equal to the number of devices we're pmapping over.\n",
        "  init_params = init_random_params(param_scale, layer_sizes)\n",
        "  replicate_array = lambda x: np.broadcast_to(x, (num_devices,) + x.shape)\n",
        "  replicated_params = tree_map(replicate_array, init_params)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_batches):\n",
        "      replicated_params = spmd_update(replicated_params, next(batches))\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # We evaluate using the jitted `accuracy` function (not using pmap) by\n",
        "    # grabbing just one of the replicated parameter values.\n",
        "    params = tree_map(lambda x: x[0], replicated_params)\n",
        "    train_acc = accuracy(params, (train_images, train_labels))\n",
        "    test_acc = accuracy(params, (test_images, test_labels))\n",
        "    output.clear() #to_clear_the_output_console_everytime\n",
        "    print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
        "    print(f\"Training set accuracy {train_acc}\")\n",
        "    print(f\"Test set accuracy {test_acc}\")"
      ],
      "metadata": {
        "id": "aVJlb1-tuorJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}