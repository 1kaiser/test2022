{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y9ZP-oZjoATD",
        "SI1aeBA-dCqC",
        "DllYcUtgovO5",
        "4o02pjAJqdjz",
        "PddyjIzYzu9D",
        "LvNe_bDIz0Wy",
        "w5h1ioy9Rbtl",
        "FzqGlqVxz5mJ",
        "CbgbyxgY0AaN",
        "T4WTGhNs0rep",
        "cq1KuyqhumRh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2022/blob/main/MLPCopy_of_Untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  run from here"
      ],
      "metadata": {
        "id": "Y9ZP-oZjoATD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI1aeBA-dCqC"
      },
      "source": [
        "### Model and training code\n",
        "\n",
        "Our model is a coordinate-based multilayer perceptron. In this example, for each input image coordinate $(x,y)$, the model predicts the associated color $(r,g,b)$ or any $(gray)$.\n",
        "\n",
        "![Network diagram](https://user-images.githubusercontent.com/3310961/85066930-ad444580-b164-11ea-9cc0-17494679e71f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[--xla_force_host_platform_device_count](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html#:~:text=When%20running%20on-,CPU,-you%20can%20always)"
      ],
      "metadata": {
        "id": "Zehuof-K_cZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "import os\n",
        "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
        "import jax\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "rUeQSSu5_Jwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d844ee-6d8b-4e34-d718-e9792ddffa94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[CpuDevice(id=0),\n",
              " CpuDevice(id=1),\n",
              " CpuDevice(id=2),\n",
              " CpuDevice(id=3),\n",
              " CpuDevice(id=4),\n",
              " CpuDevice(id=5),\n",
              " CpuDevice(id=6),\n",
              " CpuDevice(id=7)]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(inputs):\n",
        "    print(\"positional_encoding start\")\n",
        "    batch_size, _ = inputs.shape;print(inputs.shape)\n",
        "    inputs_freq = jax.vmap(lambda x: inputs * 2.0 ** x)(jnp.arange(positional_encoding_dims));print(inputs_freq.shape)\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)]);print(x.shape)\n",
        "    x = x.swapaxes(0, 2);print(x.shape)\n",
        "    x = x.reshape([batch_size, -1]);print(x.shape)\n",
        "    x = jnp.concatenate([inputs, x], axis=-1);print(x.shape)\n",
        "    print(\"positional_encoding end\")\n",
        "    return x\n",
        "\n",
        "# y = np.ones((256, 256, 3))\n",
        "# print(y.shape)\n",
        "# image_height, image_width, cha = y.shape\n",
        "# size = image_height * image_width\n",
        "# yt = np.ones((size, cha))\n",
        "# print(yt.shape)\n",
        "# positional_encoding(yt)\n"
      ],
      "metadata": {
        "id": "z9aPWpu5iJ1I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# positional_encoding_vmap = jax.vmap(positional_encoding)\n",
        "# ######################################\n",
        "# y = jnp.ones((8, 256, 256, 3))\n",
        "# print(y.shape)\n",
        "# batchsize, image_height, image_width, cha = y.shape\n",
        "# size = image_height * image_width\n",
        "# yt = jnp.ones((batchsize, size, cha))\n",
        "# ######################################\n",
        "# print(\"vmap >>>\")\n",
        "# print(positional_encoding_vmap(yt).shape)\n",
        "\n",
        "# positional_encoding_pmap = jax.pmap(positional_encoding)\n",
        "# print(\"pmap >>>\")\n",
        "# print(positional_encoding_pmap(yt).shape)"
      ],
      "metadata": {
        "id": "YEAZBplw6bKG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP MODEL\n",
        "Basically, passing input points through a simple Fourier Feature Mapping enables an MLP to learn high-frequency functions (such as an RGB image) in low-dimensional problem domains (such as a 2D coordinate of pixels)."
      ],
      "metadata": {
        "id": "DllYcUtgovO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -q -U flax\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import flax\n",
        "import optax\n",
        "from typing import Any\n",
        "\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "num_dense_layers = 8 # Number of dense layers in MLP\n",
        "dense_layer_width = 256 # Dimentionality of dense layers' output space \n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "        x = positional_encoding(input_points) if self.apply_positional_encoding else input_points\n",
        "        print(\"network model start\")\n",
        "        print(x.shape)\n",
        "        for i in range(num_dense_layers):\n",
        "            x = nn.Dense(\n",
        "                dense_layer_width,\n",
        "                dtype=self.dtype,\n",
        "                precision=self.precision\n",
        "                )(x)\n",
        "            x = nn.relu(x)\n",
        "            x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "            print(x.shape)\n",
        "  \n",
        "        x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "        print(x.shape)\n",
        "        print(\"network model end\")\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "VRkotxnvvHrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b35c3d90-c2ab-42c7-cfca-374784a8c6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185 kB 4.7 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 237 kB 31.1 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145 kB 74.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51 kB 6.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85 kB 3.9 MB/s \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "# !wget https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz\n",
        "# from google.colab import output\n",
        "# output.clear() #to_clear_the_output_console_everytime\n",
        "# import jax.numpy as jnp\n",
        "\n",
        "# data = jnp.load(\"tiny_nerf_data.npz\")\n",
        "# images = data[\"images\"]\n",
        "# poses = data[\"poses\"]\n",
        "# focal = float(data[\"focal\"])\n",
        "# _, image_height, image_width, _ = images.shape\n",
        "# train_images, train_poses = images[:100], poses[:100]\n",
        "# val_image, val_pose = images[101], poses[101]\n",
        "# ############################################\n",
        "# def initialize_model(key, input_pts_shape):\n",
        "#     model = MLPModel()\n",
        "#     initial_params = jax.jit(model.init)({\"params\": key},jnp.ones(input_pts_shape),)\n",
        "#     return model, initial_params[\"params\"]\n",
        "# #############################################\n",
        "# n_devices = jax.local_device_count()\n",
        "# key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "# model, params = initialize_model(key, (image_height * image_width, 3))\n"
      ],
      "metadata": {
        "id": "liiEWyFxuwkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #âœ…\n",
        "# import jax.numpy as jnp\n",
        "# import jax\n",
        "# key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "# batch_size_no = 64\n",
        "# x = jnp.ones(shape=(batch_size_no, 32, 32, 3)) # Dummy Input\n",
        "# BATCH, image_height, image_width, channel = x.shape\n",
        "# size = image_height * image_width\n",
        "# yt = jnp.ones((size, channel))\n",
        "# model = MLPModel() # Instantiate the Model\n",
        "\n",
        "# params = model.init(rng, yt) # Initialize the parameters\n",
        "# print(type(params))\n",
        "\n",
        "# params1 = model.apply # Initialize the parameters\n",
        "# print(type(params1))\n",
        "\n",
        "# jax.tree_map(lambda x: x.shape, params) # Check the parameters\n"
      ],
      "metadata": {
        "id": "rg6S9PI0vwV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MODEL SUMMARY { vertical-output: true }\n",
        "#âœ…\n",
        "# import flax.linen as nn\n",
        "# nn.tabulate(model, rng)(jnp.ones((image_height * image_width, channels)))"
      ],
      "metadata": {
        "id": "hC5jFSB-_P4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### initialize the module"
      ],
      "metadata": {
        "id": "4o02pjAJqdjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -q -U flax\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "\n",
        "def init_train_state(model, r_key, shape, learning_rate ) -> train_state.TrainState:\n",
        "    print(shape)\n",
        "    # BATCH, image_height, image_width, cha = shape\n",
        "    # size = image_height * image_width\n",
        "    # yt = jnp.ones((size, cha))\n",
        "    init_variables = model.init(r_key, jnp.ones(shape))  # Initialize the Model\n",
        "    optimizer = optax.adam(learning_rate) # Create the optimizer\n",
        "    # Create a State\n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=init_variables['params']\n",
        "    )\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size_no = 64\n",
        "model = MLPModel() # Instantiate the Model\n",
        "key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "x = jnp.ones(shape=(batch_size_no, 28, 28, 1)) # Dummy Input\n",
        "_, image_height, image_width, channels = x.shape\n",
        "state = init_train_state( model, rng, (image_height * image_width, channels), learning_rate )\n"
      ],
      "metadata": {
        "id": "rJEuhCl5xuR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(*, logits, labels):\n",
        "    one_hot_encoded_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "    return optax.softmax_cross_entropy(\n",
        "        logits=logits, labels=one_hot_encoded_labels\n",
        "    ).mean()\n",
        "def compute_metrics(*, logits, labels):\n",
        "  loss = .5 * jnp.mean((logits - labels) ** 2)\n",
        "  loss = lax.pmean(loss, axis_name=\"batch\");print(\"ok4\")\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "  }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "f-6Gf-ee-9Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "@jax.jit\n",
        "def train_step(state: train_state.TrainState, batch: jnp.ndarray, rng):\n",
        "    print(batch)\n",
        "    image, label = batch\n",
        "    print(image,\"<<<image\")\n",
        "    print(label,\"<<<label\")    \n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({'params': params}, image);print(\"done1\",logits.shape)\n",
        "        loss =  .5 * jnp.mean((logits - label) ** 2);print(\"done2\",loss.shape)\n",
        "        return loss, logits\n",
        "\n",
        "    # def loss_fn(params):\n",
        "    #     model_fn = lambda x: state.apply_fn({\"params\": params}, x)\n",
        "    #     ray_origins, ray_directions = inputs\n",
        "    #     print(ray_origins)\n",
        "    #     print(ray_directions)\n",
        "    #     rgb, *_ = perform_volume_rendering(\n",
        "    #         model_fn, ray_origins, ray_directions, rng\n",
        "    #     )\n",
        "    #     return jnp.mean((rgb - targets) ** 2)  \n",
        "    print(\"ok1really\")\n",
        "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True);print(\"ok1\")\n",
        "    (_, logits), grads = gradient_fn(state.params);print(\"ok2\")\n",
        "    #train_loss, gradients_each = jax.value_and_grad(loss_fn)(state.params);print(\"ok3\")\n",
        "    grads = lax.pmean(grads,\"batch\");print(\"ok4\")\n",
        "    # grads = jnp.mean(grads);print(\"ok4\")\n",
        "    state = state.apply_gradients(grads=grads);print(\"ok5\")\n",
        "    # train_loss = jnp.mean(train_loss);print(\"ok6\")\n",
        "    metrics = compute_metrics(logits=logits, labels=label);print(\"ok7\")\n",
        "    return state, metrics\n",
        "\n",
        "parallel_train_step = jax.pmap(train_step, \"batch\")\n",
        "# parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", in_axes = (0, 0, 0))\n",
        "\n",
        "import jax\n",
        "@jax.jit\n",
        "def eval_step(state, batch):\n",
        "    image, label = batch\n",
        "    logits = state.apply_fn({'params': state.params}, image)\n",
        "    return compute_metrics(logits=logits, labels=label)\n"
      ],
      "metadata": {
        "id": "wskhkyy49MIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### checkpoints management"
      ],
      "metadata": {
        "id": "PddyjIzYzu9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(ckpt_path, state):\n",
        "    with open(ckpt_path, \"wb\") as outfile:\n",
        "        outfile.write(msgpack_serialize(to_state_dict(state)))\n",
        "    \n",
        "\n",
        "\n",
        "def load_checkpoint(ckpt_path, ckpt_file, state):\n",
        "    ckpt_path = os.path.join(ckpt_path, ckpt_file)\n",
        "    with open(ckpt_path, \"rb\") as data_file:\n",
        "        byte_data = data_file.read()\n",
        "    return from_bytes(state, byte_data)\n",
        "\n",
        "\n",
        "def accumulate_metrics(metrics):\n",
        "    metrics = jax.device_get(metrics)\n",
        "    return {\n",
        "        k: np.mean([metric[k] for metric in metrics])\n",
        "        for k in metrics[0]\n",
        "    }"
      ],
      "metadata": {
        "id": "6e5D7TaR8os4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train & evaluation function"
      ],
      "metadata": {
        "id": "LvNe_bDIz0Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1UgWEotThxnP-Vh-h83-VcTPMkKWmgCDe #downloading MAP-DEM "
      ],
      "metadata": {
        "id": "BoQeqWgTkX3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -U tifffile imagecodecs matplotlib lxml zarr fsspec\n"
      ],
      "metadata": {
        "id": "Sh6EJ9Wbsb3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/HLSL30.020_B04_doy2021057_aid0001_43N.tif /content/a.tif #renaming file"
      ],
      "metadata": {
        "id": "yKer8jJlBMPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tifffile\n",
        "import imagecodecs\n",
        "from imagecodecs import imread, imwrite\n",
        "fp = r'/content/a.tif'\n",
        "image = imread(\"/content/a.tif\")\n",
        "b = image.reshape(-1,1)\n",
        "b.shape\n",
        "newsize = (28, 28)\n",
        "c = jnp.asarray(image.resize(newsize, refcheck=False)).reshape(-1,1)\n",
        "c.shape"
      ],
      "metadata": {
        "id": "nFJvGVWttn1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install rasterio\n"
      ],
      "metadata": {
        "id": "E7ILCMFnxUVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "# fp = r'/content/a.tif'\n",
        "# img = rasterio.open(fp)\n",
        "# show(image)\n",
        "# print(img.count) #to print number of bands\n",
        "newsize = (28, 28)\n",
        "def imageGRAY(argv):\n",
        "    im = imread(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "x, x1 = imageGRAY(\"/content/a.tif\")\n",
        "print(x.shape, x1.shape)"
      ],
      "metadata": {
        "id": "PJJCBhyEmIWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://live.staticflickr.com/7492/15677707699_d9d67acf9d_b.jpg -O a.jpg\n",
        "\n",
        "newsize = (140, 140)\n",
        "from PIL import Image, ImageFilter\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def imageGRAY(argv):\n",
        "    im = Image.open(argv).convert('L')\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu\n",
        "x, x1 = imageGRAY(\"/content/a.jpg\")\n",
        "print(x.shape)# mnist IMAGES are 28x28=784 pixels\n",
        "y, y1 = imageRGB(\"/content/a.jpg\")\n",
        "print(y.shape,y1.shape,\"<< y shape\",x.shape,x1.shape,\"<< y shape\")# mnist IMAGES are 28x28=784 pixels\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(x);plt.show()\n",
        "plt.imshow(y);plt.show()\n",
        "plt.imshow((y1[0]-x1).reshape(28,-1));plt.show()\n",
        "plt.imshow((y1[1]-x1).reshape(28,-1));plt.show()\n",
        "plt.imshow((y1[2]-x1).reshape(28,-1));plt.show()\n",
        "\n",
        "\n",
        "# batch = y1, x1  # jnp.ones((28*28,1)),jnp.ones((28*28,1)) OR jnp.ones((2, 28*28, 1))\n",
        "# shapea, channels = y1.shape\n",
        "# state = init_train_state( model, rng, (shapea, channels), learning_rate )\n",
        "# for e in range(150):\n",
        "#   state, metrics = train_step(state, batch, rng)\n",
        "#   print(metrics)"
      ],
      "metadata": {
        "id": "QjJ1VIiKgs0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #âœ…\n",
        "# import os\n",
        "# os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
        "# import jax\n",
        "# jax.devices()\n",
        "# import jax.numpy as jnp\n",
        "# from jax import pmap\n",
        "# a = jnp.arange(8*10).reshape((8, 2,5))\n",
        "# b = 2\n",
        "# print(type(a));print(a)\n",
        "# def f(x,y):a = x**2+y**2;return a\n",
        "# ff = pmap(f, in_axes=(0,None))\n",
        "# result = ff(a,b)\n",
        "# print(type(result));print(result)\n",
        "# num_devices = jax.device_count()\n",
        "\n",
        "# shape_prefix = (num_devices, 1);print(shape_prefix)"
      ],
      "metadata": {
        "id": "gcib9OwKfNO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "download [flower dataset](https://www.kaggle.com/datasets/alxmamaev/flowers-recognition?resource=download) from kaggle."
      ],
      "metadata": {
        "id": "go6vwNDjvoSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1SynswUkxdl6B3c6Uc7Q6MhLG3XirrHd9 # downloading from google drive saved location..\n",
        "!unzip /content/archive.zip #unzipping the flower images from archive.."
      ],
      "metadata": {
        "id": "F_FW-EGUd5EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing batching of the dataset frfom the total dataset, by assuming batch size as 8 then running 50 epochs over the batch , then moving processing onto next batch>>>\n",
        "batch_size = 8\n",
        "import os\n",
        "image_dir = r'/content/flowers/daisy/'\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"c\",\".jpg\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.jpg'\n",
        "expression_b2 = bandend[1]\n",
        "total_images =  [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "total_images.sort()\n",
        "total_images_path = [os.path.join(image_dir, i) for i in total_images if i != 'outputs']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o724RVHZweXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding loop function to loop over the total images and batch 8 of them together , also print path of 8 images of the batch\n",
        "print(len(total_images_path))\n",
        "liss = len(total_images_path)\n",
        "print(total_images_path[3])\n",
        "\n",
        "no_of_batches = int(len(total_images_path)/batch_size)\n",
        "print(no_of_batches)\n",
        "for e in range(no_of_batches):\n",
        "  print(e*batch_size,\":\",(e+1)*batch_size,\"\\n\")\n",
        "  for x in range(e*batch_size,(e+1)*batch_size,1):\n",
        "    print(total_images_path[x])\n"
      ],
      "metadata": {
        "id": "rt5shG0s2V0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #checking image loading to RGB and GRAY scale\n",
        "# x, x1 = imageGRAY(total_images_path[3])\n",
        "# print(x.shape)# mnist IMAGES are 28x28=784 pixels\n",
        "# y, y1 = imageRGB(total_images_path[3])\n",
        "# print(y.shape,y1.shape,\"<< y shape\",x.shape,x1.shape,\"<< x shape\")# mnist IMAGES are 28x28=784 pixels\n",
        "# from matplotlib import pyplot as plt\n",
        "# plt.imshow(x);plt.show()\n",
        "# plt.imshow(y);plt.show()"
      ],
      "metadata": {
        "id": "dudyvy_M4Re9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combining both above looping over the dataset and image viewability TOGETHER\n",
        "print(len(total_images_path))\n",
        "liss = len(total_images_path)\n",
        "print(total_images_path[3])\n",
        "\n",
        "no_of_batches = int(len(total_images_path)/batch_size)\n",
        "print(no_of_batches)\n",
        "for e in range(no_of_batches):\n",
        "  print(e*batch_size,\":\",(e+1)*batch_size,\"\\n\")\n",
        "  for x in range(batch_size):\n",
        "    print(total_images_path[e*batch_size + x])\n",
        "    #checking image loading to RGB and GRAY scale\n",
        "    xGRAY, x1GRAY = imageGRAY(total_images_path[e*batch_size + x])\n",
        "    print(xGRAY.shape)# mnist IMAGES are 28x28=784 pixels\n",
        "    yRGB, y1RGB = imageRGB(total_images_path[e*batch_size + x])\n",
        "    print(yRGB.shape, y1RGB.shape,\"<< y shape\", xGRAY.shape, x1GRAY.shape,\"<< x shape\")# mnist IMAGES are 28x28=784 pixels\n",
        "    # from matplotlib import pyplot as plt\n",
        "    # plt.imshow(xGRAY);plt.show()\n",
        "    # plt.imshow(yRGB);plt.show()\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "w3AWeuMb6aag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.device_count()"
      ],
      "metadata": {
        "id": "uiksWjeoDmWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Runs correctly >>> but commenting out"
      ],
      "metadata": {
        "id": "Dh6VWL--mH8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #combining both above looping over the dataset and image viewability TOGETHER, REcalibrating to list images as batch array>>>\n",
        "# print(len(total_images_path))\n",
        "# liss = len(total_images_path)\n",
        "# print(total_images_path[3])\n",
        "# ##############\n",
        "# num_devices = jax.device_count()   # adding number of devices to process in parallel\n",
        "# ##############\n",
        "# no_of_batches = int(len(total_images_path)/batch_size)\n",
        "# print(no_of_batches)\n",
        "# for e in range(no_of_batches):\n",
        "#   print(e*batch_size,\":\",(e+1)*batch_size,\"\\n\")\n",
        "#   xeGRAY = []\n",
        "#   yeRGB = []\n",
        "#   for x in range(batch_size):\n",
        "#     print(total_images_path[e*batch_size + x])\n",
        "#     #checking image loading to RGB and GRAY scale\n",
        "#     xGRAY, x1GRAY = imageGRAY(total_images_path[e*batch_size + x])\n",
        "#     xeGRAY.append(x1GRAY)\n",
        "#     print(xGRAY.shape)# mnist IMAGES are 28x28=784 pixels\n",
        "#     yRGB, y1RGB = imageRGB(total_images_path[e*batch_size + x])\n",
        "#     yeRGB.append(y1RGB)\n",
        "#     print(yRGB.shape, yeRGB[x].shape,\"<< y shape\", xGRAY.shape, xeGRAY[x].shape,\"<< x shape\")# mnist IMAGES are 28x28=784 pixels\n",
        "#     # from matplotlib import pyplot as plt\n",
        "#     # plt.imshow(xGRAY);plt.show()\n",
        "#     # plt.imshow(yRGB);plt.show()\n",
        "#   ##########################################<<< PARALLEL CALCULATION OF MODEL\n",
        "#   #adding jnp.asarray to batch of images, \n",
        "#   ddyz = jnp.asarray((yeRGB[0],yeRGB[1],yeRGB[2],yeRGB[3],yeRGB[4],yeRGB[5],yeRGB[6],yeRGB[7]))       # rgb images (width * height, 3)\n",
        "#   print(ddyz.shape,\"<<< ddy shape\")\n",
        "#   ddxz = jnp.asarray((xeGRAY[0],xeGRAY[1],xeGRAY[2],xeGRAY[3],xeGRAY[4],xeGRAY[5],xeGRAY[6],xeGRAY[7]))    #gray images (width * height, 1)\n",
        "#   shape_prefix = (num_devices, 1);print(shape_prefix);print(ddyz.shape,\"<<<< ddyz.shape???\")\n",
        "#   batch_ccc = ddyz, ddxz  # jnp.ones((28*28,1)),jnp.ones((28*28,1)) OR jnp.ones((2, 28*28, 1))\n",
        "#   print(len(batch_ccc),\"<<< batch\")\n",
        "#   vv, shapea, channels = ddyz.shape\n",
        "#   rng = jax.random.PRNGKey(0)\n",
        "#   # dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "#   ######################\n",
        "#   count = 0\n",
        "#   if count == 0 :\n",
        "#     state = init_train_state( model, rng, (shapea, channels), learning_rate ) \n",
        "#     count = 1\n",
        "#   state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "\n",
        "#   for epochs in range(50):   # EPOCHS for training & updating the initiated state, metrics may show the loss in each epochs or iteration\n",
        "#     dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "#     state, metrics = parallel_train_step(state, batch_ccc, dropout_rngs)\n",
        "#     print(\"<<âœ…âœ…âœ…epoc : \",epochs,\" completeâœ…âœ…âœ…>>\\n\",metrics)\n",
        "#   ##########################################\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "ZCcVKRRj_tqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **tensorboard visualization of loss graph**"
      ],
      "metadata": {
        "id": "h7tUJb9cn4Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "8CzwuHhg0I7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir={logdir}"
      ],
      "metadata": {
        "id": "w1h6oZge0L7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***ðŸ‘ HIGH HEELS RUN >>>>>>>>>>>***"
      ],
      "metadata": {
        "id": "le0yBm0Ap4mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import random\n",
        "def batchedimages(image_locations):\n",
        "  ddyss = jnp.asarray((imageRGB(total_images_path[image_locations[0]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[1]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[2]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[3]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[4]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[5]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[6]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[7]])[1]))\n",
        "  ddxss = jnp.asarray((imageGRAY(total_images_path[image_locations[0]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[1]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[2]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[3]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[4]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[5]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[6]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[7]])[1]))\n",
        "  #print(ddyss.shape,\"<<<< ddyss.shape???\",ddxss.shape,\"<<<< ddxss.shape???\") #to check shape \n",
        "  batch_ccc = ddyss, ddxss\n",
        "  return batch_ccc\n",
        "  \n",
        "def data_stream():\n",
        "  key = random.PRNGKey(0)\n",
        "  perm = random.permutation(key, len(total_images_path))\n",
        "  for i in range(no_of_batches):\n",
        "    batch_idx = perm[i * batch_size : (i + 1) * batch_size]; #print(batch_idx)\n",
        "    yield batchedimages(batch_idx)\n",
        "\n",
        "batches = data_stream()  ### this stream will utilize the array of paths of images to a folder, then \"generate\" batches into the variable\n",
        "\n",
        "next(batches)[0].shape ### this command starts initial 8 image  stream, if callled inside a iteration loop then it will get next images for calculations>>>\n",
        "vv, shapea, channels = next(batches)[0].shape # seitting values >>> [0] 8 784 3  [1] 8 784 1 ; RGB & GRAYSCALE versions 8 images each converted to 1-D array\n",
        "print(vv, shapea, channels)\n",
        "######################<<< summary writer for tensor board\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "logdir = \"runs\"\n",
        "\n",
        "writer = SummaryWriter(logdir)\n",
        "######################\n",
        "######################\n",
        "rng = jax.random.PRNGKey(0)\n",
        "# dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "######################\n",
        "#################################<<< checking if checkpoint already available\n",
        "import os # importing os module\n",
        "import re # to find file using regular expression\n",
        "checkpoint_available = 0\n",
        "pattern = re.compile(\"checkpoint_\\d+\")   # to search for \"checkpoint_*munerical value*\" numerical value of any length is denoted by regular expression \"\\d+\"\n",
        "dir = \"/content/ckpts/\"\n",
        "isFile = os.path.isdir(dir)\n",
        "if isFile:\n",
        "  for filepath in os.listdir(dir):\n",
        "      if pattern.match(filepath):\n",
        "          checkpoint_available = 1\n",
        "#################################\n",
        "##########################################<<< loading checkpoint by checking the Flag available\n",
        "from flax.training import checkpoints\n",
        "if checkpoint_available:\n",
        "  CKPT_DIR = 'ckpts'\n",
        "  restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "  #state = flax.jax_utils.replicate(restored_state)\n",
        "  print(\"true <<< File loaded for and replicated to all devices\")\n",
        "##########################################\n",
        "######################<<<< initiating train state\n",
        "count = 0\n",
        "if count == 0 :\n",
        "  state = init_train_state( model, rng, (shapea, channels), learning_rate ) \n",
        "  count = 1\n",
        "state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "######################\n",
        "from flax.training import checkpoints\n",
        "import time\n",
        "total_epochs = 10\n",
        "for epochs in range(total_epochs):   # EPOCHS for training & updating the initiated state, metrics may show the loss in each epochs or iteration\n",
        "  start_time = time.time()\n",
        "  batches = data_stream()  ### this stream will utilize the array of paths of images to a folder, then \"generate\" batches into the variable\n",
        "  if checkpoint_available:\n",
        "    CKPT_DIR = 'ckpts'\n",
        "    restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "    state = restored_state\n",
        "    checkpoint_available = 0 # << Flag updated >>> to stop loading the same checkpoint in the next iteration then remove the checkpoint directory\n",
        "    !rm -r /content/ckpts\n",
        "  for bbb in range(no_of_batches-5):\n",
        "    print(bbb,\"of total number of batches\",no_of_batches)\n",
        "    state, metrics = parallel_train_step(state, next(batches), dropout_rngs)\n",
        "    print(\"<<âœ…âœ…âœ…epoc : \",epochs,\" completeâœ…âœ…âœ…>>\\n\",metrics['loss'][0])\n",
        "    writer.add_scalar('Loss', int(metrics['loss'][0]), epochs)\n",
        "  epoch_time = time.time() - start_time\n",
        "  print(f\"Epoch {epochs} in {epoch_time:0.2f} sec\")\n",
        "  ##################################################<<< model saving mechanism for flax model state as checkpoints for each epochs,\"checkpoint\" is a terminology that means all the model weights and biases during the calculation till the completion of 1 epoch were being updated, then this final set of weights and biases including their placement inside the model will be saves as a (schema+weight values) saved as checkpoint in the mentioned <<CKPT_DIR = 'ckpts'>> mentioned folder.  \n",
        "  CKPT_DIR = 'ckpts'\n",
        "  checkpoints.save_checkpoint(ckpt_dir=CKPT_DIR, target=state, step= epochs)     # naming of the checkpoint is \"checkpoint_*\"  where \"*\" => value of the steps variable, i.e. 'epochs'\n",
        "  restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state) # using to get the checkpoint loaded , it can be latest one , or if already available as checkpoint in the \"CKPT_DIR\" directory then take the file from directory then save in >> restored_checkpoints\n",
        "  ##################################################\n",
        "  # images = total_images_path[batch_idx]\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "y2IWaBMaWUkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /content/runs\n",
        "# !rm -r /content/ckpts"
      ],
      "metadata": {
        "id": "xDPUCO57upls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ…âœ…âœ…âœ…âœ…commented out but runs correctly\n",
        "      # num_devices = jax.device_count()\n",
        "      # ddy = jnp.asarray((y1,y1,y1,y1,y1,y1,y1,y1))       # rgb images (width * height, 3)\n",
        "      # print(ddy.shape,\"<<< ddy shape\")\n",
        "      # ddx = jnp.asarray((x1,x1,x1,x1,x1,x1,x1,x1))    #gray images (width * height, 1)\n",
        "      # shape_prefix = (num_devices, 1);print(shape_prefix);print(ddy.shape,\"<<<< ???\")\n",
        "      # # ddy = ddy.reshape(shape_prefix + ddy.shape[1:]);print(ddy.shape,\"<< train_images_incorrect\")\n",
        "      # # ddx = ddx.reshape(shape_prefix + ddx.shape[1:]);print(ddx.shape,\"<< train_images_incorrect\")\n",
        "      # batch = ddy, ddx  # jnp.ones((28*28,1)),jnp.ones((28*28,1)) OR jnp.ones((2, 28*28, 1))\n",
        "      # print(len(batch),\"<<< batch\")\n",
        "      # vv, shapea, channels = ddy.shape\n",
        "      # ######################\n",
        "      # rng = jax.random.PRNGKey(0)\n",
        "      # # dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "      # ######################\n",
        "      # state = init_train_state( model, rng, (shapea, channels), learning_rate ) \n",
        "      # state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "\n",
        "      # for e in range(50):   # EPOCHS for training & updating the initiated state, metrics may show the loss in each epochs or iteration\n",
        "      #   dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "      #   state, metrics = parallel_train_step(state, batch, dropout_rngs)\n",
        "      #   print(\"<<âœ…âœ…âœ…epoc : \",e,\" completeâœ…âœ…âœ…>>\\n\",metrics)"
      ],
      "metadata": {
        "id": "qUpVOU52BWfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âŒâŒâŒâŒâŒâŒâŒdoesnot work >>>"
      ],
      "metadata": {
        "id": "w5h1ioy9Rbtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 32\n",
        "import datasets\n",
        "train_images, train_labels, test_images, test_labels = datasets.mnist()\n",
        "num_train = train_images.shape[0]\n",
        "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
        "num_batches = num_complete_batches + bool(leftover)\n",
        "import jax\n",
        "num_devices = jax.device_count()\n",
        "def data_stream():\n",
        "  key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "  while True:\n",
        "    perm = jax.random.permutation(rng, num_train); print(perm.shape)\n",
        "    for i in range(num_batches):\n",
        "      batch_idx = perm[i * batch_size:(i + 1) * batch_size]; print(batch_idx)\n",
        "      images, labels = train_images[batch_idx], train_labels[batch_idx]; print(images.shape,\"<< train_images_incorrect\");print(labels.shape,\"<< train_images_correct\")\n",
        "      # For this SPMD example, we reshape the data batch dimension into two\n",
        "      # batch dimensions, one of which is mapped over parallel devices.\n",
        "      batch_size_per_device, ragged = divmod(images.shape[0], num_devices);print(batch_size_per_device,\"<<< batch_size_per_device\")\n",
        "      if ragged:\n",
        "        msg = \"batch size must be divisible by device count, got {} and {}.\"\n",
        "        raise ValueError(msg.format(batch_size, num_devices))\n",
        "      shape_prefix = (num_devices, batch_size_per_device);print(shape_prefix)\n",
        "      images = images.reshape(shape_prefix + images.shape[1:]);print(images.shape,\"<< train_images_incorrect\")\n",
        "      labels = labels.reshape(shape_prefix + labels.shape[1:]);print(labels.shape,\"<< train_images_correct\")\n",
        "      return images, labels\n",
        "batches = data_stream()\n"
      ],
      "metadata": {
        "id": "i7oLNzL4U5m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_batches = tf.data.experimental.cardinality(train_dataset)\n",
        "train_datagen = iter(tfds.as_numpy(train_dataset))\n",
        "batch = next(train_datagen)\n",
        "batch = jnp.ones((28*28,1)),jnp.ones((28*28,1))  # jnp.ones((28*28,1)),jnp.ones((28*28,1)) OR jnp.ones((2, 28*28, 1))\n",
        "# state = flax.jax_utils.replicate(state)\n",
        "state = init_train_state( model, rng, (image_height * image_width, channels), learning_rate )\n",
        "for e in range(50):\n",
        "  state, metrics = train_step(state, batch, rng)\n",
        "  print(metrics)"
      ],
      "metadata": {
        "id": "_W-fMd-P07bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_and_evaluate(train_dataset, eval_dataset, test_dataset, state: train_state.TrainState, epochs: int,):\n",
        "    num_train_batches = tf.data.experimental.cardinality(train_dataset)\n",
        "    num_eval_batches = tf.data.experimental.cardinality(eval_dataset)\n",
        "    num_test_batches = tf.data.experimental.cardinality(test_dataset)\n",
        "   \n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        best_eval_loss = 1e6\n",
        "        # ============== Training ============== #\n",
        "        train_batch_metrics = []\n",
        "        train_datagen = iter(tfds.as_numpy(train_dataset))\n",
        "        for batch_idx in range(num_train_batches):\n",
        "            batch = next(train_datagen)\n",
        "            state, metrics = train_step(state, batch, rng)\n",
        "            train_batch_metrics.append(metrics)\n",
        "        train_batch_metrics = accumulate_metrics(train_batch_metrics)\n",
        "        print('TRAIN (%d/%d): Loss: %.4f' % (\n",
        "                epoch, epochs, train_batch_metrics['loss'],\n",
        "              ))\n",
        "        # ============== Validation ============= #\n",
        "        eval_batch_metrics = []\n",
        "        eval_datagen = iter(tfds.as_numpy(eval_dataset))\n",
        "        for batch_idx in range(num_eval_batches):\n",
        "            batch = next(eval_datagen)\n",
        "            metrics = eval_step(state, batch)\n",
        "            eval_batch_metrics.append(metrics)\n",
        "        eval_batch_metrics = accumulate_metrics(eval_batch_metrics)\n",
        "        print('EVAL (%d/%d):  Loss: %.4f\\n' % (\n",
        "                epoch, epochs, eval_batch_metrics['loss'],\n",
        "              ))    \n",
        "\n",
        "        if eval_batch_metrics['loss'] < best_eval_loss:\n",
        "            save_checkpoint(\"checkpoint.msgpack\", state)\n",
        "\n",
        "    restored_state = load_checkpoint(\"checkpoint.msgpack\", state)\n",
        "    test_batch_metrics = []\n",
        "    test_datagen = iter(tfds.as_numpy(test_dataset))\n",
        "    for batch_idx in range(num_test_batches):\n",
        "        batch = next(test_datagen)\n",
        "        metrics = eval_step(restored_state, batch)\n",
        "        test_batch_metrics.append(metrics)\n",
        "    \n",
        "    test_batch_metrics = accumulate_metrics(test_batch_metrics)\n",
        "    print(\n",
        "        'Test: Loss: %.4f,' % (\n",
        "            test_batch_metrics['loss'],\n",
        "        )\n",
        "    )\n",
        "    # Log Metrics to Weights & Biases\n",
        "    history = {\n",
        "        \"Train Loss\": train_batch_metrics['loss'],\n",
        "        \"Validation Loss\": eval_batch_metrics['loss'],\n",
        "    }\n",
        "    return state, restored_state, history\n",
        "\n"
      ],
      "metadata": {
        "id": "3wWR27rO-yGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loading 'mnist' data"
      ],
      "metadata": {
        "id": "FzqGlqVxz5mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title @inproceedings{zhou2017scene, title={Scene Parsing through ADE20K Dataset}, author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio}, booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, year={2017} }\n",
        "#âœ…âœ…\n",
        "import tensorflow_datasets as tfds\n",
        "(full_train_set, test_dataset), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "validation_split = 0.2\n",
        "def normalize_img(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.\n",
        "    return image, label\n",
        "\n"
      ],
      "metadata": {
        "id": "DNQMoyp_HPM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…âœ…\n",
        "import tensorflow as tf\n",
        "batch_size = 64\n",
        "full_train_set = full_train_set.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "num_data = tf.data.experimental.cardinality(full_train_set).numpy()\n",
        "print(\"Total number of data points:\", num_data)\n",
        "train_dataset = full_train_set.take(num_data * (1 - validation_split))\n",
        "val_dataset = full_train_set.take(num_data * (validation_split))\n",
        "print(\"Number of train data points:\",tf.data.experimental.cardinality(train_dataset).numpy())\n",
        "print(\"Number of val data points:\",tf.data.experimental.cardinality(val_dataset).numpy())\n",
        "#############TRAIN##################\n",
        "train_dataset = train_dataset.cache();print(len(train_dataset))\n",
        "train_dataset = train_dataset.shuffle(tf.data.experimental.cardinality(train_dataset).numpy());print(train_dataset)\n",
        "train_dataset = train_dataset.batch(batch_size);print(train_dataset)\n",
        "#############TRAIN(EVALUATE)##################\n",
        "val_dataset = val_dataset.cache()\n",
        "val_dataset = val_dataset.shuffle(tf.data.experimental.cardinality(val_dataset).numpy())\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "#############TEST##################\n",
        "test_dataset = test_dataset.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "print(\"Number of test data points:\",tf.data.experimental.cardinality(test_dataset).numpy())\n",
        "test_dataset = test_dataset.cache()\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "g7_nuCh5b12e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_batches = tf.data.experimental.cardinality(train_dataset)\n",
        "num_train_batches"
      ],
      "metadata": {
        "id": "atLCxN738JSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run inferences"
      ],
      "metadata": {
        "id": "CbgbyxgY0AaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# state, inference_state, history = train_and_evaluate(state, parallelized_train_step, eval_step)\n",
        "\n",
        "\n",
        "# train_dataset, eval_dataset, test_dataset, state, epochs\n",
        "from tqdm.notebook import tqdm\n",
        "epochs = 15\n",
        "state, best_state, history = train_and_evaluate(\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    test_dataset,\n",
        "    state,\n",
        "    epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "ABcUcSG65REH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "_ULguI_OmYwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# go"
      ],
      "metadata": {
        "id": "T4WTGhNs0rep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import imageio\n",
        "import requests\n",
        "from typing import Any\n",
        "import ipywidgets as widgets\n",
        "from functools import partial\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "!pip3 install -q -U flax\n",
        "\n",
        "import jax\n",
        "import flax\n",
        "import optax\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n"
      ],
      "metadata": {
        "id": "_pHA9kuNvNf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://www.kaggle.com/code/odins0n/jax-flax-tf-data-vision-transformers-tutorial\n",
        "\n",
        "\n",
        "# Detect if Kaggle Notebook has access to TPUs or not\n",
        "if 'TPU_NAME' in os.environ:\n",
        "    import requests\n",
        "    if 'TPU_DRIVER_MODE' not in globals():\n",
        "        url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475/requestversion/tpu_driver_nightly'\n",
        "        resp = requests.post(url)\n",
        "        TPU_DRIVER_MODE = 1\n",
        "    from jax.config import config\n",
        "    jax_xla_backend = \"tpu_driver\"\n",
        "    jax_backend_target = os.environ['TPU_NAME']\n",
        "    print(\"TPU DETECTED!\")\n",
        "    print('Registered TPU:', jax_backend_target)\n",
        "\n",
        "\n",
        "# Detect if Google Colab Notebook has access to TPUs or not\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ:\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "\n",
        "else:\n",
        "    print('No TPU detected.')\n",
        "\n",
        "\n",
        "DEVICE_COUNT = len(jax.local_devices())\n",
        "TPU = DEVICE_COUNT==8\n",
        "\n",
        "\n",
        "if TPU:\n",
        "    print(\"8 cores of TPU ( Local devices in Jax ):\")\n",
        "    print('\\n'.join(map(str,jax.local_devices())))\n"
      ],
      "metadata": {
        "id": "eaTPv-kSvMA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# sync all experiment configs with Weights and Biases\n",
        "\n",
        "near_bound = 2. # Near Bound of sample space for 3d points\n",
        "far_bound = 6.  # Far Bound of sample space for 3d points\n",
        "batch_size = int(1e4) # Batch Size\n",
        "num_sample_points = 256 # Number of points to be sampled across the volume\n",
        "epsilon = 1e10 # Hyperparameter for volume rendering\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "positional_encoding_dims = 3 # Number of positional encodings applied\n",
        "num_dense_layers = 5 # Number of dense layers in MLP\n",
        "dense_layer_width = 256 # Dimentionality of dense layers' output space \n",
        "learning_rate = 5e-4 # Learning Rate\n",
        "train_epochs = 1000 # Number of training epochs\n",
        "plot_interval = 100 # Epoch interval for plotting results during training\n"
      ],
      "metadata": {
        "id": "Dj-uVwfWvJmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encoding_dims = 3 # Number of positional encodings applied\n",
        "def positional_encoding(inputs):\n",
        "    batch_size, _ = inputs.shape;                                                   print(inputs.shape)\n",
        "    # Applying vmap transform to vectorize the multiplication operation\n",
        "    f = jax.vmap(lambda x: inputs * 2.0 ** x)(jnp.arange(positional_encoding_dims));print(f.shape)\n",
        "    fy = jnp.stack([jnp.sin(f), jnp.cos(f)]);                                       print(fy)\n",
        "    fy = fy.swapaxes(0, 2).reshape([batch_size, -1]);                               print(fy)\n",
        "    fy = jnp.concatenate([inputs, fy], axis=-1);                                    print(fy)\n",
        "    return fy\n"
      ],
      "metadata": {
        "id": "tqiAfLsYvFdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(10000, 3)\n",
        "\n",
        "\n",
        "(3, 10000, 3)\n",
        "\n",
        "Traced<ShapedArray(float32[2,3,10000,3])>with<DynamicJaxprTrace(level=0/1)>\n",
        "\n",
        "Traced<ShapedArray(float32[10000,18])>with<DynamicJaxprTrace(level=0/1)>\n",
        "\n",
        "Traced<ShapedArray(float32[10000,21])>with<DynamicJaxprTrace(level=0/1)>"
      ],
      "metadata": {
        "id": "uR_Bu2DpbChW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test mnist pmap gpu 1000 epocs runtime 9 minutes"
      ],
      "metadata": {
        "id": "cq1KuyqhumRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile datasets.py\n",
        "# Copyright 2018 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Datasets used in examples.\"\"\"\n",
        "\n",
        "\n",
        "import array\n",
        "import gzip\n",
        "import os\n",
        "from os import path\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "_DATA = \"/tmp/jax_example_data/\"\n",
        "\n",
        "\n",
        "def _download(url, filename):\n",
        "  \"\"\"Download a url to a file in the JAX data temp directory.\"\"\"\n",
        "  if not path.exists(_DATA):\n",
        "    os.makedirs(_DATA)\n",
        "  out_file = path.join(_DATA, filename)\n",
        "  if not path.isfile(out_file):\n",
        "    urllib.request.urlretrieve(url, out_file)\n",
        "    print(f\"downloaded {url} to {_DATA}\")\n",
        "\n",
        "\n",
        "def _partial_flatten(x):\n",
        "  \"\"\"Flatten all but the first dimension of an ndarray.\"\"\"\n",
        "  return np.reshape(x, (x.shape[0], -1))\n",
        "\n",
        "\n",
        "def _one_hot(x, k, dtype=np.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return np.array(x[:, None] == np.arange(k), dtype)\n",
        "\n",
        "\n",
        "def mnist_raw():\n",
        "  \"\"\"Download and parse the raw MNIST dataset.\"\"\"\n",
        "  # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
        "  base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
        "\n",
        "  def parse_labels(filename):\n",
        "    with gzip.open(filename, \"rb\") as fh:\n",
        "      _ = struct.unpack(\">II\", fh.read(8))\n",
        "      return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
        "\n",
        "  def parse_images(filename):\n",
        "    with gzip.open(filename, \"rb\") as fh:\n",
        "      _, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "      return np.array(array.array(\"B\", fh.read()),\n",
        "                      dtype=np.uint8).reshape(num_data, rows, cols)\n",
        "\n",
        "  for filename in [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n",
        "                   \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]:\n",
        "    _download(base_url + filename, filename)\n",
        "\n",
        "  train_images = parse_images(path.join(_DATA, \"train-images-idx3-ubyte.gz\"))\n",
        "  train_labels = parse_labels(path.join(_DATA, \"train-labels-idx1-ubyte.gz\"))\n",
        "  test_images = parse_images(path.join(_DATA, \"t10k-images-idx3-ubyte.gz\"))\n",
        "  test_labels = parse_labels(path.join(_DATA, \"t10k-labels-idx1-ubyte.gz\"))\n",
        "\n",
        "  return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "def mnist(permute_train=False):\n",
        "  \"\"\"Download, parse and process MNIST data to unit scale and one-hot labels.\"\"\"\n",
        "  train_images, train_labels, test_images, test_labels = mnist_raw()\n",
        "\n",
        "  train_images = _partial_flatten(train_images) / np.float32(255.)\n",
        "  test_images = _partial_flatten(test_images) / np.float32(255.)\n",
        "  train_labels = _one_hot(train_labels, 10)\n",
        "  test_labels = _one_hot(test_labels, 10)\n",
        "\n",
        "  if permute_train:\n",
        "    perm = np.random.RandomState(0).permutation(train_images.shape[0])\n",
        "    train_images = train_images[perm]\n",
        "    train_labels = train_labels[perm]\n",
        "\n",
        "  return train_images, train_labels, test_images, test_labels"
      ],
      "metadata": {
        "id": "pn2wusArvMza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[datasets](https://github.com/google/jax/blob/main/examples/datasets.py) link\n",
        "\n",
        "[spmd_mnist_classifier_fromscratch](https://github.com/google/jax/blob/main/examples/spmd_mnist_classifier_fromscratch.py) link"
      ],
      "metadata": {
        "id": "mBuFyBnWy9S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2018 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"An MNIST example with single-program multiple-data (SPMD) data parallelism.\n",
        "\n",
        "The aim here is to illustrate how to use JAX's `pmap` to express and execute\n",
        "SPMD programs for data parallelism along a batch dimension, while also\n",
        "minimizing dependencies by avoiding the use of higher-level layers and\n",
        "optimizers libraries.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "\n",
        "import jax\n",
        "from jax import jit, grad, pmap\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax.tree_util import tree_map\n",
        "from jax import lax\n",
        "import jax.numpy as jnp\n",
        "import datasets\n",
        "from google.colab import output\n",
        "\n",
        "\n",
        "def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):\n",
        "  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
        "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
        "\n",
        "def predict(params, inputs):\n",
        "  activations = inputs\n",
        "  for w, b in params[:-1]:\n",
        "    outputs = jnp.dot(activations, w) + b\n",
        "    activations = jnp.tanh(outputs)\n",
        "\n",
        "  final_w, final_b = params[-1]\n",
        "  logits = jnp.dot(activations, final_w) + final_b\n",
        "  return logits - logsumexp(logits, axis=1, keepdims=True)\n",
        "\n",
        "def loss(params, batch):\n",
        "  inputs, targets = batch\n",
        "  preds = predict(params, inputs)\n",
        "  return -jnp.mean(jnp.sum(preds * targets, axis=1))\n",
        "\n",
        "@jit\n",
        "def accuracy(params, batch):\n",
        "  inputs, targets = batch\n",
        "  target_class = jnp.argmax(targets, axis=1)\n",
        "  predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n",
        "  return jnp.mean(predicted_class == target_class)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  layer_sizes = [784, 1024, 1024, 10]\n",
        "  param_scale = 0.1\n",
        "  step_size = 0.001\n",
        "  num_epochs = 1000\n",
        "  batch_size = 128\n",
        "\n",
        "  train_images, train_labels, test_images, test_labels = datasets.mnist()\n",
        "  num_train = train_images.shape[0]\n",
        "  num_complete_batches, leftover = divmod(num_train, batch_size)\n",
        "  num_batches = num_complete_batches + bool(leftover)\n",
        "\n",
        "  # For this manual SPMD example, we get the number of devices (e.g. GPUs or\n",
        "  # TPU cores) that we're using, and use it to reshape data minibatches.\n",
        "  num_devices = jax.device_count()\n",
        "  def data_stream():\n",
        "    rng = npr.RandomState(0)\n",
        "    while True:\n",
        "      perm = rng.permutation(num_train)\n",
        "      for i in range(num_batches):\n",
        "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "        images, labels = train_images[batch_idx], train_labels[batch_idx]\n",
        "        # For this SPMD example, we reshape the data batch dimension into two\n",
        "        # batch dimensions, one of which is mapped over parallel devices.\n",
        "        batch_size_per_device, ragged = divmod(images.shape[0], num_devices)\n",
        "        if ragged:\n",
        "          msg = \"batch size must be divisible by device count, got {} and {}.\"\n",
        "          raise ValueError(msg.format(batch_size, num_devices))\n",
        "        shape_prefix = (num_devices, batch_size_per_device)\n",
        "        images = images.reshape(shape_prefix + images.shape[1:])\n",
        "        labels = labels.reshape(shape_prefix + labels.shape[1:])\n",
        "        yield images, labels\n",
        "  batches = data_stream()\n",
        "\n",
        "  @partial(pmap, axis_name='batch')\n",
        "  def spmd_update(params, batch):\n",
        "    grads = grad(loss)(params, batch)\n",
        "    # We compute the total gradients, summing across the device-mapped axis,\n",
        "    # using the `lax.psum` SPMD primitive, which does a fast all-reduce-sum.\n",
        "    grads = [(lax.psum(dw, 'batch'), lax.psum(db, 'batch')) for dw, db in grads]\n",
        "    return [(w - step_size * dw, b - step_size * db)\n",
        "            for (w, b), (dw, db) in zip(params, grads)]\n",
        "\n",
        "  # We replicate the parameters so that the constituent arrays have a leading\n",
        "  # dimension of size equal to the number of devices we're pmapping over.\n",
        "  init_params = init_random_params(param_scale, layer_sizes)\n",
        "  replicate_array = lambda x: np.broadcast_to(x, (num_devices,) + x.shape)\n",
        "  replicated_params = tree_map(replicate_array, init_params)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_batches):\n",
        "      replicated_params = spmd_update(replicated_params, next(batches))\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # We evaluate using the jitted `accuracy` function (not using pmap) by\n",
        "    # grabbing just one of the replicated parameter values.\n",
        "    params = tree_map(lambda x: x[0], replicated_params)\n",
        "    train_acc = accuracy(params, (train_images, train_labels))\n",
        "    test_acc = accuracy(params, (test_images, test_labels))\n",
        "    output.clear() #to_clear_the_output_console_everytime\n",
        "    print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
        "    print(f\"Training set accuracy {train_acc}\")\n",
        "    print(f\"Test set accuracy {test_acc}\")"
      ],
      "metadata": {
        "id": "aVJlb1-tuorJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}