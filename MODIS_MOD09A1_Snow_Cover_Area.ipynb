{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MODIS_MOD09A1_Snow_Cover_Area.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2022/blob/main/MODIS_MOD09A1_Snow_Cover_Area.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " >MOD09A1.061 data DOWNLOAD & PROCESSING calculating NDSI  > 0.4 && band2 reflectance is more than 11%"
      ],
      "metadata": {
        "id": "hlcgiXaXS1-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v6GiEeRvYbBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35cab6e-615f-4bee-aced-246a4c95167f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NbFxO-hu1Tx"
      },
      "source": [
        "## **RUNNING CODE NDSI *MOD09A1.061* 8 DAY**  \n",
        "## ðŸŒ¨ï¸â„ï¸ðŸ”ï¸  **SNOW COVER AREA**  ðŸ”ï¸â„ï¸ðŸŒ¨ï¸"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "Xx-WJA_Bhm3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef1ddc8-2e14-4d10-c016-d964fe27d3a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre requisites please run this section >>>>"
      ],
      "metadata": {
        "id": "E8YMNHt3soFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq"
      ],
      "metadata": {
        "id": "79zaMlOGsmAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e432cf0-d751-416b-9cf9-586b3262c432"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyModis\n",
            "  Downloading pyModis-2.3.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: GDAL in /usr/local/lib/python3.7/dist-packages (from pyModis) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyModis) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyModis) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyModis) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (1.24.3)\n",
            "Installing collected packages: pyModis\n",
            "Successfully installed pyModis-2.3.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libjq1 libonig4\n",
            "The following NEW packages will be installed:\n",
            "  jq libjq1 libonig4\n",
            "0 upgraded, 3 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 276 kB of archives.\n",
            "After this operation, 930 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libonig4 amd64 6.7.0-1 [119 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libjq1 amd64 1.5+dfsg-2 [111 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 jq amd64 1.5+dfsg-2 [45.6 kB]\n",
            "Fetched 276 kB in 0s (892 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libonig4:amd64.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../libonig4_6.7.0-1_amd64.deb ...\n",
            "Unpacking libonig4:amd64 (6.7.0-1) ...\n",
            "Selecting previously unselected package libjq1:amd64.\n",
            "Preparing to unpack .../libjq1_1.5+dfsg-2_amd64.deb ...\n",
            "Unpacking libjq1:amd64 (1.5+dfsg-2) ...\n",
            "Selecting previously unselected package jq.\n",
            "Preparing to unpack .../jq_1.5+dfsg-2_amd64.deb ...\n",
            "Unpacking jq (1.5+dfsg-2) ...\n",
            "Setting up libonig4:amd64 (6.7.0-1) ...\n",
            "Setting up libjq1:amd64 (1.5+dfsg-2) ...\n",
            "Setting up jq (1.5+dfsg-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown https://drive.google.com/uc?id=1oyqXeHZgaTOjLod-Vqz-VAzS88O13JDr\n",
        "# !unzip /content/beasBasinShapeFile.zip -d /content"
      ],
      "metadata": {
        "id": "s73AxlUG7E0i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK\n",
        "!unzip /content/elev_correct.zip\n",
        "!gdown https://drive.google.com/uc?id=18n7BBOAzzNqAKGjtEyvLqtfxWuCr3Tdf\n",
        "!unzip /content/elev_polygon_shp.zip"
      ],
      "metadata": {
        "id": "BuiH0mvfFZix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb0966e-a1f1-46c8-9dcd-2e08f9f47291"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK\n",
            "To: /content/elev_correct.zip\n",
            "\r  0% 0.00/480k [00:00<?, ?B/s]\r100% 480k/480k [00:00<00:00, 123MB/s]\n",
            "Archive:  /content/elev_correct.zip\n",
            " extracting: elev_correct/E1003000.cpg  \n",
            "  inflating: elev_correct/E1003000.dbf  \n",
            "  inflating: elev_correct/E1003000.prj  \n",
            "  inflating: elev_correct/E1003000.qmd  \n",
            "  inflating: elev_correct/E1003000.shp  \n",
            "  inflating: elev_correct/E1003000.shx  \n",
            " extracting: elev_correct/E30005000.cpg  \n",
            "  inflating: elev_correct/E30005000.dbf  \n",
            "  inflating: elev_correct/E30005000.prj  \n",
            "  inflating: elev_correct/E30005000.qmd  \n",
            "  inflating: elev_correct/E30005000.shp  \n",
            "  inflating: elev_correct/E30005000.shx  \n",
            " extracting: elev_correct/E50007000.cpg  \n",
            "  inflating: elev_correct/E50007000.dbf  \n",
            "  inflating: elev_correct/E50007000.prj  \n",
            "  inflating: elev_correct/E50007000.qmd  \n",
            "  inflating: elev_correct/E50007000.shp  \n",
            "  inflating: elev_correct/E50007000.shx  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18n7BBOAzzNqAKGjtEyvLqtfxWuCr3Tdf\n",
            "To: /content/elev_polygon_shp.zip\n",
            "100% 55.5M/55.5M [00:00<00:00, 94.5MB/s]\n",
            "Archive:  /content/elev_polygon_shp.zip\n",
            "   creating: polygon_shp/A1003000_shpFile/\n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D1.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D1.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D1.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D1.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D1.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D1.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D2.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D2.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D2.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D2.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D2.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D2.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D3.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D3.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D3.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D3.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D3.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D3.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D4.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D4.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D4.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D4.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D4.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D4.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D5.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D5.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D5.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D5.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D5.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D5.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D6.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D6.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D6.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D6.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D6.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D6.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D7.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D7.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D7.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D7.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D7.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D7.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D8.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D8.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D8.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D8.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D8.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D8.shx  \n",
            "   creating: polygon_shp/A30005000_shpFile/\n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D1.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D1.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D1.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D1.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D1.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D1.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D2.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D2.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D2.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D2.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D2.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D2.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D3.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D3.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D3.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D3.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D3.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D3.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D4.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D4.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D4.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D4.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D4.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D4.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D5.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D5.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D5.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D5.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D5.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D5.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D6.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D6.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D6.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D6.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D6.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D6.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D7.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D7.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D7.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D7.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D7.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D7.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D8.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D8.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D8.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D8.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D8.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D8.shx  \n",
            "   creating: polygon_shp/A50007000_shpFile/\n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D1.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D1.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D1.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D1.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D1.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D1.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D2.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D2.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D2.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D2.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D2.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D2.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D3.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D3.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D3.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D3.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D3.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D3.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D4.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D4.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D4.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D4.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D4.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D4.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D5.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D5.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D5.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D5.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D5.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D5.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D6.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D6.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D6.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D6.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D6.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D6.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D7.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D7.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D7.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D7.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D7.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D7.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D8.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D8.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D8.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D8.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D8.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D8.shx  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Data Here"
      ],
      "metadata": {
        "id": "PmqyTwzIyPI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "imgs_list_b4 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b4)]\n",
        "imgs_list_b6 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b6)]\n",
        "imgs_list_b2.sort()\n",
        "imgs_list_b4.sort()\n",
        "imgs_list_b6.sort()\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "imgs_path_b4 = [os.path.join(image_dir, i) for i in imgs_list_b4 if i != 'outputs']\n",
        "imgs_path_b6 = [os.path.join(image_dir, i) for i in imgs_list_b6 if i != 'outputs']\n",
        "print(len(imgs_path_b2),len(imgs_path_b4),len(imgs_path_b6))\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    pathb4 = imgs_list_b4[i]\n",
        "    pathb6 = imgs_list_b6[i]\n",
        "    data = []\n",
        "    data_output = image_dir[:-5] + \"out2.txt\"\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    with open(data_output, \"r\", encoding = \"utf-8\") as f:\n",
        "      data = f.read().split('\\n')\n",
        "      check = 0\n",
        "      for i in range(len(data)):\n",
        "        if (str(pathb2[25:25+10])==data[i].split(',')[0]):\n",
        "          check = 1\n",
        "          print(check)\n",
        "          print(str(pathb2[25:25+10]))\n",
        "\n",
        "      if (check != 1):\n",
        "        print(str(pathb2[25:25+10]))\n",
        "      ########################################################################\n",
        "        #creating file NDSI\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb4} \\\n",
        "          --A_band 1 \\\n",
        "          -B {image_dir}{pathb6} \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "          --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb2} \\\n",
        "          --A_band 1 \\\n",
        "          -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "          --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "        !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        #counting Snow and Non-Snowpixels  \n",
        "        #import gdalnumeric\n",
        "        raster_file = gdalnumeric.LoadFile(temp_dir+\"BothCheck_result.tif\")\n",
        "        pixel_count_snow = (raster_file ==1).sum()\n",
        "        pixel_count_notsnow = (raster_file ==0).sum()\n",
        "        print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)\n",
        "\n",
        "        SC_values = []\n",
        "        ########################################################################\n",
        "        temp_dir_elevations = r'/content/elev_correct/E'\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          print(j)\n",
        "          !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "            {temp_dir}\"BothCheck_result.tif\" \\\n",
        "            {temp_dir}result_\"{str(j)}\".tif\n",
        "\n",
        "          #counting Snow and Non-Snowpixels  \n",
        "          #import gdalnumeric\n",
        "          raster_file = gdalnumeric.LoadFile(temp_dir+\"result_\"+str(j)+\".tif\")\n",
        "          pixel_nos_snow = (raster_file ==1).sum()\n",
        "          pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "          SC_values.append(pixel_nos_snow)\n",
        "          SC_values.append(pixel_nos_notsnow)\n",
        "          ###########################################################################\n",
        "          #os.system (cmd)\n",
        "          # slp1=gdal.Open(temp_dir+\"result_\"+str(j)+\".tif\")\n",
        "          # slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "          # plt.figure()\n",
        "          # plt.imshow(slp1Array)\n",
        "          # plt.colorbar()\n",
        "          # plt.show()\n",
        "          ###########################################################################\n",
        "          !rm -r {temp_dir}result_{str(j)}.tif\n",
        "        ########################################################################\n",
        "        \n",
        "        SC_aspect_values = []\n",
        "        ########################################################################\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          dir_to_store = \"/content/polygon_shp/A\"+str(elev[j])+\"_shpFile/\"\n",
        "          for i in range(1,9):\n",
        "            !gdalwarp \\\n",
        "              --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "              -overwrite -of GTiff -ot Float32 \\\n",
        "              -cutline {dir_to_store}Cresult_D{i}.shp -cl Cresult_D{i} -crop_to_cutline \\\n",
        "              {temp_dir}\"BothCheck_result.tif\" \\\n",
        "              {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "            #counting Snow and Non-Snowpixels  \n",
        "            #import gdalnumeric\n",
        "            print(\"calculating \", i ,\"\\n\")\n",
        "            raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "            pixel_nos_snow = (raster_file ==1).sum()\n",
        "            pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "            SC_aspect_values.append(pixel_nos_snow)\n",
        "            SC_aspect_values.append(pixel_nos_notsnow)\n",
        "            !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "        ########################################################################\n",
        "        ########################################################################\n",
        "        m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "        ########################################################################\n",
        "        !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "        ########################################################################\n",
        "        area_sc_values = [number * coonstant_c for number in SC_values]\n",
        "        ########################################################################\n",
        "        area_aspect_sc_values = [number * coonstant_c for number in SC_aspect_values]\n",
        "        print(str(coonstant_c*pixel_count_snow),str(coonstant_c*pixel_count_notsnow),str(coonstant_c*pixel_count_snow+coonstant_c*pixel_count_notsnow))\n",
        "        ########################################################################\n",
        "        lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow) + \",\" + str(area_sc_values)[1:-1] + \",\" + str(area_aspect_sc_values)[1:-1])\n",
        "        with open(data_output, \"a+\", encoding = \"utf-8\") as f:\n",
        "            f.writelines('\\n' + lines)\n",
        "            output.clear() #to_clear_the_output_console_everytime\n",
        "      ########################################################################  "
      ],
      "metadata": {
        "id": "O4TyjxtGGWah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7061f807-58fd-4cca-b3e1-051693381a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n",
            "/content/drive/MyDrive/OUT/data/MOD09A1061/files/MOD09A1.061_sur_refl_b02_doy2009313_aid0001.tif\n",
            "doy2009313\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "snow: 15981  not snow: 46827\n",
            "0\n",
            "Creating output file that is 389P x 219L.\n",
            "Processing input file /content/BothCheck_result.tif.\n",
            "Using internal nodata values (e.g. 3.40282e+38) for image /content/BothCheck_result.tif.\n",
            "Copying nodata values from source /content/BothCheck_result.tif to destination /content/result_0.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "1\n",
            "Creating output file that is 361P x 228L.\n",
            "Processing input file /content/BothCheck_result.tif.\n",
            "Using internal nodata values (e.g. 3.40282e+38) for image /content/BothCheck_result.tif.\n",
            "Copying nodata values from source /content/BothCheck_result.tif to destination /content/result_1.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "2\n",
            "Creating output file that is 265P x 182L.\n",
            "Processing input file /content/BothCheck_result.tif.\n",
            "Using internal nodata values (e.g. 3.40282e+38) for image /content/BothCheck_result.tif.\n",
            "Copying nodata values from source /content/BothCheck_result.tif to destination /content/result_2.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 389P x 216L.\n",
            "Processing input file /content/BothCheck_result.tif.\n",
            "Using internal nodata values (e.g. 3.40282e+38) for image /content/BothCheck_result.tif.\n",
            "Copying nodata values from source /content/BothCheck_result.tif to destination /content/Xresult_1.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "calculating  1 \n",
            "\n",
            "Creating output file that is 384P x 217L.\n",
            "Processing input file /content/BothCheck_result.tif.\n",
            "Using internal nodata values (e.g. 3.40282e+38) for image /content/BothCheck_result.tif.\n",
            "Copying nodata values from source /content/BothCheck_result.tif to destination /content/Xresult_2.tif.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ“œDATA DOWNLOAD SECTIONðŸ“œ**"
      ],
      "metadata": {
        "id": "I7d6uagHFbGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### APPEEAR LDDAC DATA DOWNLOAD"
      ],
      "metadata": {
        "id": "XsU-Aivn7ihA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1SrMZ6KTEpO0yuTwm9DlLFREjksvn1_mZ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4N7TJmtX7K4",
        "outputId": "5d5d4496-6c20-44ba-a49a-679264138c5a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SrMZ6KTEpO0yuTwm9DlLFREjksvn1_mZ\n",
            "To: /content/url.txt\n",
            "\r  0% 0.00/2.33M [00:00<?, ?B/s]\r100% 2.33M/2.33M [00:00<00:00, 157MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/OUT/data/MOD09A1061/files\n",
        "!ls /content/drive/MyDrive/OUT/data/MOD09A1061/files\n",
        "%cd /content/drive/MyDrive/OUT/data/MOD09A1061/files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLh93IAEYs04",
        "outputId": "12055cd6-6111-4c6a-ff21-08f4c6a2b3cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "/content/drive/MyDrive/OUT/data/MOD09A1061/files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try download the list data"
      ],
      "metadata": {
        "id": "Iia9EHMD72b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --request POST --user kroy0001:/#j%kWrPA,8.HRe --header \"Content-Length: 0\" \"https://appeears.earthdatacloud.nasa.gov/api/login\""
      ],
      "metadata": {
        "id": "N7PNs-Mu9ibu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa87075-e7ec-4c7c-f726-964b49a67d3c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"token_type\": \"Bearer\", \"token\": \"B9rG-ZpBea75Ds9E_jOGc-DC5_C1vel5aPMln6KTulGEDa0U2OpxTt7TyicnkyDBYpgjqhnk7aFp9e-juzo2Qg\", \"expiration\": \"2022-08-26T07:40:28Z\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !curl -L -O --remote-header-name \\\n",
        "#   --header \"Authorization: Bearer bVVLVOIv29Lds-zADthteUE_1QlykgndjN5T6BaKMzMS-A11Z8UWtVsNbAJ85LWcGGerQH1KpM7eb-1KZS_Nig\" \\\n",
        "#   --location https://appeears.earthdatacloud.nasa.gov/api/bundle/908b9b61-5acf-48ca-933e-1fcd3b2704fc/c4d1addc-4e43-43e6-aac4-04cdcf04faca/MOD09A1.061_sur_refl_b01_doy2000129_aid0001.tif"
      ],
      "metadata": {
        "id": "IDFusRPU9up5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "\n",
        "def download_url(url):\n",
        "    t0 = time.time()\n",
        "###########################################################################################################################\n",
        "    !curl -L -O --remote-header-name \\\n",
        "      --header \"Authorization: Bearer B9rG-ZpBea75Ds9E_jOGc-DC5_C1vel5aPMln6KTulGEDa0U2OpxTt7TyicnkyDBYpgjqhnk7aFp9e-juzo2Qg\" \\\n",
        "      --location {url}\n",
        "###########################################################################################################################\n",
        "    return( time.time() - t0)\n",
        "        \n",
        "t0 = time.time()\n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('time (s):', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "file1 = open(\"/content/url.txt\", 'r')\n",
        "###########################################################################################################################\n",
        "download_parallel(file1)\n"
      ],
      "metadata": {
        "id": "rtfs6--L7RMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "1e65ef7a-cf07-4fdf-dbcb-7267d3fe6ea8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-48af16815aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mfile1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/url.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m###########################################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdownload_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-48af16815aa0>\u001b[0m in \u001b[0;36mdownload_parallel\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpus\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time (s):'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    735\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import output\n",
        "\n",
        "# file1 = open(\"/content/url.txt\", 'r')\n",
        "# link_list = [f for f in enumerate(file1)]\n",
        "# for i,link in enumerate(link_list):\n",
        "#     print(\"ok\")\n",
        "#     !curl -L -O --remote-header-name \\\n",
        "#     --header \"Authorization: Bearer bVVLVOIv29Lds-zADthteUE_1QlykgndjN5T6BaKMzMS-A11Z8UWtVsNbAJ85LWcGGerQH1KpM7eb-1KZS_Nig\" \\\n",
        "#     --location {link_list[i][1]}\n",
        "#     output.clear()\n"
      ],
      "metadata": {
        "id": "690P6zBI_OEb",
        "outputId": "5a39cfd9-bdc5-49d1-b6bf-f77756f053b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# backend test works"
      ],
      "metadata": {
        "id": "8hKAHlIZEYv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pathb2[25:25+10]   #file naming <<<<"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6_NO_NNYDJp0",
        "outputId": "df1abcdd-fb64-4e40-cc3c-6d28556f315a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'doy2000089'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "# coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "# print(str(coonstant_c*pixel_count_snow),str(coonstant_c*pixel_count_notsnow),str(coonstant_c*pixel_count_snow+coonstant_c*pixel_count_notsnow))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uFk-Ego40dt",
        "outputId": "b77b8504-ee2a-44f9-b731-0da12a6b5f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "snow: 16636  not snow: 46171\n",
            "2888.1944439222593 8015.798609661855 10903.993053584114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pathb2 = imgs_list_b2[5]\n",
        "# pathb4 = imgs_list_b4[5]\n",
        "# pathb6 = imgs_list_b6[5]\n",
        "# #creating file NDSI\n",
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A {image_dir}{pathb4} \\\n",
        "#   --A_band 1 \\\n",
        "#   -B {image_dir}{pathb6} \\\n",
        "#   --B_band 1 \\\n",
        "#   --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "#   --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A {image_dir}{pathb2} \\\n",
        "#   --A_band 1 \\\n",
        "#   -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "#   --B_band 1 \\\n",
        "#   --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "#   --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.40*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyJZfC6Y8Uxj",
        "outputId": "ee301f3d-0e93-4a8c-91e7-be6a1ffb3bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import pymodis\n",
        "# import gdalnumeric\n",
        "# #to clear output\n",
        "# from google.colab import output\n",
        "\n",
        "\n",
        "# image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "# prefix = \"sur_refl_\"\n",
        "# bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "# DayOY = \"_doy\\d{7}_aid0001\"\n",
        "# fileExt = r'.tif'\n",
        "# temp_dir = r'/content/'\n",
        "# imgs_list = [f for f in os.listdir(image_dir) if f.endswith(fileExt)]\n",
        "# imgs_list.sort()\n",
        "# imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n",
        "# #creating a loop to run for all files in the directory having extension \".hdf\"\n",
        "# for name, image in enumerate(imgs_path):\n",
        "#   print('ok')\n",
        "#   print(name)\n",
        "#   print(image)\n",
        "#   path = imgs_list[name]\n",
        "#   #converting to sinusodial coordinate system to wgs 84 utm 43\n",
        "#   #import pymodis\n",
        "#   subset = [0,0,0,1,0,0,0]\n",
        "#   pymodis.convertmodis_gdal.convertModisGDAL( image_dir + path, temp_dir + path[:-4], subset, 2400,outformat=\"GTiff\",epsg=32643).run()\n",
        "  \n",
        "#   #cutting into Area Of interest using shape file\n",
        "#   !gdalwarp \\\n",
        "#   -cutline /content/beasBasinShapeFile.shp  -crop_to_cutline \\\n",
        "#   {temp_dir}{path[:-4]}\"_ndsi.tif\" \\\n",
        "#   {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\"\n",
        "\n",
        "#   !rm -r {temp_dir}{path[:-4]}\"_ndsi.tif\"\n",
        "#   #deleting file\n",
        "\n",
        "#   #creating file NDSI>=0.4\n",
        "#   !gdal_calc.py \\\n",
        "#   -A {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\" \\\n",
        "#   --outfile={temp_dir}{path[:-4]}\"_result.tif\" \\\n",
        "#   --calc=\"A/10>=40\"\n",
        "\n",
        "#   !rm -r {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\"\n",
        "#   #deleting file\n",
        "\n",
        "#   #counting Snow and Non-Snowpixels  \n",
        "#   #import gdalnumeric\n",
        "#   raster_file = gdalnumeric.LoadFile(temp_dir + path[:-4]+\"_result.tif\")\n",
        "#   pixel_count_snow = (raster_file ==0).sum()\n",
        "#   pixel_count_notsnow = (raster_file ==1).sum()\n",
        "#   print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)\n",
        "\n",
        "#   #creating constant for multiplication at pixel size with count of pixels\n",
        "#   m1 = !gdalinfo -json {temp_dir}{path[:-4]}\"_result.tif\" | jq -r .geoTransform \n",
        "#   coonstant_c = int(m1[2][9:9+4])*int(m1[6][9+1:9+4+1])/1000000\n",
        "\n",
        "#   !rm -r {temp_dir}{path[:-4]}\"_result.tif\"\n",
        "#   #deleting file\n",
        "\n",
        "#   #combining name of the  file , SnowCoverArea , NonSnowCoverArea to a text file format\n",
        "#   lines = str(path[8:8+8] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow))\n",
        "#   with open(temp_dir + \"out.txt\", \"a+\", encoding = \"utf-8\") as f:\n",
        "#     f.writelines('\\n' + lines)\n",
        "#     output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "ioDVU1ltWkZ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}