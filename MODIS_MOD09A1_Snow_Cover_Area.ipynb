{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MODIS_MOD09A1_Snow_Cover_Area.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2022/blob/main/MODIS_MOD09A1_Snow_Cover_Area.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " >MOD09A1.061 data DOWNLOAD & PROCESSING calculating NDSI  > 0.4 && band2 reflectance is more than 11%"
      ],
      "metadata": {
        "id": "hlcgiXaXS1-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v6GiEeRvYbBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NbFxO-hu1Tx"
      },
      "source": [
        "## **RUNNING CODE NDSI *MOD09A1.061* 8 DAY**  \n",
        "## ðŸŒ¨ï¸â„ï¸ðŸ”ï¸  **SNOW COVER AREA**  ðŸ”ï¸â„ï¸ðŸŒ¨ï¸"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "Xx-WJA_Bhm3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre requisites please run this section >>>>"
      ],
      "metadata": {
        "id": "E8YMNHt3soFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq"
      ],
      "metadata": {
        "id": "79zaMlOGsmAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown https://drive.google.com/uc?id=1oyqXeHZgaTOjLod-Vqz-VAzS88O13JDr\n",
        "# !unzip /content/beasBasinShapeFile.zip -d /content"
      ],
      "metadata": {
        "id": "s73AxlUG7E0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK\n",
        "!unzip /content/elev_correct.zip\n",
        "!gdown https://drive.google.com/uc?id=18n7BBOAzzNqAKGjtEyvLqtfxWuCr3Tdf\n",
        "!unzip /content/elev_polygon_shp.zip"
      ],
      "metadata": {
        "id": "BuiH0mvfFZix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Data Here"
      ],
      "metadata": {
        "id": "PmqyTwzIyPI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "save_dir = \"/content/elev_polygon_shp/\"\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "imgs_list_b4 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b4)]\n",
        "imgs_list_b6 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b6)]\n",
        "imgs_list_b2.sort()\n",
        "imgs_list_b4.sort()\n",
        "imgs_list_b6.sort()\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "imgs_path_b4 = [os.path.join(image_dir, i) for i in imgs_list_b4 if i != 'outputs']\n",
        "imgs_path_b6 = [os.path.join(image_dir, i) for i in imgs_list_b6 if i != 'outputs']\n",
        "print(len(imgs_path_b2),len(imgs_path_b4),len(imgs_path_b6))\n",
        "\n",
        "data_output = image_dir[:-6] + \"DATA/AspectOutfinalx.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "if(path.exists(data_output) == False):\n",
        "  f_new = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  f_new.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      pathb4 = imgs_list_b4[i]\n",
        "      pathb6 = imgs_list_b6[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[25:25+10])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    pathb4 = imgs_list_b4[i]\n",
        "    pathb6 = imgs_list_b6[i]\n",
        "    data = []\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[25:25+10])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[25:25+10]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[25:25+10]), \"1,\" + str(pathb2[25:25+10]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[25:25+10]))\n",
        "      ########################################################################\n",
        "        #creating file NDSI\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb4} \\\n",
        "          --A_band 1 \\\n",
        "          -B {image_dir}{pathb6} \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "          --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb2} \\\n",
        "          --A_band 1 \\\n",
        "          -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "          --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "        !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        #counting Snow and Non-Snowpixels  \n",
        "        #import gdalnumeric\n",
        "        raster_file = gdalnumeric.LoadFile(temp_dir+\"BothCheck_result.tif\")\n",
        "        pixel_count_snow = (raster_file ==1).sum()\n",
        "        pixel_count_notsnow = (raster_file ==0).sum()\n",
        "        print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)\n",
        "\n",
        "        SC_values = []\n",
        "        ########################################################################\n",
        "        temp_dir_elevations = r'/content/elev_correct/E'\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          print(j)\n",
        "          !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "            {temp_dir}\"BothCheck_result.tif\" \\\n",
        "            {temp_dir}result_\"{str(j)}\".tif\n",
        "\n",
        "          #counting Snow and Non-Snowpixels  \n",
        "          #import gdalnumeric\n",
        "          raster_file = gdalnumeric.LoadFile(temp_dir+\"result_\"+str(j)+\".tif\")\n",
        "          pixel_nos_snow = (raster_file ==1).sum()\n",
        "          pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "          SC_values.append(pixel_nos_snow)\n",
        "          SC_values.append(pixel_nos_notsnow)\n",
        "          ###########################################################################\n",
        "          #os.system (cmd)\n",
        "          # slp1=gdal.Open(temp_dir+\"result_\"+str(j)+\".tif\")\n",
        "          # slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "          # plt.figure()\n",
        "          # plt.imshow(slp1Array)\n",
        "          # plt.colorbar()\n",
        "          # plt.show()\n",
        "          ###########################################################################\n",
        "          !rm -r {temp_dir}result_{str(j)}.tif\n",
        "        ########################################################################\n",
        "        \n",
        "        SC_aspect_values = []\n",
        "        ########################################################################\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          dir_to_store = \"/content/polygon_shp/A\"+str(elev[j])+\"_shpFile/\"\n",
        "          for i in range(1,9):\n",
        "            !gdalwarp \\\n",
        "              --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "              -overwrite -of GTiff -ot Float32 \\\n",
        "              -cutline {dir_to_store}Cresult_D{i}.shp -cl Cresult_D{i} -crop_to_cutline \\\n",
        "              {temp_dir}\"BothCheck_result.tif\" \\\n",
        "              {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "            #counting Snow and Non-Snowpixels  \n",
        "            #import gdalnumeric\n",
        "            print(\"calculating \", i ,\"\\n\")\n",
        "            raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "            pixel_nos_snow = (raster_file ==1).sum()\n",
        "            pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "            SC_aspect_values.append(pixel_nos_snow)\n",
        "            SC_aspect_values.append(pixel_nos_notsnow)\n",
        "            !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "        ########################################################################\n",
        "        ########################################################################\n",
        "        m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "        ########################################################################\n",
        "        !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "        ########################################################################\n",
        "        area_sc_values = [number * coonstant_c for number in SC_values]\n",
        "        ########################################################################\n",
        "        area_aspect_sc_values = [number * coonstant_c for number in SC_aspect_values]\n",
        "        print(str(coonstant_c*pixel_count_snow),str(coonstant_c*pixel_count_notsnow),str(coonstant_c*pixel_count_snow+coonstant_c*pixel_count_notsnow))\n",
        "        ########################################################################\n",
        "        lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow) + \",\" + str(area_sc_values)[1:-1] + \",\" + str(area_aspect_sc_values)[1:-1])\n",
        "        ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[25:25+10]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "O4TyjxtGGWah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ“œDATA DOWNLOAD SECTIONðŸ“œ**"
      ],
      "metadata": {
        "id": "I7d6uagHFbGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### APPEEAR LDDAC DATA DOWNLOAD"
      ],
      "metadata": {
        "id": "XsU-Aivn7ihA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1SrMZ6KTEpO0yuTwm9DlLFREjksvn1_mZ"
      ],
      "metadata": {
        "id": "p4N7TJmtX7K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/OUT/data/MOD09A1061/files\n",
        "!ls /content/drive/MyDrive/OUT/data/MOD09A1061/files\n",
        "%cd /content/drive/MyDrive/OUT/data/MOD09A1061/files"
      ],
      "metadata": {
        "id": "cLh93IAEYs04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try download the list data"
      ],
      "metadata": {
        "id": "Iia9EHMD72b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --request POST --user kroy0001:/#j%kWrPA,8.HRe --header \"Content-Length: 0\" \"https://appeears.earthdatacloud.nasa.gov/api/login\""
      ],
      "metadata": {
        "id": "N7PNs-Mu9ibu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !curl -L -O --remote-header-name \\\n",
        "#   --header \"Authorization: Bearer bVVLVOIv29Lds-zADthteUE_1QlykgndjN5T6BaKMzMS-A11Z8UWtVsNbAJ85LWcGGerQH1KpM7eb-1KZS_Nig\" \\\n",
        "#   --location https://appeears.earthdatacloud.nasa.gov/api/bundle/908b9b61-5acf-48ca-933e-1fcd3b2704fc/c4d1addc-4e43-43e6-aac4-04cdcf04faca/MOD09A1.061_sur_refl_b01_doy2000129_aid0001.tif"
      ],
      "metadata": {
        "id": "IDFusRPU9up5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "\n",
        "def download_url(url):\n",
        "    t0 = time.time()\n",
        "###########################################################################################################################\n",
        "    !curl -L -O --remote-header-name \\\n",
        "      --header \"Authorization: Bearer B9rG-ZpBea75Ds9E_jOGc-DC5_C1vel5aPMln6KTulGEDa0U2OpxTt7TyicnkyDBYpgjqhnk7aFp9e-juzo2Qg\" \\\n",
        "      --location {url}\n",
        "###########################################################################################################################\n",
        "    return( time.time() - t0)\n",
        "        \n",
        "t0 = time.time()\n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('time (s):', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "file1 = open(\"/content/url.txt\", 'r')\n",
        "###########################################################################################################################\n",
        "download_parallel(file1)\n"
      ],
      "metadata": {
        "id": "rtfs6--L7RMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import output\n",
        "\n",
        "# file1 = open(\"/content/url.txt\", 'r')\n",
        "# link_list = [f for f in enumerate(file1)]\n",
        "# for i,link in enumerate(link_list):\n",
        "#     print(\"ok\")\n",
        "#     !curl -L -O --remote-header-name \\\n",
        "#     --header \"Authorization: Bearer bVVLVOIv29Lds-zADthteUE_1QlykgndjN5T6BaKMzMS-A11Z8UWtVsNbAJ85LWcGGerQH1KpM7eb-1KZS_Nig\" \\\n",
        "#     --location {link_list[i][1]}\n",
        "#     output.clear()\n"
      ],
      "metadata": {
        "id": "690P6zBI_OEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# backend test works"
      ],
      "metadata": {
        "id": "8hKAHlIZEYv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pathb2[25:25+10]   #file naming <<<<"
      ],
      "metadata": {
        "id": "6_NO_NNYDJp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "# coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "# print(str(coonstant_c*pixel_count_snow),str(coonstant_c*pixel_count_notsnow),str(coonstant_c*pixel_count_snow+coonstant_c*pixel_count_notsnow))\n",
        "    "
      ],
      "metadata": {
        "id": "9uFk-Ego40dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pathb2 = imgs_list_b2[5]\n",
        "# pathb4 = imgs_list_b4[5]\n",
        "# pathb6 = imgs_list_b6[5]\n",
        "# #creating file NDSI\n",
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A {image_dir}{pathb4} \\\n",
        "#   --A_band 1 \\\n",
        "#   -B {image_dir}{pathb6} \\\n",
        "#   --B_band 1 \\\n",
        "#   --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "#   --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A {image_dir}{pathb2} \\\n",
        "#   --A_band 1 \\\n",
        "#   -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "#   --B_band 1 \\\n",
        "#   --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "#   --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.40*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#"
      ],
      "metadata": {
        "id": "xyJZfC6Y8Uxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import pymodis\n",
        "# import gdalnumeric\n",
        "# #to clear output\n",
        "# from google.colab import output\n",
        "\n",
        "\n",
        "# image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "# prefix = \"sur_refl_\"\n",
        "# bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "# DayOY = \"_doy\\d{7}_aid0001\"\n",
        "# fileExt = r'.tif'\n",
        "# temp_dir = r'/content/'\n",
        "# imgs_list = [f for f in os.listdir(image_dir) if f.endswith(fileExt)]\n",
        "# imgs_list.sort()\n",
        "# imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n",
        "# #creating a loop to run for all files in the directory having extension \".hdf\"\n",
        "# for name, image in enumerate(imgs_path):\n",
        "#   print('ok')\n",
        "#   print(name)\n",
        "#   print(image)\n",
        "#   path = imgs_list[name]\n",
        "#   #converting to sinusodial coordinate system to wgs 84 utm 43\n",
        "#   #import pymodis\n",
        "#   subset = [0,0,0,1,0,0,0]\n",
        "#   pymodis.convertmodis_gdal.convertModisGDAL( image_dir + path, temp_dir + path[:-4], subset, 2400,outformat=\"GTiff\",epsg=32643).run()\n",
        "  \n",
        "#   #cutting into Area Of interest using shape file\n",
        "#   !gdalwarp \\\n",
        "#   -cutline /content/beasBasinShapeFile.shp  -crop_to_cutline \\\n",
        "#   {temp_dir}{path[:-4]}\"_ndsi.tif\" \\\n",
        "#   {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\"\n",
        "\n",
        "#   !rm -r {temp_dir}{path[:-4]}\"_ndsi.tif\"\n",
        "#   #deleting file\n",
        "\n",
        "#   #creating file NDSI>=0.4\n",
        "#   !gdal_calc.py \\\n",
        "#   -A {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\" \\\n",
        "#   --outfile={temp_dir}{path[:-4]}\"_result.tif\" \\\n",
        "#   --calc=\"A/10>=40\"\n",
        "\n",
        "#   !rm -r {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\"\n",
        "#   #deleting file\n",
        "\n",
        "#   #counting Snow and Non-Snowpixels  \n",
        "#   #import gdalnumeric\n",
        "#   raster_file = gdalnumeric.LoadFile(temp_dir + path[:-4]+\"_result.tif\")\n",
        "#   pixel_count_snow = (raster_file ==0).sum()\n",
        "#   pixel_count_notsnow = (raster_file ==1).sum()\n",
        "#   print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)\n",
        "\n",
        "#   #creating constant for multiplication at pixel size with count of pixels\n",
        "#   m1 = !gdalinfo -json {temp_dir}{path[:-4]}\"_result.tif\" | jq -r .geoTransform \n",
        "#   coonstant_c = int(m1[2][9:9+4])*int(m1[6][9+1:9+4+1])/1000000\n",
        "\n",
        "#   !rm -r {temp_dir}{path[:-4]}\"_result.tif\"\n",
        "#   #deleting file\n",
        "\n",
        "#   #combining name of the  file , SnowCoverArea , NonSnowCoverArea to a text file format\n",
        "#   lines = str(path[8:8+8] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow))\n",
        "#   with open(temp_dir + \"out.txt\", \"a+\", encoding = \"utf-8\") as f:\n",
        "#     f.writelines('\\n' + lines)\n",
        "#     output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "ioDVU1ltWkZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸªœSLOPEðŸªœ DATA SNOW COVER CALCULATOR >>>"
      ],
      "metadata": {
        "id": "m7fIE_mGhWmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "nEHp5mXEhuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq"
      ],
      "metadata": {
        "id": "OEuDDcBlhuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1hJqzhX9smqsTlMvWyT3U2vc9X52rLbmI #downloading SLOPE-MAP\n",
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "QNNIqyxOhuFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "imgs_list_b4 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b4)]\n",
        "imgs_list_b6 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b6)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_list_b4.sort(reverse=True)\n",
        "imgs_list_b6.sort(reverse=True)\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "imgs_path_b4 = [os.path.join(image_dir, i) for i in imgs_list_b4 if i != 'outputs']\n",
        "imgs_path_b6 = [os.path.join(image_dir, i) for i in imgs_list_b6 if i != 'outputs']\n",
        "print(len(imgs_path_b2),len(imgs_path_b4),len(imgs_path_b6))\n",
        "\n",
        "data_output = image_dir[:-6] + \"DATA/SlopeOutfinalTestxxx.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "if(path.exists(data_output) == False):\n",
        "  fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  fw.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      pathb4 = imgs_list_b4[i]\n",
        "      pathb6 = imgs_list_b6[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[25:25+10])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    pathb4 = imgs_list_b4[i]\n",
        "    pathb6 = imgs_list_b6[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[25:25+10])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[25:25+10]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[25:25+10]), \"1,\" + str(pathb2[25:25+10]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[25:25+10]))\n",
        "        ########################################################################\n",
        "        #creating file NDSI\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb4} \\\n",
        "          --A_band 1 \\\n",
        "          -B {image_dir}{pathb6} \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "          --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb2} \\\n",
        "          --A_band 1 \\\n",
        "          -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "          --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "\n",
        "        !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "        #deleting file\n",
        "        ########################################################################\n",
        "        #counting Snow and Non-Snowpixels  \n",
        "        raster_file = gdalnumeric.LoadFile(temp_dir+\"BothCheck_result.tif\")\n",
        "        pixel_count_snow = (raster_file ==1).sum()\n",
        "        pixel_count_notsnow = (raster_file ==0).sum()\n",
        "        print(\"snow:\", pixel_count_snow,\" not snow:\", pixel_count_notsnow)\n",
        "\n",
        "        ######################################READING SLOPE MAP HAVING 9 CLASSES##################\n",
        "        INPUT_SLOPE_IN_IMAGE = \"/content/Reclass_Slop21.tif\"\n",
        "        !gdal_translate -outsize 454 233 {INPUT_SLOPE_IN_IMAGE} {temp_dir}output.tif   #<<<< RESIZING SLOPE MAP for size of NDSI result\n",
        "        ##############################RASTER CALCULATION OF SLOPEMAP & NDSI RESULT#########\n",
        "        ######################################CREATING SCA ON 9 SLOPES##################\n",
        "        for i in range(1,10):\n",
        "          !gdal_calc.py \\\n",
        "            --overwrite \\\n",
        "            --type=Float32 \\\n",
        "            -A {temp_dir}output.tif \\\n",
        "            --A_band 1 \\\n",
        "            -B {temp_dir}BothCheck_result.tif \\\n",
        "            --B_band 1 \\\n",
        "            --outfile=SmallerFileB\"{str(i)}\".tif \\\n",
        "            --NoDataValue=0 \\\n",
        "            --calc=\"((A=={i})*1)*(B.astype(float))\"\n",
        "        ######################################CREATING SCA ON 9 SLOPES##################\n",
        "        ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        import gdalnumeric\n",
        "        SC_Slope_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          for i in range(1,10):\n",
        "            !gdalwarp \\\n",
        "              --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "              -overwrite -of GTiff -ot Float32 \\\n",
        "              -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "              {temp_dir}SmallerFileB\"{str(i)}\".tif \\\n",
        "              {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "\n",
        "            #counting Snow and Non-Snowpixels\n",
        "            raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "            pixel_nos_snow = (raster_file ==1).sum(); pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "            SC_Slope_values.append(pixel_nos_snow) #; SC_Slope_values.append(pixel_nos_notsnow)\n",
        "            !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "        print(len(SC_Slope_values))\n",
        "        ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        ########################################################################\n",
        "        m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "        ########################################################################\n",
        "        !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "        #######################################################################\n",
        "        area_slope_sc_values = [number * coonstant_c for number in SC_Slope_values]\n",
        "        area_slope_sc_values = [int(x) for x in area_slope_sc_values]\n",
        "        print(str(coonstant_c*pixel_count_snow),str(coonstant_c*pixel_count_notsnow),str(coonstant_c*pixel_count_snow+coonstant_c*pixel_count_notsnow))\n",
        "        #######################################################################\n",
        "        lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow) + \",\" + str(area_slope_sc_values)[1:-1])\n",
        "        # lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow))\n",
        "\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[25:25+10]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n"
      ],
      "metadata": {
        "id": "WKgCghZLi9SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAINFALL DATA CALCULATOR >>>**"
      ],
      "metadata": {
        "id": "SVspPObHbvl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c10bbbf-0356-4e75-b024-6c6ca626579d",
        "id": "g0uX07f7bvl6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq"
      ],
      "metadata": {
        "id": "-gd5_Y9Ubvl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "LwWc4vYLbvl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/rainfall/data/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"chirps_global_pentad_data_\"\n",
        "DayOY = \"[0-9]+_[0-9]+\"\n",
        "fileExt = r'.tiff'\n",
        "expression_temp = prefix\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_temp)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "print(len(imgs_path_b2))\n",
        "\n",
        "data_output = image_dir[:-6] + \"/RainfallOutfinalTestxxx.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "if(path.exists(data_output) == False):\n",
        "  fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  fw.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[26:26+17])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[26:26+17])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[26:26+17]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[26:26+17]), \"1,\" + str(pathb2[26:26+17]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[26:26+17]))\n",
        " ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        import gdalnumeric\n",
        "        Rainfall_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "            {image_dir}{pathb2} \\\n",
        "            {temp_dir}Xresult_\"{str(j)}\".tif\n",
        "          \n",
        "          #calculating mean rainfall for the elevation sections\n",
        "          raster = gdal.Open(str(temp_dir)+\"Xresult_\"+str(j)+\".tif\")\n",
        "          rain = raster.GetRasterBand(1).GetStatistics(0,1)\n",
        "          print(rain)\n",
        "          Rainfall_values.append(rain[2]) \n",
        "          !rm -r {temp_dir}Xresult_{str(j)}.tif\n",
        "        print(len(Rainfall_values))\n",
        "        lines = str(pathb2[26:26+17] +\",\"+ str(Rainfall_values)[1:-1])\n",
        "        # lines = str(pathb2[26:26+17] + \",\" + str(constant_c*pixel_count_snow) + \",\" + str(constant_c*pixel_count_notsnow))\n",
        "\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[26:26+17]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n"
      ],
      "metadata": {
        "id": "YNmBzY9sbvl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEMPERATURE DATA CALCULATOR >>>**"
      ],
      "metadata": {
        "id": "1GazSrsHCwzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c10bbbf-0356-4e75-b024-6c6ca626579d",
        "id": "hGsijHZoCwzC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq"
      ],
      "metadata": {
        "id": "UuYd5_ICCwzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "POo0Q1ZdCwzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"lstc6_global_dekad_data_\"\n",
        "DayOY = \"[0-9]+_[0-9]+\"\n",
        "fileExt = r'.tiff'\n",
        "expression_temp = prefix\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_temp)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "print(len(imgs_path_b2))\n",
        "\n",
        "data_output = image_dir[:-6] + \"/TemperatureOutfinalTestxxx.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "if(path.exists(data_output) == False):\n",
        "  fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  fw.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[24:23+18])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[24:23+18])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[24:23+18]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[24:23+18]), \"1,\" + str(pathb2[24:23+18]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[24:23+18]))\n",
        " ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        import gdalnumeric\n",
        "        Temperature_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "            {image_dir}{pathb2} \\\n",
        "            {temp_dir}Xresult_\"{str(j)}\".tif\n",
        "          \n",
        "          #calculating mean Temperature_values for the elevation sections\n",
        "          raster = gdal.Open(str(temp_dir)+\"Xresult_\"+str(j)+\".tif\")\n",
        "          temp = raster.GetRasterBand(1).GetStatistics(0,1)\n",
        "          print(temp)\n",
        "          Temperature_values.append(temp[2]) \n",
        "          !rm -r {temp_dir}Xresult_{str(j)}.tif\n",
        "        print(len(Temperature_values))\n",
        "        lines = str(pathb2[24:23+18] +\",\"+ str(Temperature_values)[1:-1])\n",
        "        # lines = str(pathb2[24:23+18] + \",\" + str(constant_c*pixel_count_snow) + \",\" + str(constant_c*pixel_count_notsnow))\n",
        "\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[24:23+18]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n"
      ],
      "metadata": {
        "id": "4Et5wXE1CwzD",
        "outputId": "6e90e716-4326-4633-cd0e-001849cf1dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok 210901_20210910.t 210901_20210910.t\n",
            "210901_20210910.t\n",
            "Creating output file that is 32P x 18L.\n",
            "Processing input file /content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data/lstc6_global_dekad_data_20210901_20210910.tiff.\n",
            "Using internal nodata values (e.g. -9999) for image /content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data/lstc6_global_dekad_data_20210901_20210910.tiff.\n",
            "Copying nodata values from source /content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data/lstc6_global_dekad_data_20210901_20210910.tiff to destination /content/Xresult_0.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "[2.4682276248932, 18.109395980835, 9.466851473652, 2.8656674614545]\n",
            "Creating output file that is 30P x 19L.\n",
            "Processing input file /content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data/lstc6_global_dekad_data_20210901_20210910.tiff.\n",
            "Using internal nodata values (e.g. -9999) for image /content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data/lstc6_global_dekad_data_20210901_20210910.tiff.\n",
            "Copying nodata values from source /content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data/lstc6_global_dekad_data_20210901_20210910.tiff to destination /content/Xresult_1.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "[5.3123688697815, 17.371192932129, 10.680920318165, 2.8550896128028]\n",
            "^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-d9369d558f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m           \u001b[0;31m#calculating mean rainfall for the elevation sections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m           \u001b[0mraster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Xresult_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".tif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m           \u001b[0mrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetRasterBand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetStatistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mOpen\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;34m\"\"\"Open(char const * utf8_path, GDALAccess eAccess) -> Dataset\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_gdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mOpenEx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: /content/Xresult_2.tif: No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RjK4PxVEFZdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ASPECT DATA SNOW COVER CALCULATOR >>>**"
      ],
      "metadata": {
        "id": "aIReqAKs4r-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25e1f72-aa52-4edc-d280-3be14a33486a",
        "id": "0bzIDwLG4r-C"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfTjeAyr4r-C",
        "outputId": "37edc418-019a-4a8a-a0a5-966562259ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyModis in /usr/local/lib/python3.7/dist-packages (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyModis) (2.23.0)\n",
            "Requirement already satisfied: GDAL in /usr/local/lib/python3.7/dist-packages (from pyModis) (2.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyModis) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyModis) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (2.10)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "jq is already the newest version (1.5+dfsg-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1vWxu7OaprxNLeIBYEU0QFwmH69uXZU5n #downloading MAP-DEM\n",
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IE7ycHC4r-C",
        "outputId": "ebb0b723-4857-43bc-82c1-ce524fd267c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vWxu7OaprxNLeIBYEU0QFwmH69uXZU5n\n",
            "To: /content/ASTER_DEM.tif\n",
            "100% 99.1M/99.1M [00:00<00:00, 221MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK\n",
            "To: /content/elev_correct.zip\n",
            "100% 480k/480k [00:00<00:00, 74.6MB/s]\n",
            "Archive:  /content/elev_correct.zip\n",
            "replace elev_correct/E1003000.cpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "imgs_list_b4 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b4)]\n",
        "imgs_list_b6 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b6)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_list_b4.sort(reverse=True)\n",
        "imgs_list_b6.sort(reverse=True)\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "imgs_path_b4 = [os.path.join(image_dir, i) for i in imgs_list_b4 if i != 'outputs']\n",
        "imgs_path_b6 = [os.path.join(image_dir, i) for i in imgs_list_b6 if i != 'outputs']\n",
        "print(len(imgs_path_b2),len(imgs_path_b4),len(imgs_path_b6))\n",
        "\n",
        "data_output = image_dir[:-6] + \"DATA/AspectOutfinalTestxxx.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "if(path.exists(data_output) == False):\n",
        "  fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  fw.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      pathb4 = imgs_list_b4[i]\n",
        "      pathb6 = imgs_list_b6[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[25:25+10])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    pathb4 = imgs_list_b4[i]\n",
        "    pathb6 = imgs_list_b6[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[25:25+10])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[25:25+10]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[25:25+10]), \"1,\" + str(pathb2[25:25+10]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[25:25+10]))\n",
        "        ########################################################################\n",
        "        #creating file NDSI\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb4} \\\n",
        "          --A_band 1 \\\n",
        "          -B {image_dir}{pathb6} \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "          --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb2} \\\n",
        "          --A_band 1 \\\n",
        "          -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "          --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "\n",
        "        !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "        #deleting file\n",
        "        ########################################################################\n",
        "        #counting Snow and Non-Snowpixels  \n",
        "        raster_file = gdalnumeric.LoadFile(temp_dir+\"BothCheck_result.tif\")\n",
        "        pixel_count_snow = (raster_file ==1).sum()\n",
        "        pixel_count_notsnow = (raster_file ==0).sum()\n",
        "        print(\"snow:\", pixel_count_snow,\" not snow:\", pixel_count_notsnow)\n",
        "\n",
        "        ######################################READING SLOPE MAP HAVING 9 CLASSES##################\n",
        "        INPUT_DEM_IN_IMAGE = \"/content/Reclass_Slop21.tif\"\n",
        "        !gdaldem aspect {INPUT_DEM_IN_IMAGE} {temp_dir}ASPECToutput.tif\n",
        "        !gdal_translate -outsize 454 233 {temp_dir}ASPECToutput.tif {temp_dir}output.tif   #<<<< RESIZING SLOPE MAP for size of NDSI result\n",
        "        ##############################RASTER CALCULATION OF SLOPEMAP & NDSI RESULT#########\n",
        "        ######################################CREATING SCA ON 9 SLOPES##################\n",
        "        angle = 22.5\n",
        "        last_num = int(360/angle)\n",
        "        for i in range(1, last_num + 1):\n",
        "          !gdal_calc.py \\\n",
        "            --overwrite \\\n",
        "            --type=Float32 \\\n",
        "            -A {temp_dir}output.tif \\\n",
        "            --A_band 1 \\\n",
        "            -B {temp_dir}BothCheck_result.tif \\\n",
        "            --B_band 1 \\\n",
        "            --outfile=SmallerFileB\"{str(i)}\".tif \\\n",
        "            --NoDataValue=0 \\\n",
        "            --calc=\"((A.astype(float)>{angle*(i-1)})*(A.astype(float)<={angle*i})*1)*(B.astype(float))\"\n",
        "        ######################################CREATING SCA ON 9 SLOPES##################\n",
        "        ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        import gdalnumeric\n",
        "        SC_Aspect_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          for i in range(1, last_num + 1):\n",
        "            !gdalwarp \\\n",
        "              --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "              -overwrite -of GTiff -ot Float32 \\\n",
        "              -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "              {temp_dir}SmallerFileB\"{str(i)}\".tif \\\n",
        "              {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "\n",
        "            #counting Snow and Non-Snowpixels\n",
        "            raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "            pixel_nos_snow = (raster_file ==1).sum(); pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "            SC_Aspect_values.append(pixel_nos_snow) #; SC_Aspect_values.append(pixel_nos_notsnow)\n",
        "            !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "        print(len(SC_Aspect_values))\n",
        "        ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        ########################################################################\n",
        "        m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "        ########################################################################\n",
        "        !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        constant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "        #######################################################################\n",
        "        area_Aspect_sc_values = [number * constant_c for number in SC_Aspect_values]\n",
        "        area_Aspect_sc_values = [int(x) for x in area_Aspect_sc_values]\n",
        "        print(str(constant_c * pixel_count_snow),str(constant_c * pixel_count_notsnow),str(constant_c * pixel_count_snow + constant_c * pixel_count_notsnow))\n",
        "        #######################################################################\n",
        "        lines = str(pathb2[25:25+10] + \",\" + str(constant_c * pixel_count_snow) + \",\" + str(constant_c * pixel_count_notsnow) + \",\" + str(area_Aspect_sc_values)[1:-1])\n",
        "        # lines = str(pathb2[25:25+10] + \",\" + str(constant_c*pixel_count_snow) + \",\" + str(constant_c*pixel_count_notsnow))\n",
        "\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[25:25+10]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "016357a9-c05c-4837-a1ff-3c6a14f5c757",
        "id": "ntxxZJ8Y4r-C"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok doy2019329 doy2019329\n",
            "doy2019329\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "snow: 17235  not snow: 45578\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Input file size is 6794, 3468\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB1.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB1.tif.\n",
            "Copying nodata values from source /content/SmallerFileB1.tif to destination /content/Xresult_1.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB2.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB2.tif.\n",
            "Copying nodata values from source /content/SmallerFileB2.tif to destination /content/Xresult_2.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB3.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB3.tif.\n",
            "Copying nodata values from source /content/SmallerFileB3.tif to destination /content/Xresult_3.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB4.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB4.tif.\n",
            "Copying nodata values from source /content/SmallerFileB4.tif to destination /content/Xresult_4.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB5.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB5.tif.\n",
            "Copying nodata values from source /content/SmallerFileB5.tif to destination /content/Xresult_5.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB6.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB6.tif.\n",
            "Copying nodata values from source /content/SmallerFileB6.tif to destination /content/Xresult_6.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB7.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB7.tif.\n",
            "Copying nodata values from source /content/SmallerFileB7.tif to destination /content/Xresult_7.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB8.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB8.tif.\n",
            "Copying nodata values from source /content/SmallerFileB8.tif to destination /content/Xresult_8.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB9.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB9.tif.\n",
            "Copying nodata values from source /content/SmallerFileB9.tif to destination /content/Xresult_9.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB10.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB10.tif.\n",
            "Copying nodata values from source /content/SmallerFileB10.tif to destination /content/Xresult_10.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB11.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB11.tif.\n",
            "Copying nodata values from source /content/SmallerFileB11.tif to destination /content/Xresult_11.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB12.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB12.tif.\n",
            "Copying nodata values from source /content/SmallerFileB12.tif to destination /content/Xresult_12.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB13.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB13.tif.\n",
            "Copying nodata values from source /content/SmallerFileB13.tif to destination /content/Xresult_13.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB14.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB14.tif.\n",
            "Copying nodata values from source /content/SmallerFileB14.tif to destination /content/Xresult_14.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB15.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB15.tif.\n",
            "Copying nodata values from source /content/SmallerFileB15.tif to destination /content/Xresult_15.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 391P x 220L.\n",
            "Processing input file /content/SmallerFileB16.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB16.tif.\n",
            "Copying nodata values from source /content/SmallerFileB16.tif to destination /content/Xresult_16.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB1.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB1.tif.\n",
            "Copying nodata values from source /content/SmallerFileB1.tif to destination /content/Xresult_1.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB2.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB2.tif.\n",
            "Copying nodata values from source /content/SmallerFileB2.tif to destination /content/Xresult_2.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB3.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB3.tif.\n",
            "Copying nodata values from source /content/SmallerFileB3.tif to destination /content/Xresult_3.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB4.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB4.tif.\n",
            "Copying nodata values from source /content/SmallerFileB4.tif to destination /content/Xresult_4.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB5.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB5.tif.\n",
            "Copying nodata values from source /content/SmallerFileB5.tif to destination /content/Xresult_5.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB6.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB6.tif.\n",
            "Copying nodata values from source /content/SmallerFileB6.tif to destination /content/Xresult_6.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB7.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB7.tif.\n",
            "Copying nodata values from source /content/SmallerFileB7.tif to destination /content/Xresult_7.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB8.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB8.tif.\n",
            "Copying nodata values from source /content/SmallerFileB8.tif to destination /content/Xresult_8.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB9.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB9.tif.\n",
            "Copying nodata values from source /content/SmallerFileB9.tif to destination /content/Xresult_9.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 363P x 229L.\n",
            "Processing input file /content/SmallerFileB10.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/SmallerFileB10.tif.\n",
            "Copying nodata values from source /content/SmallerFileB10.tif to destination /content/Xresult_10.tif.\n",
            "^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-63deea20ff32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m#counting Snow and Non-Snowpixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mraster_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdalnumeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Xresult_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".tif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mpixel_nos_snow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mraster_file\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpixel_nos_notsnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mraster_file\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mSC_Aspect_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_nos_snow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#; SC_Aspect_values.append(pixel_nos_notsnow)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/osgeo/gdal_array.py\u001b[0m in \u001b[0;36mLoadFile\u001b[0;34m(filename, xoff, yoff, xsize, ysize, buf_xsize, buf_ysize, buf_type, resample_alg, callback, callback_data)\u001b[0m\n\u001b[1;32m    222\u001b[0m               \u001b[0mresample_alg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRIORA_NearestNeighbour\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m               callback=None, callback_data=None ):\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't open \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetLastErrorMsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mOpen\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;34m\"\"\"Open(char const * utf8_path, GDALAccess eAccess) -> Dataset\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_gdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mOpenEx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: `/content/Xresult_10.tif' not recognized as a supported file format."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nj0ZA6BZbvID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}