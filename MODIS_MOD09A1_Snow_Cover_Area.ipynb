{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MODIS_MOD09A1_Snow_Cover_Area.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2022/blob/main/MODIS_MOD09A1_Snow_Cover_Area.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " >MOD09A1.061 data DOWNLOAD & PROCESSING calculating NDSI  > 0.4 && band2 reflectance is more than 11%"
      ],
      "metadata": {
        "id": "hlcgiXaXS1-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v6GiEeRvYbBi",
        "outputId": "81a6f113-6a3b-4f0a-a689-1c29195c87b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NbFxO-hu1Tx"
      },
      "source": [
        "## **RUNNING CODE NDSI *MOD09A1.061* 8 DAY**  \n",
        "## ðŸŒ¨ï¸â„ï¸ðŸ”ï¸  **SNOW COVER AREA**  ðŸ”ï¸â„ï¸ðŸŒ¨ï¸"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "Xx-WJA_Bhm3E",
        "outputId": "7230e6f7-2760-4f39-e546-6b4bd5295385",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre requisites please run this section >>>>"
      ],
      "metadata": {
        "id": "E8YMNHt3soFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq"
      ],
      "metadata": {
        "id": "79zaMlOGsmAE",
        "outputId": "d4007734-4c07-40fc-a7b3-6b5a728fc426",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyModis\n",
            "  Downloading pyModis-2.3.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyModis) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyModis) (2.23.0)\n",
            "Requirement already satisfied: GDAL in /usr/local/lib/python3.7/dist-packages (from pyModis) (2.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyModis) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyModis) (2.10)\n",
            "Installing collected packages: pyModis\n",
            "Successfully installed pyModis-2.3.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libjq1 libonig4\n",
            "The following NEW packages will be installed:\n",
            "  jq libjq1 libonig4\n",
            "0 upgraded, 3 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 276 kB of archives.\n",
            "After this operation, 930 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libonig4 amd64 6.7.0-1 [119 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libjq1 amd64 1.5+dfsg-2 [111 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 jq amd64 1.5+dfsg-2 [45.6 kB]\n",
            "Fetched 276 kB in 1s (482 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libonig4:amd64.\n",
            "(Reading database ... 155685 files and directories currently installed.)\n",
            "Preparing to unpack .../libonig4_6.7.0-1_amd64.deb ...\n",
            "Unpacking libonig4:amd64 (6.7.0-1) ...\n",
            "Selecting previously unselected package libjq1:amd64.\n",
            "Preparing to unpack .../libjq1_1.5+dfsg-2_amd64.deb ...\n",
            "Unpacking libjq1:amd64 (1.5+dfsg-2) ...\n",
            "Selecting previously unselected package jq.\n",
            "Preparing to unpack .../jq_1.5+dfsg-2_amd64.deb ...\n",
            "Unpacking jq (1.5+dfsg-2) ...\n",
            "Setting up libonig4:amd64 (6.7.0-1) ...\n",
            "Setting up libjq1:amd64 (1.5+dfsg-2) ...\n",
            "Setting up jq (1.5+dfsg-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown https://drive.google.com/uc?id=1oyqXeHZgaTOjLod-Vqz-VAzS88O13JDr\n",
        "# !unzip /content/beasBasinShapeFile.zip -d /content"
      ],
      "metadata": {
        "id": "s73AxlUG7E0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK\n",
        "!unzip /content/elev_correct.zip\n",
        "!gdown https://drive.google.com/uc?id=18n7BBOAzzNqAKGjtEyvLqtfxWuCr3Tdf\n",
        "!unzip /content/elev_polygon_shp.zip"
      ],
      "metadata": {
        "id": "BuiH0mvfFZix",
        "outputId": "658055b6-3990-48a3-cd4e-05726b370d5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK\n",
            "To: /content/elev_correct.zip\n",
            "\r  0% 0.00/480k [00:00<?, ?B/s]\r100% 480k/480k [00:00<00:00, 112MB/s]\n",
            "Archive:  /content/elev_correct.zip\n",
            " extracting: elev_correct/E1003000.cpg  \n",
            "  inflating: elev_correct/E1003000.dbf  \n",
            "  inflating: elev_correct/E1003000.prj  \n",
            "  inflating: elev_correct/E1003000.qmd  \n",
            "  inflating: elev_correct/E1003000.shp  \n",
            "  inflating: elev_correct/E1003000.shx  \n",
            " extracting: elev_correct/E30005000.cpg  \n",
            "  inflating: elev_correct/E30005000.dbf  \n",
            "  inflating: elev_correct/E30005000.prj  \n",
            "  inflating: elev_correct/E30005000.qmd  \n",
            "  inflating: elev_correct/E30005000.shp  \n",
            "  inflating: elev_correct/E30005000.shx  \n",
            " extracting: elev_correct/E50007000.cpg  \n",
            "  inflating: elev_correct/E50007000.dbf  \n",
            "  inflating: elev_correct/E50007000.prj  \n",
            "  inflating: elev_correct/E50007000.qmd  \n",
            "  inflating: elev_correct/E50007000.shp  \n",
            "  inflating: elev_correct/E50007000.shx  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18n7BBOAzzNqAKGjtEyvLqtfxWuCr3Tdf\n",
            "To: /content/elev_polygon_shp.zip\n",
            "100% 55.5M/55.5M [00:00<00:00, 109MB/s]\n",
            "Archive:  /content/elev_polygon_shp.zip\n",
            "   creating: polygon_shp/A1003000_shpFile/\n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D1.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D1.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D1.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D1.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D1.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D1.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D2.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D2.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D2.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D2.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D2.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D2.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D3.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D3.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D3.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D3.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D3.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D3.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D4.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D4.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D4.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D4.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D4.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D4.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D5.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D5.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D5.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D5.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D5.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D5.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D6.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D6.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D6.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D6.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D6.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D6.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D7.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D7.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D7.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D7.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D7.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D7.shx  \n",
            " extracting: polygon_shp/A1003000_shpFile/Cresult_D8.cpg  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D8.dbf  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D8.prj  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D8.qmd  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D8.shp  \n",
            "  inflating: polygon_shp/A1003000_shpFile/Cresult_D8.shx  \n",
            "   creating: polygon_shp/A30005000_shpFile/\n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D1.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D1.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D1.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D1.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D1.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D1.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D2.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D2.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D2.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D2.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D2.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D2.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D3.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D3.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D3.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D3.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D3.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D3.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D4.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D4.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D4.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D4.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D4.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D4.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D5.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D5.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D5.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D5.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D5.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D5.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D6.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D6.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D6.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D6.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D6.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D6.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D7.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D7.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D7.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D7.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D7.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D7.shx  \n",
            " extracting: polygon_shp/A30005000_shpFile/Cresult_D8.cpg  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D8.dbf  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D8.prj  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D8.qmd  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D8.shp  \n",
            "  inflating: polygon_shp/A30005000_shpFile/Cresult_D8.shx  \n",
            "   creating: polygon_shp/A50007000_shpFile/\n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D1.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D1.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D1.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D1.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D1.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D1.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D2.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D2.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D2.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D2.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D2.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D2.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D3.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D3.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D3.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D3.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D3.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D3.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D4.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D4.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D4.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D4.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D4.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D4.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D5.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D5.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D5.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D5.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D5.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D5.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D6.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D6.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D6.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D6.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D6.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D6.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D7.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D7.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D7.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D7.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D7.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D7.shx  \n",
            " extracting: polygon_shp/A50007000_shpFile/Cresult_D8.cpg  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D8.dbf  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D8.prj  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D8.qmd  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D8.shp  \n",
            "  inflating: polygon_shp/A50007000_shpFile/Cresult_D8.shx  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Data Here"
      ],
      "metadata": {
        "id": "PmqyTwzIyPI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "save_dir = \"/content/elev_polygon_shp/\"\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "imgs_list_b4 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b4)]\n",
        "imgs_list_b6 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b6)]\n",
        "imgs_list_b2.sort()\n",
        "imgs_list_b4.sort()\n",
        "imgs_list_b6.sort()\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "imgs_path_b4 = [os.path.join(image_dir, i) for i in imgs_list_b4 if i != 'outputs']\n",
        "imgs_path_b6 = [os.path.join(image_dir, i) for i in imgs_list_b6 if i != 'outputs']\n",
        "print(len(imgs_path_b2),len(imgs_path_b4),len(imgs_path_b6))\n",
        "\n",
        "data_output = image_dir[:-6] + \"DATA/AspectOutfinalx.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "if(path.exists(data_output) == False):\n",
        "  f_new = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  f_new.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      pathb4 = imgs_list_b4[i]\n",
        "      pathb6 = imgs_list_b6[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[25:25+10])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    pathb4 = imgs_list_b4[i]\n",
        "    pathb6 = imgs_list_b6[i]\n",
        "    data = []\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[25:25+10])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        print('ok',str(pathb2[25:25+10]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[25:25+10]), \"1,\" + str(pathb2[25:25+10]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[25:25+10]))\n",
        "      ########################################################################\n",
        "        #creating file NDSI\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb4} \\\n",
        "          --A_band 1 \\\n",
        "          -B {image_dir}{pathb6} \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "          --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb2} \\\n",
        "          --A_band 1 \\\n",
        "          -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "          --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "        !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        #counting Snow and Non-Snowpixels  \n",
        "        #import gdalnumeric\n",
        "        raster_file = gdalnumeric.LoadFile(temp_dir+\"BothCheck_result.tif\")\n",
        "        pixel_count_snow = (raster_file ==1).sum()\n",
        "        pixel_count_notsnow = (raster_file ==0).sum()\n",
        "        print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)\n",
        "\n",
        "        SC_values = []\n",
        "        ########################################################################\n",
        "        temp_dir_elevations = r'/content/elev_correct/E'\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          print(j)\n",
        "          !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "            {temp_dir}\"BothCheck_result.tif\" \\\n",
        "            {temp_dir}result_\"{str(j)}\".tif\n",
        "\n",
        "          #counting Snow and Non-Snowpixels  \n",
        "          #import gdalnumeric\n",
        "          raster_file = gdalnumeric.LoadFile(temp_dir+\"result_\"+str(j)+\".tif\")\n",
        "          pixel_nos_snow = (raster_file ==1).sum()\n",
        "          pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "          SC_values.append(pixel_nos_snow)\n",
        "          SC_values.append(pixel_nos_notsnow)\n",
        "          ###########################################################################\n",
        "          #os.system (cmd)\n",
        "          # slp1=gdal.Open(temp_dir+\"result_\"+str(j)+\".tif\")\n",
        "          # slp1Array= slp1.GetRasterBand(1).ReadAsArray()\n",
        "          # plt.figure()\n",
        "          # plt.imshow(slp1Array)\n",
        "          # plt.colorbar()\n",
        "          # plt.show()\n",
        "          ###########################################################################\n",
        "          !rm -r {temp_dir}result_{str(j)}.tif\n",
        "        ########################################################################\n",
        "        \n",
        "        SC_aspect_values = []\n",
        "        ########################################################################\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          dir_to_store = \"/content/polygon_shp/A\"+str(elev[j])+\"_shpFile/\"\n",
        "          for i in range(1,9):\n",
        "            !gdalwarp \\\n",
        "              --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "              -overwrite -of GTiff -ot Float32 \\\n",
        "              -cutline {dir_to_store}Cresult_D{i}.shp -cl Cresult_D{i} -crop_to_cutline \\\n",
        "              {temp_dir}\"BothCheck_result.tif\" \\\n",
        "              {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "            #counting Snow and Non-Snowpixels  \n",
        "            #import gdalnumeric\n",
        "            print(\"calculating \", i ,\"\\n\")\n",
        "            raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "            pixel_nos_snow = (raster_file ==1).sum()\n",
        "            pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "            SC_aspect_values.append(pixel_nos_snow)\n",
        "            SC_aspect_values.append(pixel_nos_notsnow)\n",
        "            !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "        ########################################################################\n",
        "        ########################################################################\n",
        "        m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "        ########################################################################\n",
        "        !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "        ########################################################################\n",
        "        area_sc_values = [number * coonstant_c for number in SC_values]\n",
        "        ########################################################################\n",
        "        area_aspect_sc_values = [number * coonstant_c for number in SC_aspect_values]\n",
        "        print(str(coonstant_c*pixel_count_snow),str(coonstant_c*pixel_count_notsnow),str(coonstant_c*pixel_count_snow+coonstant_c*pixel_count_notsnow))\n",
        "        ########################################################################\n",
        "        lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow) + \",\" + str(area_sc_values)[1:-1] + \",\" + str(area_aspect_sc_values)[1:-1])\n",
        "        ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[25:25+10]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "O4TyjxtGGWah",
        "outputId": "ebdfe82a-a8e1-476e-f847-3b52c35e2ce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1021 1021 1021\n",
            "file ready please continue\n",
            "ok\n",
            "/content/drive/MyDrive/OUT/data/MOD09A1061/files/MOD09A1.061_sur_refl_b02_doy2000049_aid0001.tif\n",
            "ok doy2000049 doy2000049\n",
            "doy2000049\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "snow: 0  not snow: 0\n",
            "0\n",
            "Creating output file that is 389P x 219L.\n",
            "Processing input file /content/BothCheck_result.tif.\n",
            "Using internal nodata values (e.g. 3.40282e+38) for image /content/BothCheck_result.tif.\n",
            "Copying nodata values from source /content/BothCheck_result.tif to destination /content/result_0.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "1\n",
            "Creating output file that is 361P x 228L.\n",
            "Processing input file /content/BothCheck_result.tif.\n",
            "Using internal nodata values (e.g. 3.40282e+38) for image /content/BothCheck_result.tif.\n",
            "Copying nodata values from source /content/BothCheck_result.tif to destination /content/result_1.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "2\n",
            "Creating output file that is 265P x 182L.\n",
            "Processing input file /content/BothCheck_result.tif.\n",
            "Using internal nodata values (e.g. 3.40282e+38) for image /content/BothCheck_result.tif.\n",
            "Copying nodata values from source /content/BothCheck_result.tif to destination /content/result_2.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "Creating output file that is 389P x 216L.\n",
            "Processing input file /content/BothCheck_result.tif.\n",
            "Using internal nodata values (e.g. 3.40282e+38) for image /content/BothCheck_result.tif.\n",
            "Copying nodata values from source /content/BothCheck_result.tif to destination /content/Xresult_1.tif.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ“œDATA DOWNLOAD SECTIONðŸ“œ**"
      ],
      "metadata": {
        "id": "I7d6uagHFbGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### APPEEAR LDDAC DATA DOWNLOAD"
      ],
      "metadata": {
        "id": "XsU-Aivn7ihA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1SrMZ6KTEpO0yuTwm9DlLFREjksvn1_mZ"
      ],
      "metadata": {
        "id": "p4N7TJmtX7K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/OUT/data/MOD09A1061/files\n",
        "!ls /content/drive/MyDrive/OUT/data/MOD09A1061/files\n",
        "%cd /content/drive/MyDrive/OUT/data/MOD09A1061/files"
      ],
      "metadata": {
        "id": "cLh93IAEYs04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try download the list data"
      ],
      "metadata": {
        "id": "Iia9EHMD72b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --request POST --user kroy0001:/#j%kWrPA,8.HRe --header \"Content-Length: 0\" \"https://appeears.earthdatacloud.nasa.gov/api/login\""
      ],
      "metadata": {
        "id": "N7PNs-Mu9ibu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !curl -L -O --remote-header-name \\\n",
        "#   --header \"Authorization: Bearer bVVLVOIv29Lds-zADthteUE_1QlykgndjN5T6BaKMzMS-A11Z8UWtVsNbAJ85LWcGGerQH1KpM7eb-1KZS_Nig\" \\\n",
        "#   --location https://appeears.earthdatacloud.nasa.gov/api/bundle/908b9b61-5acf-48ca-933e-1fcd3b2704fc/c4d1addc-4e43-43e6-aac4-04cdcf04faca/MOD09A1.061_sur_refl_b01_doy2000129_aid0001.tif"
      ],
      "metadata": {
        "id": "IDFusRPU9up5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "\n",
        "def download_url(url):\n",
        "    t0 = time.time()\n",
        "###########################################################################################################################\n",
        "    !curl -L -O --remote-header-name \\\n",
        "      --header \"Authorization: Bearer B9rG-ZpBea75Ds9E_jOGc-DC5_C1vel5aPMln6KTulGEDa0U2OpxTt7TyicnkyDBYpgjqhnk7aFp9e-juzo2Qg\" \\\n",
        "      --location {url}\n",
        "###########################################################################################################################\n",
        "    return( time.time() - t0)\n",
        "        \n",
        "t0 = time.time()\n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('time (s):', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "file1 = open(\"/content/url.txt\", 'r')\n",
        "###########################################################################################################################\n",
        "download_parallel(file1)\n"
      ],
      "metadata": {
        "id": "rtfs6--L7RMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import output\n",
        "\n",
        "# file1 = open(\"/content/url.txt\", 'r')\n",
        "# link_list = [f for f in enumerate(file1)]\n",
        "# for i,link in enumerate(link_list):\n",
        "#     print(\"ok\")\n",
        "#     !curl -L -O --remote-header-name \\\n",
        "#     --header \"Authorization: Bearer bVVLVOIv29Lds-zADthteUE_1QlykgndjN5T6BaKMzMS-A11Z8UWtVsNbAJ85LWcGGerQH1KpM7eb-1KZS_Nig\" \\\n",
        "#     --location {link_list[i][1]}\n",
        "#     output.clear()\n"
      ],
      "metadata": {
        "id": "690P6zBI_OEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# backend test works"
      ],
      "metadata": {
        "id": "8hKAHlIZEYv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pathb2[25:25+10]   #file naming <<<<"
      ],
      "metadata": {
        "id": "6_NO_NNYDJp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "# coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "# print(str(coonstant_c*pixel_count_snow),str(coonstant_c*pixel_count_notsnow),str(coonstant_c*pixel_count_snow+coonstant_c*pixel_count_notsnow))\n",
        "    "
      ],
      "metadata": {
        "id": "9uFk-Ego40dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pathb2 = imgs_list_b2[5]\n",
        "# pathb4 = imgs_list_b4[5]\n",
        "# pathb6 = imgs_list_b6[5]\n",
        "# #creating file NDSI\n",
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A {image_dir}{pathb4} \\\n",
        "#   --A_band 1 \\\n",
        "#   -B {image_dir}{pathb6} \\\n",
        "#   --B_band 1 \\\n",
        "#   --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "#   --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "# !gdal_calc.py \\\n",
        "#   --overwrite \\\n",
        "#   --type=Float32 \\\n",
        "#   -A {image_dir}{pathb2} \\\n",
        "#   --A_band 1 \\\n",
        "#   -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "#   --B_band 1 \\\n",
        "#   --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "#   --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.40*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#"
      ],
      "metadata": {
        "id": "xyJZfC6Y8Uxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import pymodis\n",
        "# import gdalnumeric\n",
        "# #to clear output\n",
        "# from google.colab import output\n",
        "\n",
        "\n",
        "# image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "# prefix = \"sur_refl_\"\n",
        "# bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "# DayOY = \"_doy\\d{7}_aid0001\"\n",
        "# fileExt = r'.tif'\n",
        "# temp_dir = r'/content/'\n",
        "# imgs_list = [f for f in os.listdir(image_dir) if f.endswith(fileExt)]\n",
        "# imgs_list.sort()\n",
        "# imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n",
        "# #creating a loop to run for all files in the directory having extension \".hdf\"\n",
        "# for name, image in enumerate(imgs_path):\n",
        "#   print('ok')\n",
        "#   print(name)\n",
        "#   print(image)\n",
        "#   path = imgs_list[name]\n",
        "#   #converting to sinusodial coordinate system to wgs 84 utm 43\n",
        "#   #import pymodis\n",
        "#   subset = [0,0,0,1,0,0,0]\n",
        "#   pymodis.convertmodis_gdal.convertModisGDAL( image_dir + path, temp_dir + path[:-4], subset, 2400,outformat=\"GTiff\",epsg=32643).run()\n",
        "  \n",
        "#   #cutting into Area Of interest using shape file\n",
        "#   !gdalwarp \\\n",
        "#   -cutline /content/beasBasinShapeFile.shp  -crop_to_cutline \\\n",
        "#   {temp_dir}{path[:-4]}\"_ndsi.tif\" \\\n",
        "#   {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\"\n",
        "\n",
        "#   !rm -r {temp_dir}{path[:-4]}\"_ndsi.tif\"\n",
        "#   #deleting file\n",
        "\n",
        "#   #creating file NDSI>=0.4\n",
        "#   !gdal_calc.py \\\n",
        "#   -A {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\" \\\n",
        "#   --outfile={temp_dir}{path[:-4]}\"_result.tif\" \\\n",
        "#   --calc=\"A/10>=40\"\n",
        "\n",
        "#   !rm -r {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\"\n",
        "#   #deleting file\n",
        "\n",
        "#   #counting Snow and Non-Snowpixels  \n",
        "#   #import gdalnumeric\n",
        "#   raster_file = gdalnumeric.LoadFile(temp_dir + path[:-4]+\"_result.tif\")\n",
        "#   pixel_count_snow = (raster_file ==0).sum()\n",
        "#   pixel_count_notsnow = (raster_file ==1).sum()\n",
        "#   print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)\n",
        "\n",
        "#   #creating constant for multiplication at pixel size with count of pixels\n",
        "#   m1 = !gdalinfo -json {temp_dir}{path[:-4]}\"_result.tif\" | jq -r .geoTransform \n",
        "#   coonstant_c = int(m1[2][9:9+4])*int(m1[6][9+1:9+4+1])/1000000\n",
        "\n",
        "#   !rm -r {temp_dir}{path[:-4]}\"_result.tif\"\n",
        "#   #deleting file\n",
        "\n",
        "#   #combining name of the  file , SnowCoverArea , NonSnowCoverArea to a text file format\n",
        "#   lines = str(path[8:8+8] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow))\n",
        "#   with open(temp_dir + \"out.txt\", \"a+\", encoding = \"utf-8\") as f:\n",
        "#     f.writelines('\\n' + lines)\n",
        "#     output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "ioDVU1ltWkZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸªœSLOPEðŸªœ DATA SNOW COVER CALCULATOR >>>"
      ],
      "metadata": {
        "id": "m7fIE_mGhWmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "nEHp5mXEhuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq"
      ],
      "metadata": {
        "id": "OEuDDcBlhuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1ISVpFRmCNlRDHvIyVlYAgLMe2uZVaFou\n",
        "!unzip /content/slope_polygon_shp_files.zip"
      ],
      "metadata": {
        "id": "QNNIqyxOhuFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "save_dir = \"/content/slope_polygon_shp_files/\"\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "imgs_list_b4 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b4)]\n",
        "imgs_list_b6 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b6)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_list_b4.sort(reverse=True)\n",
        "imgs_list_b6.sort(reverse=True)\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "imgs_path_b4 = [os.path.join(image_dir, i) for i in imgs_list_b4 if i != 'outputs']\n",
        "imgs_path_b6 = [os.path.join(image_dir, i) for i in imgs_list_b6 if i != 'outputs']\n",
        "print(len(imgs_path_b2),len(imgs_path_b4),len(imgs_path_b6))\n",
        "\n",
        "data_output = image_dir[:-6] + \"DATA/SlopeOutfinalx.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "if(path.exists(data_output) == False):\n",
        "  f = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      pathb4 = imgs_list_b4[i]\n",
        "      pathb6 = imgs_list_b6[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[25:25+10])\n",
        "      f = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "      f.writelines(\"0,\" + lines + '\\n')\n",
        "  f.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    pathb4 = imgs_list_b4[i]\n",
        "    pathb6 = imgs_list_b6[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[25:25+10])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        print('ok',str(pathb2[25:25+10]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[25:25+10]), \"1,\" + str(pathb2[25:25+10]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[25:25+10]))\n",
        "        ########################################################################\n",
        "        #creating file NDSI\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb4} \\\n",
        "          --A_band 1 \\\n",
        "          -B {image_dir}{pathb6} \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "          --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb2} \\\n",
        "          --A_band 1 \\\n",
        "          -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "          --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "        !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "        #deleting file\n",
        "        ########################################################################\n",
        "        #counting Snow and Non-Snowpixels  \n",
        "        raster_file = gdalnumeric.LoadFile(temp_dir+\"BothCheck_result.tif\")\n",
        "        pixel_count_snow = (raster_file ==1).sum()\n",
        "        pixel_count_notsnow = (raster_file ==0).sum()\n",
        "        print(\"snow:\", pixel_count_snow,\" not snow:\", pixel_count_notsnow)\n",
        "\n",
        "        \n",
        "        SC_Slope_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "      ########################################################################\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          dir_to_store = str(save_dir)+\"A\"+str(elev[j])+\"_shpFile/\"\n",
        "          for i in range(1,10):\n",
        "            print(\"calculating \", i ,\"\\n\")\n",
        "            !gdalwarp \\\n",
        "              --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "              -overwrite -of GTiff -ot Float32 \\\n",
        "              -cutline {dir_to_store}Cresult_B{i}.shp -cl Cresult_B{i} -crop_to_cutline \\\n",
        "              {temp_dir}\"BothCheck_result.tif\" \\\n",
        "              {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "            #counting Snow and Non-Snowpixels\n",
        "            raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "            pixel_nos_snow = (raster_file ==1).sum()\n",
        "            pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "            SC_Slope_values.append(pixel_nos_snow)\n",
        "            SC_Slope_values.append(pixel_nos_notsnow)\n",
        "            !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "        #######################################################################\n",
        "        ########################################################################\n",
        "        m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "        ########################################################################\n",
        "        !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*10000\n",
        "        #######################################################################\n",
        "        area_slope_sc_values = [number * coonstant_c for number in SC_Slope_values]\n",
        "        print(str(coonstant_c*pixel_count_snow),str(coonstant_c*pixel_count_notsnow),str(coonstant_c*pixel_count_snow+coonstant_c*pixel_count_notsnow))\n",
        "        #######################################################################\n",
        "        lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow) + \",\" + str(area_slope_sc_values)[1:-1])\n",
        "        #lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow))\n",
        "\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[25:25+10]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n"
      ],
      "metadata": {
        "id": "WKgCghZLi9SJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}