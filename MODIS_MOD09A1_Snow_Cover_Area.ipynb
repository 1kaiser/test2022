{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MODIS_MOD09A1_Snow_Cover_Area.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2022/blob/main/MODIS_MOD09A1_Snow_Cover_Area.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " >MOD09A1.061 data DOWNLOAD & PROCESSING calculating NDSI  > 0.4 && band2 reflectance is more than 11%"
      ],
      "metadata": {
        "id": "hlcgiXaXS1-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v6GiEeRvYbBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NbFxO-hu1Tx"
      },
      "source": [
        "# **ðŸ–¥ï¸RUNNING CODE FOR NDSI *MOD09A1.061* 8 DAY**  \n",
        "## ðŸŒ¨ï¸â„ï¸ðŸ”ï¸  **SNOW COVER AREA**  ðŸ”ï¸â„ï¸ðŸŒ¨ï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ›—ELEVATION DATA SNOW COVER CALCULATOR >>>**"
      ],
      "metadata": {
        "id": "ExcIZPM2bj4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre requisite"
      ],
      "metadata": {
        "id": "_TOeZVc3bj4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "JtAA0f0Bbj4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq\n",
        "output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "iztucvWQbj4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!gdown https://drive.google.com/uc?id=1vWxu7OaprxNLeIBYEU0QFwmH69uXZU5n #downloading MAP-DEM\n",
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "output.clear() #to_clear_the_output_console_everytime\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MpcKc4Zbj4a",
        "outputId": "4769b19f-2273-4135-c45a-ba5822b46bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### maincodeRun"
      ],
      "metadata": {
        "id": "fxLlWOcKbj4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "imgs_list_b4 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b4)]\n",
        "imgs_list_b6 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b6)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_list_b4.sort(reverse=True)\n",
        "imgs_list_b6.sort(reverse=True)\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "imgs_path_b4 = [os.path.join(image_dir, i) for i in imgs_list_b4 if i != 'outputs']\n",
        "imgs_path_b6 = [os.path.join(image_dir, i) for i in imgs_list_b6 if i != 'outputs']\n",
        "print(len(imgs_path_b2),len(imgs_path_b4),len(imgs_path_b6))\n",
        "\n",
        "data_output = image_dir[:-6] + \"DATA/ElevationOutfinalTestx2.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "if(path.exists(data_output) == False):\n",
        "  fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  fw.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      pathb4 = imgs_list_b4[i]\n",
        "      pathb6 = imgs_list_b6[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[25:25+10])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    pathb4 = imgs_list_b4[i]\n",
        "    pathb6 = imgs_list_b6[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[25:25+10])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        tic = time.perf_counter() #<<< start timer\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[25:25+10]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[25:25+10]), \"1,\" + str(pathb2[25:25+10]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[25:25+10]))\n",
        "        ########################################################################\n",
        "        #creating file NDSI\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb4} \\\n",
        "          --A_band 1 \\\n",
        "          -B {image_dir}{pathb6} \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "          --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb2} \\\n",
        "          --A_band 1 \\\n",
        "          -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "          --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "\n",
        "        !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "        #deleting file\n",
        "        ########################################################################\n",
        "        #counting Snow and Non-Snowpixels  \n",
        "        raster_file = gdalnumeric.LoadFile(temp_dir+\"BothCheck_result.tif\")\n",
        "        pixel_count_snow = (raster_file ==1).sum()\n",
        "        pixel_count_notsnow = (raster_file ==0).sum()\n",
        "        print(\"snow:\", pixel_count_snow,\" not snow:\", pixel_count_notsnow)\n",
        "\n",
        "        ######################################READING DEM MAP##################\n",
        "        \n",
        "        INPUT_DEM_IN_IMAGE = \"/content/ASTER_DEM.tif\"\n",
        "        !gdal_translate -outsize 454 233 {INPUT_DEM_IN_IMAGE} {temp_dir}output.tif   #<<<< RESIZING DEM MAP for size of NDSI result\n",
        "        ##############################RASTER CALCULATION OF DEM & NDSI RESULT#########\n",
        "        ######################################CREATING SCA ON 14 elevations##################\n",
        "        \n",
        "        E_difference = 500\n",
        "        last_num = int(7000/E_difference)\n",
        "        for i in range(1, last_num + 1):\n",
        "          !gdal_calc.py \\\n",
        "            --overwrite \\\n",
        "            --type=Float32 \\\n",
        "            -A {temp_dir}output.tif \\\n",
        "            --A_band 1 \\\n",
        "            -B {temp_dir}BothCheck_result.tif \\\n",
        "            --B_band 1 \\\n",
        "            --outfile=SmallerFileB\"{str(i)}\".tif \\\n",
        "            --NoDataValue=0 \\\n",
        "            --calc=\"((A.astype(float)>{E_difference*(i-1)})*(A.astype(float)<={E_difference*i})*1)*(B.astype(float))\"\n",
        "        ######################################CREATING SCA ON 14 elevations##################\n",
        "        ##############CALCULATING ELEVATION VALUES IN  100 TO 7000 RANGES USING SHAPEFILE OVER NDSI RASTER\n",
        "        \n",
        "        import gdalnumeric\n",
        "        SC_Elevation_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        \n",
        "        for i in range(1, last_num + 1):\n",
        "          #counting Snow and Non-Snowpixels \n",
        "          raster_file = gdalnumeric.LoadFile(temp_dir+\"SmallerFileB\"+str(i)+\".tif\")\n",
        "          pixel_nos_snow = (raster_file ==1).sum(); pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "          SC_Elevation_values.append(pixel_nos_snow) #; SC_Elevation_values.append(pixel_nos_notsnow)\n",
        "        ##############CALCULATING ELEVATION VALUES IN  1003000, 30005000, 50007000 RANGES USING SHAPEFILE OVER NDSI RASTER\n",
        "        \n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "            {temp_dir}BothCheck_result.tif \\\n",
        "            {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "\n",
        "          #counting Snow and Non-Snowpixels\n",
        "          raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "          pixel_nos_snow = (raster_file ==1).sum(); pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "          SC_Elevation_values.append(pixel_nos_snow) ; SC_Elevation_values.append(pixel_nos_notsnow)\n",
        "          !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "        print(len(SC_Elevation_values))\n",
        "        ##############CALCULATING ELEVATION VALUES IN  1003000, 30005000, 50007000 RANGES USING SHAPEFILE OVER NDSI RASTER\n",
        "        ########################################################################\n",
        "        m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "        ########################################################################\n",
        "        !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        constant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*94*110  #<<< 1 degree  = 111 km >>> km square area\n",
        "        #######################################################################\n",
        "        area_Elevation_sc_values = [number * constant_c for number in SC_Elevation_values]\n",
        "        area_Elevation_sc_values = [int(x) for x in area_Elevation_sc_values]\n",
        "        print(str(constant_c * pixel_count_snow),str(constant_c * pixel_count_notsnow),str(constant_c * pixel_count_snow + constant_c * pixel_count_notsnow))\n",
        "        #######################################################################\n",
        "        lines = str(pathb2[25:25+10] + \",\" + str(constant_c * pixel_count_snow) + \",\" + str(constant_c * pixel_count_notsnow) + \",\" + str(area_Elevation_sc_values)[1:-1])\n",
        "        # lines = str(pathb2[25:25+10] + \",\" + str(constant_c*pixel_count_snow) + \",\" + str(constant_c*pixel_count_notsnow))\n",
        "\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[25:25+10]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        toc = time.perf_counter() #<<< Stop Timer\n",
        "        print(f\"ðŸ’» â²ï¸ in {toc - tic:0.4f} seconds\") #print computation time\n",
        "        print(f\"time for all ðŸ’» â²ï¸ in {((toc - tic)*(len(imgs_path_b2)-i)):0.4f} seconds\") #print computation time remaining for complete execution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "fKoJKKdubj4a",
        "outputId": "2fb51b47-d48b-42f4-cb2d-ab3ad5b99e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok doy2021321 doy2021321\n",
            "doy2021321\n",
            "0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/bin/gdal_calc.py\", line 401, in <module>\n",
            "    main()\n",
            "  File \"/usr/bin/gdal_calc.py\", line 394, in main\n",
            "    doit(opts, args)\n",
            "  File \"/usr/bin/gdal_calc.py\", line 109, in doit\n",
            "    myNDV.append(myFile.GetRasterBand(myBand).GetNoDataValue())\n",
            "  File \"/usr/lib/python2.7/dist-packages/osgeo/gdal.py\", line 2263, in GetNoDataValue\n",
            "    return _gdal.Band_GetNoDataValue(self, *args)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3846e6cff705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m#counting Snow and Non-Snowpixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mraster_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdalnumeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"BothCheck_result.tif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mpixel_count_snow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mraster_file\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mpixel_count_notsnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mraster_file\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/osgeo/gdal_array.py\u001b[0m in \u001b[0;36mLoadFile\u001b[0;34m(filename, xoff, yoff, xsize, ysize, buf_xsize, buf_ysize, buf_type, resample_alg, callback, callback_data)\u001b[0m\n\u001b[1;32m    222\u001b[0m               \u001b[0mresample_alg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRIORA_NearestNeighbour\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m               callback=None, callback_data=None ):\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't open \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetLastErrorMsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mOpen\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;34m\"\"\"Open(char const * utf8_path, GDALAccess eAccess) -> Dataset\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_gdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mOpenEx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: /content/BothCheck_result.tif: No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **â„ï¸ASPECTðŸ§­ DATA SNOW COVER CALCULATOR >>>**"
      ],
      "metadata": {
        "id": "aIReqAKs4r-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre requisite"
      ],
      "metadata": {
        "id": "I-RyUf3Z1vRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "0bzIDwLG4r-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq\n",
        "output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "JfTjeAyr4r-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!gdown https://drive.google.com/uc?id=1vWxu7OaprxNLeIBYEU0QFwmH69uXZU5n #downloading MAP-DEM\n",
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "output.clear() #to_clear_the_output_console_everytime\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "8IE7ycHC4r-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### maincodeRun"
      ],
      "metadata": {
        "id": "KKYJVyu01yeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "imgs_list_b4 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b4)]\n",
        "imgs_list_b6 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b6)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_list_b4.sort(reverse=True)\n",
        "imgs_list_b6.sort(reverse=True)\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "imgs_path_b4 = [os.path.join(image_dir, i) for i in imgs_list_b4 if i != 'outputs']\n",
        "imgs_path_b6 = [os.path.join(image_dir, i) for i in imgs_list_b6 if i != 'outputs']\n",
        "print(len(imgs_path_b2),len(imgs_path_b4),len(imgs_path_b6))\n",
        "\n",
        "data_output = image_dir[:-6] + \"DATA/AspectOutfinalTestx2.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "if(path.exists(data_output) == False):\n",
        "  fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  fw.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      pathb4 = imgs_list_b4[i]\n",
        "      pathb6 = imgs_list_b6[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[25:25+10])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    tic = time.perf_counter() #<<< start timer\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    pathb4 = imgs_list_b4[i]\n",
        "    pathb6 = imgs_list_b6[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[25:25+10])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[25:25+10]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[25:25+10]), \"1,\" + str(pathb2[25:25+10]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[25:25+10]))\n",
        "        ########################################################################\n",
        "        #creating file NDSI\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb4} \\\n",
        "          --A_band 1 \\\n",
        "          -B {image_dir}{pathb6} \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "          --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb2} \\\n",
        "          --A_band 1 \\\n",
        "          -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "          --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "\n",
        "        !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "        #deleting file\n",
        "        ########################################################################\n",
        "        #counting Snow and Non-Snowpixels  \n",
        "        raster_file = gdalnumeric.LoadFile(temp_dir+\"BothCheck_result.tif\")\n",
        "        pixel_count_snow = (raster_file ==1).sum()\n",
        "        pixel_count_notsnow = (raster_file ==0).sum()\n",
        "        print(\"snow:\", pixel_count_snow,\" not snow:\", pixel_count_notsnow)\n",
        "\n",
        "        ######################################READING SLOPE MAP HAVING 9 CLASSES##################\n",
        "        INPUT_DEM_IN_IMAGE = \"/content/ASTER_DEM.tif\"\n",
        "        !gdaldem aspect {INPUT_DEM_IN_IMAGE} {temp_dir}ASPECToutput.tif\n",
        "        !gdal_translate -outsize 454 233 {temp_dir}ASPECToutput.tif {temp_dir}output.tif   #<<<< RESIZING SLOPE MAP for size of NDSI result\n",
        "        ##############################RASTER CALCULATION OF SLOPEMAP & NDSI RESULT#########\n",
        "        ######################################CREATING SCA ON 9 SLOPES##################\n",
        "        angle = 22.5\n",
        "        last_num = int(360/angle)\n",
        "        for i in range(1, last_num + 1):\n",
        "          !gdal_calc.py \\\n",
        "            --overwrite \\\n",
        "            --type=Float32 \\\n",
        "            -A {temp_dir}output.tif \\\n",
        "            --A_band 1 \\\n",
        "            -B {temp_dir}BothCheck_result.tif \\\n",
        "            --B_band 1 \\\n",
        "            --outfile=SmallerFileB\"{str(i)}\".tif \\\n",
        "            --NoDataValue=0 \\\n",
        "            --calc=\"((A.astype(float)>{angle*(i-1)})*(A.astype(float)<={angle*i})*1)*(B.astype(float))\"\n",
        "        ######################################CREATING SCA ON 9 SLOPES##################\n",
        "        ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        import gdalnumeric\n",
        "        SC_Aspect_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          for i in range(1, last_num + 1):\n",
        "            !gdalwarp \\\n",
        "              --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "              -overwrite -of GTiff -ot Float32 \\\n",
        "              -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "              {temp_dir}SmallerFileB\"{str(i)}\".tif \\\n",
        "              {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "\n",
        "            #counting Snow and Non-Snowpixels\n",
        "            raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "            pixel_nos_snow = (raster_file ==1).sum(); pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "            SC_Aspect_values.append(pixel_nos_snow) #; SC_Aspect_values.append(pixel_nos_notsnow)\n",
        "            !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "        print(len(SC_Aspect_values))\n",
        "        ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        ########################################################################\n",
        "        m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "        ########################################################################\n",
        "        !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        constant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*94*110  #<<< 1 degree  = 111 km >>> km square area\n",
        "        #######################################################################\n",
        "        area_Aspect_sc_values = [number * constant_c for number in SC_Aspect_values]\n",
        "        area_Aspect_sc_values = [int(x) for x in area_Aspect_sc_values]\n",
        "        print(str(constant_c * pixel_count_snow),str(constant_c * pixel_count_notsnow),str(constant_c * pixel_count_snow + constant_c * pixel_count_notsnow))\n",
        "        #######################################################################\n",
        "        lines = str(pathb2[25:25+10] + \",\" + str(constant_c * pixel_count_snow) + \",\" + str(constant_c * pixel_count_notsnow) + \",\" + str(area_Aspect_sc_values)[1:-1])\n",
        "        # lines = str(pathb2[25:25+10] + \",\" + str(constant_c*pixel_count_snow) + \",\" + str(constant_c*pixel_count_notsnow))\n",
        "\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[25:25+10]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "    toc = time.perf_counter() #<<< Stop Timer\n",
        "    print(f\"ðŸ’» â²ï¸ in {toc - tic:0.4f} seconds\") #print computation time\n",
        "    print(f\"time for all ðŸ’» â²ï¸ in {((toc - tic)*(len(imgs_path_b2)-i)):0.4f} seconds\") #print computation time remaining for complete execution"
      ],
      "metadata": {
        "id": "ntxxZJ8Y4r-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸªœSLOPEðŸªœ DATA SNOW COVER CALCULATOR >>>**"
      ],
      "metadata": {
        "id": "m7fIE_mGhWmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre requisite"
      ],
      "metadata": {
        "id": "Ul3J5ORn1eTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "nEHp5mXEhuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq\n",
        "output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "OEuDDcBlhuFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1hJqzhX9smqsTlMvWyT3U2vc9X52rLbmI #downloading SLOPE-MAP\n",
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "QNNIqyxOhuFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main code run"
      ],
      "metadata": {
        "id": "ZAsKUyh71h6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "imgs_list_b4 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b4)]\n",
        "imgs_list_b6 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b6)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_list_b4.sort(reverse=True)\n",
        "imgs_list_b6.sort(reverse=True)\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "imgs_path_b4 = [os.path.join(image_dir, i) for i in imgs_list_b4 if i != 'outputs']\n",
        "imgs_path_b6 = [os.path.join(image_dir, i) for i in imgs_list_b6 if i != 'outputs']\n",
        "print(len(imgs_path_b2),len(imgs_path_b4),len(imgs_path_b6))\n",
        "\n",
        "data_output = image_dir[:-6] + \"DATA/SlopeOutfinalTestx2.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "if(path.exists(data_output) == False):\n",
        "  fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  fw.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      pathb4 = imgs_list_b4[i]\n",
        "      pathb6 = imgs_list_b6[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[25:25+10])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    tic = time.perf_counter() #<<< start timer\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    pathb4 = imgs_list_b4[i]\n",
        "    pathb6 = imgs_list_b6[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[25:25+10])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[25:25+10]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[25:25+10]), \"1,\" + str(pathb2[25:25+10]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[25:25+10]))\n",
        "        ########################################################################\n",
        "        #creating file NDSI\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb4} \\\n",
        "          --A_band 1 \\\n",
        "          -B {image_dir}{pathb6} \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"NDSI_result.tif\" \\\n",
        "          --calc=\"(A.astype(float) - B)/(A.astype(float) + B)\"\n",
        "\n",
        "        !gdal_calc.py \\\n",
        "          --overwrite \\\n",
        "          --type=Float32 \\\n",
        "          -A {image_dir}{pathb2} \\\n",
        "          --A_band 1 \\\n",
        "          -B {temp_dir}\"NDSI_result.tif\" \\\n",
        "          --B_band 1 \\\n",
        "          --outfile={temp_dir}\"BothCheck_result.tif\" \\\n",
        "          --calc=\"(B.astype(float)>=0.4)*(A.astype(float)>0.11*A.astype(float))\"#--calc=\"(A.astype(float)>0.011*A.astype(float))\"#\n",
        "        \n",
        "\n",
        "        !rm -r {temp_dir}\"NDSI_result.tif\"\n",
        "        #deleting file\n",
        "        ########################################################################\n",
        "        #counting Snow and Non-Snowpixels  \n",
        "        raster_file = gdalnumeric.LoadFile(temp_dir+\"BothCheck_result.tif\")\n",
        "        pixel_count_snow = (raster_file ==1).sum()\n",
        "        pixel_count_notsnow = (raster_file ==0).sum()\n",
        "        print(\"snow:\", pixel_count_snow,\" not snow:\", pixel_count_notsnow)\n",
        "\n",
        "        ######################################READING SLOPE MAP HAVING 9 CLASSES##################\n",
        "        INPUT_SLOPE_IN_IMAGE = \"/content/Reclass_Slop21.tif\"\n",
        "        !gdal_translate -outsize 454 233 {INPUT_SLOPE_IN_IMAGE} {temp_dir}output.tif   #<<<< RESIZING SLOPE MAP for size of NDSI result\n",
        "        ##############################RASTER CALCULATION OF SLOPEMAP & NDSI RESULT#########\n",
        "        ######################################CREATING SCA ON 9 SLOPES##################\n",
        "        for i in range(1,10):\n",
        "          !gdal_calc.py \\\n",
        "            --overwrite \\\n",
        "            --type=Float32 \\\n",
        "            -A {temp_dir}output.tif \\\n",
        "            --A_band 1 \\\n",
        "            -B {temp_dir}BothCheck_result.tif \\\n",
        "            --B_band 1 \\\n",
        "            --outfile=SmallerFileB\"{str(i)}\".tif \\\n",
        "            --NoDataValue=0 \\\n",
        "            --calc=\"((A=={i})*1)*(B.astype(float))\"\n",
        "        ######################################CREATING SCA ON 9 SLOPES##################\n",
        "        ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        import gdalnumeric\n",
        "        SC_Slope_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          for i in range(1,10):\n",
        "            !gdalwarp \\\n",
        "              --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "              -overwrite -of GTiff -ot Float32 \\\n",
        "              -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "              {temp_dir}SmallerFileB\"{str(i)}\".tif \\\n",
        "              {temp_dir}Xresult_\"{str(i)}\".tif\n",
        "\n",
        "            #counting Snow and Non-Snowpixels\n",
        "            raster_file = gdalnumeric.LoadFile(temp_dir+\"Xresult_\"+str(i)+\".tif\")\n",
        "            pixel_nos_snow = (raster_file ==1).sum(); pixel_nos_notsnow = (raster_file ==0).sum()\n",
        "            SC_Slope_values.append(pixel_nos_snow) #; SC_Slope_values.append(pixel_nos_notsnow)\n",
        "            !rm -r {temp_dir}Xresult_{str(i)}.tif\n",
        "        print(len(SC_Slope_values))\n",
        "        ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        ########################################################################\n",
        "        m1 = !gdalinfo -json {temp_dir}\"BothCheck_result.tif\" | jq -r .geoTransform \n",
        "        ########################################################################\n",
        "        !rm -r {temp_dir}\"BothCheck_result.tif\"\n",
        "        #deleting file\n",
        "\n",
        "        coonstant_c = float(m1[2][9:9+16])*float(m1[6][9+1:9+16+1])*94*110  #<<< 1 degree  = 111 km >>> km square area\n",
        "        #######################################################################\n",
        "        area_slope_sc_values = [number * coonstant_c for number in SC_Slope_values]\n",
        "        area_slope_sc_values = [int(x) for x in area_slope_sc_values]\n",
        "        print(str(coonstant_c*pixel_count_snow),str(coonstant_c*pixel_count_notsnow),str(coonstant_c*pixel_count_snow+coonstant_c*pixel_count_notsnow))\n",
        "        #######################################################################\n",
        "        lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow) + \",\" + str(area_slope_sc_values)[1:-1])\n",
        "        # lines = str(pathb2[25:25+10] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow))\n",
        "\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[25:25+10]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "    toc = time.perf_counter() #<<< Stop Timer\n",
        "    print(f\"ðŸ’» â²ï¸ in {toc - tic:0.4f} seconds\") #print computation time\n",
        "    print(f\"time for all ðŸ’» â²ï¸ in {((toc - tic)*(len(imgs_path_b2)-i)):0.4f} seconds\") #print computation time remaining for complete execution"
      ],
      "metadata": {
        "id": "WKgCghZLi9SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸŒ¡ï¸TEMPERATURE & â˜”RAINFALL**"
      ],
      "metadata": {
        "id": "CeMQ6xHo2K9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **â˜”RAINFALL DATA CALCULATOR >>>**"
      ],
      "metadata": {
        "id": "SVspPObHbvl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "g0uX07f7bvl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq\n",
        "output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "-gd5_Y9Ubvl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "%cd /content\n",
        "output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "LwWc4vYLbvl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/rainfall/data/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"chirps_global_pentad_data_\"\n",
        "DayOY = \"[0-9]+_[0-9]+\"\n",
        "fileExt = r'.tiff'\n",
        "expression_temp = prefix\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_temp)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "print(len(imgs_path_b2))\n",
        "\n",
        "data_output = image_dir[:-6] + \"/RainfallOutfinalTestxxx.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "if(path.exists(data_output) == False):\n",
        "  fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  fw.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[26:26+17])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    tic = time.perf_counter() #<<< start timer\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[26:26+17])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[26:26+17]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[26:26+17]), \"1,\" + str(pathb2[26:26+17]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[26:26+17]))\n",
        " ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        import gdalnumeric\n",
        "        Rainfall_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "            {image_dir}{pathb2} \\\n",
        "            {temp_dir}Xresult_\"{str(j)}\".tif\n",
        "          \n",
        "          #calculating mean rainfall for the elevation sections\n",
        "          raster = gdal.Open(str(temp_dir)+\"Xresult_\"+str(j)+\".tif\")\n",
        "          rain = raster.GetRasterBand(1).ComputeStatistics(False)\n",
        "          print(rain)\n",
        "          Rainfall_values.append(rain[2]) \n",
        "          !rm -r {temp_dir}Xresult_{str(j)}.tif\n",
        "        Rainfall_values = [int(x) for x in Rainfall_values]\n",
        "        print(len(Rainfall_values))\n",
        "        lines = str(pathb2[26:26+17] +\",\"+ str(Rainfall_values)[1:-1])\n",
        "        # lines = str(pathb2[26:26+17] + \",\" + str(constant_c*pixel_count_snow) + \",\" + str(constant_c*pixel_count_notsnow))\n",
        "\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[26:26+17]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "    toc = time.perf_counter() #<<< Stop Timer\n",
        "    print(f\"ðŸ’» â²ï¸ in {toc - tic:0.4f} seconds\") #print computation time\n"
      ],
      "metadata": {
        "id": "YNmBzY9sbvl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ðŸŒ¡ï¸TEMPERATUREðŸŒ¡ï¸ DATA CALCULATOR SECTION I >>>**"
      ],
      "metadata": {
        "id": "1GazSrsHCwzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "hGsijHZoCwzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq\n",
        "output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "UuYd5_ICCwzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "!gdown https://drive.google.com/uc?id=1JsXGprlKu4jiBYy-G82BkdaQjReNXJSp #downloading BOUNDARY SHAPE FILES\n",
        "!unzip /content/beasBoundary.zip\n",
        "output.clear() #to_clear_the_output_console_everytime\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "POo0Q1ZdCwzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"lstc6_global_dekad_data_\"\n",
        "DayOY = \"[0-9]+_[0-9]+\"\n",
        "fileExt = r'.tiff'\n",
        "expression_temp = prefix\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_temp)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "print(len(imgs_path_b2))\n",
        "\n",
        "data_output = image_dir[:-6] + \"/TemperatureOutfinalTestxxx.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "if(path.exists(data_output) == False):\n",
        "  fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  fw.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[24:23+18])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    tic = time.perf_counter() #<<< start timer\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[24:23+18])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[24:23+18]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[24:23+18]), \"1,\" + str(pathb2[24:23+18]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[24:23+18]))\n",
        " ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        import gdalnumeric\n",
        "        Temperature_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        ###CALCULATE FOR FULL BOUNDARY###\n",
        "        !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir}beasBoundary.shp -cl beasBoundary -crop_to_cutline \\\n",
        "            {image_dir}{pathb2} \\\n",
        "            {temp_dir}Xresult_beasBoundary.tif\n",
        "\n",
        "        raster = gdal.Open(str(temp_dir)+\"Xresult_beasBoundary.tif\")\n",
        "        temp1 = raster.GetRasterBand(1).ComputeStatistics(False)\n",
        "        print(temp1)\n",
        "        Temperature_values.append(temp1[2]) \n",
        "        !rm -r {temp_dir}Xresult_beasBoundary.tif\n",
        "        ##CALCULATE FOR FULL BOUNDARY###\n",
        "\n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "            {image_dir}{pathb2} \\\n",
        "            {temp_dir}Xresult_\"{str(j)}\".tif\n",
        "          \n",
        "          #calculating mean Temperature_values for the elevation sections\n",
        "          raster = gdal.Open(str(temp_dir)+\"Xresult_\"+str(j)+\".tif\")\n",
        "          # temp = raster.GetRasterBand(1).GetStatistics(0,1)\n",
        "          # print(temp)\n",
        "          temp1 = raster.GetRasterBand(1).ComputeStatistics(False)\n",
        "          print(temp1)\n",
        "          Temperature_values.append(temp1[2]) \n",
        "          !rm -r {temp_dir}Xresult_{str(j)}.tif\n",
        "        Temperature_values = [int(x) for x in Temperature_values]\n",
        "        print(len(Temperature_values))\n",
        "        lines = str(pathb2[24:23+18] +\",\"+ str(Temperature_values)[1:-1])\n",
        "        # lines = str(pathb2[24:23+18] + \",\" + str(constant_c*pixel_count_snow) + \",\" + str(constant_c*pixel_count_notsnow))\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[24:23+18]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "    toc = time.perf_counter() #<<< Stop Timer\n",
        "    print(f\"ðŸ’» â²ï¸ in {toc - tic:0.4f} seconds\") #print computation time\n"
      ],
      "metadata": {
        "id": "4Et5wXE1CwzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ðŸŒ¡ï¸TEMPERATUREðŸŒ¡ï¸ DATA CALCULATOR SECTION II  >>>**"
      ],
      "metadata": {
        "id": "4Tt6akoLV4pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "SyD_UZM3V4pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!python -m pip install pyModis\n",
        "!sudo apt-get install jq\n",
        "output.clear() #to_clear_the_output_console_everytime"
      ],
      "metadata": {
        "id": "jB7kO04hV4pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1lU78Fte5QAJevfmwFLhCFBIoc8xLzLoK #downloading ELEVATION SHAPE FILES\n",
        "!unzip /content/elev_correct.zip\n",
        "!gdown https://drive.google.com/uc?id=1JsXGprlKu4jiBYy-G82BkdaQjReNXJSp #downloading BOUNDARY SHAPE FILES\n",
        "!unzip /content/beasBoundary.zip\n",
        "output.clear() #to_clear_the_output_console_everytime\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "zp21n9ttV4pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROCESSING IMAGES FOR SNOW COVER >>> SAVING  *.TXT FILE { vertical-output: true }\n",
        "import os\n",
        "import pymodis\n",
        "import gdalnumeric\n",
        "#to clear output\n",
        "from google.colab import output\n",
        "\n",
        "import subprocess\n",
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"MYD21A1N.061_LST_1KM_\"\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_temp = prefix \n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_temp)]\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "print(len(imgs_path_b2))\n",
        "\n",
        "data_output = image_dir[:-6] + \"/TemperatureOutfinalTestxxxC.txt\"\n",
        "###########################<<FILE CREATION PROCESS>>#############################\n",
        "import os.path\n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "if(path.exists(data_output) == False):\n",
        "  fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "  fw.close()\n",
        "  fa = open(data_output, \"a+\", encoding = \"utf-8\")\n",
        "  for i, file_name in enumerate(imgs_path_b2):\n",
        "      print('ok')\n",
        "      print(file_name)\n",
        "      pathb2 = imgs_list_b2[i]\n",
        "      data = []\n",
        "      lines = str(pathb2[21:21+10])\n",
        "      fa.writelines(\"0,\" + lines + '\\n')\n",
        "  fa.close()\n",
        "else:print(\"file ready please continue\")\n",
        "###############################################################################\n",
        "for i, file_name in enumerate(imgs_path_b2):\n",
        "    tic = time.perf_counter() #<<< start timer\n",
        "    print('ok')\n",
        "    print(file_name)\n",
        "    pathb2 = imgs_list_b2[i]\n",
        "    data = []\n",
        "    # with open(data_output, \"w+\", encoding = \"utf-8\") as haveit:\n",
        "    #   print(\"\\nREADY>>>>>\\n\")\n",
        "    fr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "    data = fr.read().split('\\n')\n",
        "    fr.close()\n",
        "    check_LINE = 0\n",
        "    for j in range(len(data)-1):\n",
        "      if ((str(pathb2[21:21+10])==data[j].split(',')[1])&(data[j].split(',')[0]==\"0\")):\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "        print('ok',str(pathb2[21:21+10]),data[j].split(',')[1])\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"0,\" + str(pathb2[21:21+10]), \"1,\" + str(pathb2[21:21+10]))\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        #################statr calculation\n",
        "        print(str(pathb2[21:21+10]))\n",
        " ######################################CUTTING 9 SLOPES WITH 3 ELEVATIONS##################\n",
        "        import gdalnumeric\n",
        "        Temperature_values = [] #<<<<< TO TORE THE CALCULATED VALUES\n",
        "        ###CALCULATE FOR FULL BOUNDARY###\n",
        "        !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir}beasBoundary.shp -cl beasBoundary -crop_to_cutline \\\n",
        "            {image_dir}{pathb2} \\\n",
        "            {temp_dir}Xresult_beasBoundary.tif\n",
        "\n",
        "        raster = gdal.Open(str(temp_dir)+\"Xresult_beasBoundary.tif\")\n",
        "        temp1 = raster.GetRasterBand(1).ComputeStatistics(False)\n",
        "        print(temp1)\n",
        "        Temperature_values.append(temp1[2]) \n",
        "        !rm -r {temp_dir}Xresult_beasBoundary.tif\n",
        "        ##CALCULATE FOR FULL BOUNDARY###\n",
        "\n",
        "        temp_dir_elevations = str(temp_dir)+\"elev_correct/E\"\n",
        "        elev = [ 1003000, 30005000, 50007000]\n",
        "        for j, elevations in enumerate(elev):\n",
        "          !gdalwarp \\\n",
        "            --config GDAL_CACHEMAX 3000 -wm 3000 -multi -co NUM_THREADS=ALL_CPUS \\\n",
        "            -overwrite -of GTiff -ot Float32 \\\n",
        "            -cutline {temp_dir_elevations}{elev[j]}.shp -cl \"E\"{elev[j]} -crop_to_cutline \\\n",
        "            {image_dir}{pathb2} \\\n",
        "            {temp_dir}Xresult_\"{str(j)}\".tif\n",
        "          \n",
        "          #calculating mean Temperature_values for the elevation sections\n",
        "          raster = gdal.Open(str(temp_dir)+\"Xresult_\"+str(j)+\".tif\")\n",
        "          # temp = raster.GetRasterBand(1).GetStatistics(0,1)\n",
        "          # print(temp)\n",
        "          temp1 = raster.GetRasterBand(1).ComputeStatistics(False)\n",
        "          print(temp1)\n",
        "          Temperature_values.append(temp1[2]) \n",
        "          !rm -r {temp_dir}Xresult_{str(j)}.tif\n",
        "        Temperature_values = [int(x) for x in Temperature_values]\n",
        "        print(len(Temperature_values))\n",
        "        lines = str(pathb2[21:21+10] +\",\"+ str(Temperature_values)[1:-1])\n",
        "        # lines = str(pathb2[21:21+10] + \",\" + str(constant_c*pixel_count_snow) + \",\" + str(constant_c*pixel_count_notsnow))\n",
        "      ########################################################################  \n",
        "        ################align results to replace\n",
        "        frr = open(data_output, \"r\", encoding = \"utf-8\")\n",
        "        all_data = frr.read()\n",
        "        frr.close()\n",
        "        all_data = all_data.replace(\"1,\" + str(pathb2[21:21+10]), \"1,\" + lines)\n",
        "        fw = open(data_output, \"w\", encoding = \"utf-8\")\n",
        "        fw.write(all_data)\n",
        "        fw.close()\n",
        "        output.clear() #to_clear_the_output_console_everytime\n",
        "    toc = time.perf_counter() #<<< Stop Timer\n",
        "    print(f\"ðŸ’» â²ï¸ in {toc - tic:0.4f} seconds\") #print computation time\n"
      ],
      "metadata": {
        "id": "AD9U0vNXV4pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ›°ï¸MODIS DATA DOWNLOADðŸ“¡**"
      ],
      "metadata": {
        "id": "36FWle_q2vZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ“œMODIS MYD09A1 V061 BAND DATA DOWNLOAD SECTIONðŸ“œ**"
      ],
      "metadata": {
        "id": "I7d6uagHFbGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### APPEEAR LDDAC DATA DOWNLOAD"
      ],
      "metadata": {
        "id": "XsU-Aivn7ihA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1SrMZ6KTEpO0yuTwm9DlLFREjksvn1_mZ "
      ],
      "metadata": {
        "id": "p4N7TJmtX7K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/OUT/data/MOD09A1061/files\n",
        "!ls /content/drive/MyDrive/OUT/data/MOD09A1061/files\n",
        "%cd /content/drive/MyDrive/OUT/data/MOD09A1061/files"
      ],
      "metadata": {
        "id": "cLh93IAEYs04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try download the list data"
      ],
      "metadata": {
        "id": "Iia9EHMD72b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --request POST --user kroy0001:/#j%kWrPA,8.HRe --header \"Content-Length: 0\" \"https://appeears.earthdatacloud.nasa.gov/api/login\""
      ],
      "metadata": {
        "id": "N7PNs-Mu9ibu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !curl -L -O --remote-header-name \\\n",
        "#   --header \"Authorization: Bearer bVVLVOIv29Lds-zADthteUE_1QlykgndjN5T6BaKMzMS-A11Z8UWtVsNbAJ85LWcGGerQH1KpM7eb-1KZS_Nig\" \\\n",
        "#   --location https://appeears.earthdatacloud.nasa.gov/api/bundle/908b9b61-5acf-48ca-933e-1fcd3b2704fc/c4d1addc-4e43-43e6-aac4-04cdcf04faca/MOD09A1.061_sur_refl_b01_doy2000129_aid0001.tif"
      ],
      "metadata": {
        "id": "IDFusRPU9up5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "\n",
        "def download_url(url):\n",
        "    t0 = time.time()\n",
        "###########################################################################################################################\n",
        "    !curl -L -O --remote-header-name \\\n",
        "      --header \"Authorization: Bearer B9rG-ZpBea75Ds9E_jOGc-DC5_C1vel5aPMln6KTulGEDa0U2OpxTt7TyicnkyDBYpgjqhnk7aFp9e-juzo2Qg\" \\\n",
        "      --location {url}\n",
        "###########################################################################################################################\n",
        "    return( time.time() - t0)\n",
        "        \n",
        "t0 = time.time()\n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('time (s):', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "file1 = open(\"/content/url.txt\", 'r')\n",
        "###########################################################################################################################\n",
        "download_parallel(file1)\n"
      ],
      "metadata": {
        "id": "rtfs6--L7RMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import output\n",
        "\n",
        "# file1 = open(\"/content/url.txt\", 'r')\n",
        "# link_list = [f for f in enumerate(file1)]\n",
        "# for i,link in enumerate(link_list):\n",
        "#     print(\"ok\")\n",
        "#     !curl -L -O --remote-header-name \\\n",
        "#     --header \"Authorization: Bearer bVVLVOIv29Lds-zADthteUE_1QlykgndjN5T6BaKMzMS-A11Z8UWtVsNbAJ85LWcGGerQH1KpM7eb-1KZS_Nig\" \\\n",
        "#     --location {link_list[i][1]}\n",
        "#     output.clear()\n"
      ],
      "metadata": {
        "id": "690P6zBI_OEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ“œMODIS MYD21A1N V061 TEMPERATURE DATA DOWNLOAD SECTIONðŸ“œ**"
      ],
      "metadata": {
        "id": "oRY0Vc6RRXBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### APPEEAR LDDAC DATA DOWNLOAD"
      ],
      "metadata": {
        "id": "Kg4pQckJRXBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1RuwF6WncC_GB5ijQ5tfyqilZSSpNqm7d"
      ],
      "metadata": {
        "id": "0s6q2eE8RXBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data\n",
        "!ls /content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data\n",
        "%cd /content/drive/MyDrive/OUT/data/MOD09A1061/temperature/data"
      ],
      "metadata": {
        "id": "LD3993qpRXBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try download the list data"
      ],
      "metadata": {
        "id": "Veqjch_ZRXBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --request POST --user kroy0001:/#j%kWrPA,8.HRe --header \"Content-Length: 0\" \"https://appeears.earthdatacloud.nasa.gov/api/login\""
      ],
      "metadata": {
        "id": "zcshtjLMRXBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !curl -L -O --remote-header-name \\\n",
        "#   --header \"Authorization: Bearer bVVLVOIv29Lds-zADthteUE_1QlykgndjN5T6BaKMzMS-A11Z8UWtVsNbAJ85LWcGGerQH1KpM7eb-1KZS_Nig\" \\\n",
        "#   --location https://appeears.earthdatacloud.nasa.gov/api/bundle/908b9b61-5acf-48ca-933e-1fcd3b2704fc/c4d1addc-4e43-43e6-aac4-04cdcf04faca/MOD09A1.061_sur_refl_b01_doy2000129_aid0001.tif"
      ],
      "metadata": {
        "id": "LEuzQI8tRXBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "\n",
        "def download_url(url):\n",
        "    t0 = time.time()\n",
        "###########################################################################################################################\n",
        "    !curl -L -O --remote-header-name \\\n",
        "      --header \"Authorization: Bearer PNPKuJSlDQavPosdUzw4DyJRP0-P4zNr3ME4ovJPvjyqM3bOxCPB1JbDHczTZrUEb2YfpKfokClVoKOH5yzoDw\" \\\n",
        "      --location {url}\n",
        "###########################################################################################################################\n",
        "    return( time.time() - t0)\n",
        "        \n",
        "t0 = time.time()\n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('time (s):', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "file1 = open(\"/content/url.txt\", 'r')\n",
        "###########################################################################################################################\n",
        "download_parallel(file1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSwBYt_dRXBv",
        "outputId": "26d548d9-0e4d-4e66-89df-7cae88e37573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import output\n",
        "\n",
        "# file1 = open(\"/content/url.txt\", 'r')\n",
        "# link_list = [f for f in enumerate(file1)]\n",
        "# for i,link in enumerate(link_list):\n",
        "#     print(\"ok\")\n",
        "#     !curl -L -O --remote-header-name \\\n",
        "#     --header \"Authorization: Bearer bVVLVOIv29Lds-zADthteUE_1QlykgndjN5T6BaKMzMS-A11Z8UWtVsNbAJ85LWcGGerQH1KpM7eb-1KZS_Nig\" \\\n",
        "#     --location {link_list[i][1]}\n",
        "#     output.clear()\n"
      ],
      "metadata": {
        "id": "GEAOYilHRXBw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}