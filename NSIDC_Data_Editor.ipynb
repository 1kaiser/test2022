{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2022/blob/main/NSIDC_Data_Editor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tR28xKL2xHV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng_wcRuL252M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c6b19d-9040-4abf-cc10-03fcd241c567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NbFxO-hu1Tx"
      },
      "source": [
        "# VIEW FIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sUj_Ezpy3CY"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/OUT/data/MOD10A1v6/MOD10A1.A2000055.h24v05.006.2016061160550.hdf /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdal_translate \\\n",
        "  HDF4_SDS:UNKNOWN:\"/content/MOD01/2021/206/MOD01.A2021206.0510.061.2021217151715.hdf\":13 \\\n",
        "  /content/MOD10A1vv.tif"
      ],
      "metadata": {
        "id": "4Ovv6ZM07_Bh",
        "outputId": "ef130f52-f8f6-4a06-9e12-acebd2c1f425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input file size is 4, 208\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdaltransform -s_srs EPSG:6842 -t_srs WGS84 {path} {path}\n",
        "#!gdalwarp -t_srs \"+proj=longlat +datum=WGS84 +no_defs\" {path} reproj_vel_mosaic.tif"
      ],
      "metadata": {
        "id": "OY03pomN7bcJ",
        "outputId": "63c3f983-0f6b-4482-95a2-532cf3e63d2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR 1: The transformation is already \"north up\" or a transformation between pixel/line and georeferenced coordinates cannot be computed for /content/MOD10A1.A2022217.h24v05.061.2022219035723.hdf. There is no affine transformation and no GCPs. Specify transformation option SRC_METHOD=NO_GEOTRANSFORM to bypass this check.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3DCyp74URzd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Default title text { vertical-output: true }\n",
        "path = \"/content/MOD10A1.A2022217.h24v05.061.2022219035723.hdf\" #@param {type:\"string\"}\n",
        "%cd /content\n",
        "!gdalinfo {path}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "image_dir = r'/content'\n",
        "fileExt = r'.hdf'\n",
        "imgs_list = [f for f in os.listdir(image_dir) if f.endswith(fileExt)]\n",
        "imgs_list.sort()\n",
        "imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n",
        "for name, image in enumerate(imgs_path):\n",
        "  print('ok')\n",
        "  print(name)\n",
        "  print(image)\n",
        "  print(imgs_list[name])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPzVPtmMQUzK",
        "outputId": "bdc82b95-7a0e-4514-e330-a7e83ad0411d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n",
            "0\n",
            "/content/MOD10A1.A2022217.h24v05.061.2022219035723.hdf\n",
            "MOD10A1.A2022217.h24v05.061.2022219035723.hdf\n",
            "ok\n",
            "1\n",
            "/content/MOD10A2.A2000049.h24v05.061.2020041210702.hdf\n",
            "MOD10A2.A2000049.h24v05.061.2020041210702.hdf\n",
            "ok\n",
            "2\n",
            "/content/MOD10A2.A2000057.h24v05.061.2020037220731.hdf\n",
            "MOD10A2.A2000057.h24v05.061.2020037220731.hdf\n",
            "ok\n",
            "3\n",
            "/content/MOD10A2.A2000065.h24v05.061.2020038183336.hdf\n",
            "MOD10A2.A2000065.h24v05.061.2020038183336.hdf\n",
            "ok\n",
            "4\n",
            "/content/MOD10A2.A2000073.h24v05.061.2020040193408.hdf\n",
            "MOD10A2.A2000073.h24v05.061.2020040193408.hdf\n",
            "ok\n",
            "5\n",
            "/content/MOD10A2.A2000081.h24v05.061.2020040180054.hdf\n",
            "MOD10A2.A2000081.h24v05.061.2020040180054.hdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_list[5]\n",
        "name[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "TwRQ6eyymyIx",
        "outputId": "8f3bfd09-ba71-4ce4-ba46-f89c9a3656fc"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-53728918661f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimgs_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyModis"
      ],
      "metadata": {
        "id": "70TAh20JybZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c9eb0d-8dd3-466b-895c-e5383b2689a4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pyModis\n",
            "Successfully installed pyModis-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = r'/content/sample_data/'\n",
        "temp_dir = r'/content/'\n",
        "path1 = r'MOD10A1.A2022217.h24v05.061.2022219035723.hdf'\n",
        "image_dir+path1\n",
        "temp_dir+path1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y3KE9ptgpEf0",
        "outputId": "05501c56-d2f0-4e2d-ccc9-3ee8d8ac9a50"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/MOD10A1.A2022217.h24v05.061.2022219035723.hdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymodis\n",
        "subset = [0,0,0,1,0,0,0]#subset = [0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "#converting to sinusodial coordinate system to wgs 84 utm 43\n",
        "pymodis.convertmodis_gdal.convertModisGDAL(image_dir+path1,temp_dir+path1[:-4],subset,2400,outformat=\"GTiff\",epsg=32643).run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjHlV0o_ygp9",
        "outputId": "aadac140-4b71-4a51-ad1e-438bc57c50b7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2022217.h24v05.061.2022219035723.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2022217.h24v05.061.2022219035723.hdf' reprojected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r {path[:-4]}\"_ndsi.tif\""
      ],
      "metadata": {
        "id": "soK5Lm9GbM-d"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1oyqXeHZgaTOjLod-Vqz-VAzS88O13JDr\n",
        "!unzip /content/beasBasinShapeFile.zip -d /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PC3a_sycpEr",
        "outputId": "c3b92d4a-d46c-4f7e-ecdb-3d43f0e8f851"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oyqXeHZgaTOjLod-Vqz-VAzS88O13JDr\n",
            "To: /content/beasBasinShapeFile.zip\n",
            "\r  0% 0.00/8.65k [00:00<?, ?B/s]\r100% 8.65k/8.65k [00:00<00:00, 11.7MB/s]\n",
            "Archive:  /content/beasBasinShapeFile.zip\n",
            " extracting: /content/beasBasinShapeFile.cpg  \n",
            "  inflating: /content/beasBasinShapeFile.dbf  \n",
            "  inflating: /content/beasBasinShapeFile.prj  \n",
            "  inflating: /content/beasBasinShapeFile.qmd  \n",
            "  inflating: /content/beasBasinShapeFile.shp  \n",
            "  inflating: /content/beasBasinShapeFile.shx  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_out2 = path[:-4]+\"_ndsi.tif\"\n",
        "!gdalwarp \\\n",
        "  -cutline /content/beasBasinShapeFile.shp  -crop_to_cutline \\\n",
        "  {temp_dir}{path1[:-4]}\"_ndsi.tif\" \\\n",
        "  {temp_dir}{path1[:-4]}\"_ndsi_clipped.tif\""
      ],
      "metadata": {
        "id": "-K06H9lwDdV8",
        "outputId": "ad21a0a7-e2fc-4e14-a155-ddabc07a400a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2022217.h24v05.061.2022219035723_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2022217.h24v05.061.2022219035723_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2022217.h24v05.061.2022219035723_ndsi.tif to destination /content/MOD10A1.A2022217.h24v05.061.2022219035723_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gdal_calc.py \n",
        "!gdal_calc.py \\\n",
        "  -A {path[:-4]}\"_ndsi_clipped.tif\" \\\n",
        "  --outfile={path[:-4]}\"_result.tif\" \\\n",
        "  --calc=\"A/10>=40\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GOGik_6luKA",
        "outputId": "cfb20317-4823-4572-bb7f-98c4951fc2b4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 .. 100 - Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import gdalnumeric\n",
        "# path1 = (\"/content/clip_ndsi-snow-cover.tif\")\n",
        "# os.chdir(path1)\n",
        "# for rasters in os.listdir(path1):\n",
        "#     raster_file = gdalnumeric.LoadFile(rasters)\n",
        "#     pixel_count_snow = ((raster_file <=0) + (raster_file >= 25)).sum()\n",
        "#     pixel_count_notsnow = ((raster_file <=0) + (raster_file >= 25)).sum()\n",
        "#     print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)\n",
        "raster_file = gdalnumeric.LoadFile(path[:-4]+\"_result.tif\")\n",
        "# pixel_count_snow = ((raster_file <=0) + (raster_file >= 25)).sum()\n",
        "# pixel_count_notsnow = ((raster_file <=0) + (raster_file >= 25)).sum()\n",
        "pixel_count_snow = (raster_file ==0).sum()\n",
        "pixel_count_notsnow = (raster_file ==1).sum()\n",
        "print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR8oqMAvekDz",
        "outputId": "85ca7981-6a46-414e-8c8d-977d53576b9f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "snow: 146  not snow: 1811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install jq"
      ],
      "metadata": {
        "id": "g1jFxM96uliw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = !gdalinfo -json {path[:-4]}\"_result.tif\" | jq -r .geoTransform \n",
        "coonstant_c = int(m1[2][9:9+4])*int(m1[6][9+1:9+4+1])/1000000\n",
        "\n"
      ],
      "metadata": {
        "id": "GQ0jZijluqKO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coonstant_c*pixel_count_snow\n",
        "str(coonstant_c*pixel_count_snow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1A9et3-tgJpT",
        "outputId": "1f7efd74-088d-44a2-b7d2-9064d8e64037"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'838.473912'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = str(path1[8:8+8] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow))\n",
        "with open(temp_dir+\"out.txt\",\"a+\", encoding=\"utf-8\") as f:\n",
        "  f.writelines('\\n'+ lines)"
      ],
      "metadata": {
        "id": "C52mRQ_TJy12"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m pip install pyModis\n",
        "# !sudo apt-get install jq\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=1oyqXeHZgaTOjLod-Vqz-VAzS88O13JDr\n",
        "# !unzip /content/beasBasinShapeFile.zip -d /content\n",
        "\n",
        "import os\n",
        "image_dir = r'/content/sample_data/'\n",
        "fileExt = r'.hdf'\n",
        "temp_dir = r'/content/'\n",
        "imgs_list = [f for f in os.listdir(image_dir) if f.endswith(fileExt)]\n",
        "imgs_list.sort()\n",
        "imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n",
        "#creating a loop to run for all files in the directory having extension \".hdf\"\n",
        "for name, image in enumerate(imgs_path):\n",
        "  print('ok')\n",
        "  print(name)\n",
        "  print(image)\n",
        "  path = imgs_list[name]\n",
        "  #converting to sinusodial coordinate system to wgs 84 utm 43\n",
        "  import pymodis\n",
        "  subset = [0,0,0,1,0,0,0]\n",
        "  pymodis.convertmodis_gdal.convertModisGDAL( image_dir + path, temp_dir + path[:-4], subset, 2400,outformat=\"GTiff\",epsg=32643).run()\n",
        "  \n",
        "  #cutting into Area Of interest using shape file\n",
        "  !gdalwarp \\\n",
        "  -cutline /content/beasBasinShapeFile.shp  -crop_to_cutline \\\n",
        "  {temp_dir}{path[:-4]}\"_ndsi.tif\" \\\n",
        "  {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\"\n",
        "\n",
        "  !rm -r {temp_dir}{path[:-4]}\"_ndsi.tif\"\n",
        "  #deleting file\n",
        "\n",
        "  #creating file NDSI>=0.4\n",
        "  !gdal_calc.py \\\n",
        "  -A {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\" \\\n",
        "  --outfile={temp_dir}{path[:-4]}\"_result.tif\" \\\n",
        "  --calc=\"A/10>=40\"\n",
        "\n",
        "  !rm -r {temp_dir}{path[:-4]}\"_ndsi_clipped.tif\"\n",
        "  #deleting file\n",
        "\n",
        "  #counting Snow and Non-Snowpixels  \n",
        "  import gdalnumeric\n",
        "  raster_file = gdalnumeric.LoadFile(temp_dir + path[:-4]+\"_result.tif\")\n",
        "  pixel_count_snow = (raster_file ==0).sum()\n",
        "  pixel_count_notsnow = (raster_file ==1).sum()\n",
        "  print(\"snow:\",pixel_count_snow,\" not snow:\",pixel_count_notsnow)\n",
        "\n",
        "  #creating constant for multiplication at pixel size with count of pixels\n",
        "  m1 = !gdalinfo -json {temp_dir}{path[:-4]}\"_result.tif\" | jq -r .geoTransform \n",
        "  coonstant_c = int(m1[2][9:9+4])*int(m1[6][9+1:9+4+1])/1000000\n",
        "\n",
        "  !rm -r {temp_dir}{path[:-4]}\"_result.tif\"\n",
        "  #deleting file\n",
        "\n",
        "  #combining name of the  file , SnowCoverArea , NonSnowCoverArea to a text file format\n",
        "  lines = str(path[8:8+8] + \",\" + str(coonstant_c*pixel_count_snow) + \",\" + str(coonstant_c*pixel_count_notsnow))\n",
        "  with open(temp_dir+\"out.txt\",\"a+\", encoding=\"utf-8\") as f:\n",
        "    f.writelines('\\n'+ lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioDVU1ltWkZ4",
        "outputId": "6a9ce84d-7cae-4954-d4c5-59ec8e743bcd"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n",
            "0\n",
            "/content/sample_data/MOD10A1.A2000055.h24v05.061.2020037171821.hdf\n",
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2000055.h24v05.061.2020037171821.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2000055.h24v05.061.2020037171821.hdf' reprojected\n",
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2000055.h24v05.061.2020037171821_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2000055.h24v05.061.2020037171821_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2000055.h24v05.061.2020037171821_ndsi.tif to destination /content/MOD10A1.A2000055.h24v05.061.2020037171821_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 100 - Done\n",
            "snow: 1957  not snow: 0\n",
            "ok\n",
            "1\n",
            "/content/sample_data/MOD10A1.A2000056.h24v05.061.2020037163228.hdf\n",
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2000056.h24v05.061.2020037163228.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2000056.h24v05.061.2020037163228.hdf' reprojected\n",
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2000056.h24v05.061.2020037163228_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2000056.h24v05.061.2020037163228_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2000056.h24v05.061.2020037163228_ndsi.tif to destination /content/MOD10A1.A2000056.h24v05.061.2020037163228_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 100 - Done\n",
            "snow: 0  not snow: 0\n",
            "ok\n",
            "2\n",
            "/content/sample_data/MOD10A1.A2000057.h24v05.061.2020037182924.hdf\n",
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2000057.h24v05.061.2020037182924.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2000057.h24v05.061.2020037182924.hdf' reprojected\n",
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2000057.h24v05.061.2020037182924_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2000057.h24v05.061.2020037182924_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2000057.h24v05.061.2020037182924_ndsi.tif to destination /content/MOD10A1.A2000057.h24v05.061.2020037182924_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 100 - Done\n",
            "snow: 1077  not snow: 880\n",
            "ok\n",
            "3\n",
            "/content/sample_data/MOD10A1.A2000058.h24v05.061.2020037193319.hdf\n",
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2000058.h24v05.061.2020037193319.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2000058.h24v05.061.2020037193319.hdf' reprojected\n",
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2000058.h24v05.061.2020037193319_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2000058.h24v05.061.2020037193319_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2000058.h24v05.061.2020037193319_ndsi.tif to destination /content/MOD10A1.A2000058.h24v05.061.2020037193319_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 100 - Done\n",
            "snow: 1247  not snow: 710\n",
            "ok\n",
            "4\n",
            "/content/sample_data/MOD10A1.A2000059.h24v05.061.2020037200350.hdf\n",
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2000059.h24v05.061.2020037200350.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2000059.h24v05.061.2020037200350.hdf' reprojected\n",
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2000059.h24v05.061.2020037200350_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2000059.h24v05.061.2020037200350_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2000059.h24v05.061.2020037200350_ndsi.tif to destination /content/MOD10A1.A2000059.h24v05.061.2020037200350_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 100 - Done\n",
            "snow: 1140  not snow: 817\n",
            "ok\n",
            "5\n",
            "/content/sample_data/MOD10A1.A2000060.h24v05.061.2020037202151.hdf\n",
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2000060.h24v05.061.2020037202151.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2000060.h24v05.061.2020037202151.hdf' reprojected\n",
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2000060.h24v05.061.2020037202151_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2000060.h24v05.061.2020037202151_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2000060.h24v05.061.2020037202151_ndsi.tif to destination /content/MOD10A1.A2000060.h24v05.061.2020037202151_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 100 - Done\n",
            "snow: 173  not snow: 1784\n",
            "ok\n",
            "6\n",
            "/content/sample_data/MOD10A1.A2000061.h24v05.061.2020037204453.hdf\n",
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2000061.h24v05.061.2020037204453.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2000061.h24v05.061.2020037204453.hdf' reprojected\n",
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2000061.h24v05.061.2020037204453_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2000061.h24v05.061.2020037204453_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2000061.h24v05.061.2020037204453_ndsi.tif to destination /content/MOD10A1.A2000061.h24v05.061.2020037204453_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 100 - Done\n",
            "snow: 626  not snow: 1330\n",
            "ok\n",
            "7\n",
            "/content/sample_data/MOD10A1.A2000062.h24v05.061.2020037210624.hdf\n",
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2000062.h24v05.061.2020037210624.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2000062.h24v05.061.2020037210624.hdf' reprojected\n",
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2000062.h24v05.061.2020037210624_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2000062.h24v05.061.2020037210624_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2000062.h24v05.061.2020037210624_ndsi.tif to destination /content/MOD10A1.A2000062.h24v05.061.2020037210624_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 100 - Done\n",
            "snow: 1278  not snow: 679\n",
            "ok\n",
            "8\n",
            "/content/sample_data/MOD10A1.A2000063.h24v05.061.2020037212628.hdf\n",
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2000063.h24v05.061.2020037212628.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2000063.h24v05.061.2020037212628.hdf' reprojected\n",
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2000063.h24v05.061.2020037212628_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2000063.h24v05.061.2020037212628_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2000063.h24v05.061.2020037212628_ndsi.tif to destination /content/MOD10A1.A2000063.h24v05.061.2020037212628_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 100 - Done\n",
            "snow: 1284  not snow: 673\n",
            "ok\n",
            "9\n",
            "/content/sample_data/MOD10A1.A2000064.h24v05.061.2020037214526.hdf\n",
            "Layer HDF4_EOS:EOS_GRID:\"/content/sample_data/MOD10A1.A2000064.h24v05.061.2020037214526.hdf\":MOD_Grid_Snow_500m:NDSI reprojected\n",
            "All layer for dataset '/content/sample_data/MOD10A1.A2000064.h24v05.061.2020037214526.hdf' reprojected\n",
            "Creating output file that is 74P x 45L.\n",
            "Processing input file /content/MOD10A1.A2000064.h24v05.061.2020037214526_ndsi.tif.\n",
            "Using internal nodata values (e.g. 0) for image /content/MOD10A1.A2000064.h24v05.061.2020037214526_ndsi.tif.\n",
            "Copying nodata values from source /content/MOD10A1.A2000064.h24v05.061.2020037214526_ndsi.tif to destination /content/MOD10A1.A2000064.h24v05.061.2020037214526_ndsi_clipped.tif.\n",
            "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
            "0 .. 100 - Done\n",
            "snow: 410  not snow: 1547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gdal_calc.py \n",
        "!gdal_calc.py \\\n",
        "  -A {path} \\\n",
        "  --A_band=4 \\\n",
        "  -B {path} \\\n",
        "  --B_band=6 \\\n",
        "  --outfile=result.tif \\\n",
        "  --calc=\"(A-B)/(A+B)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pay1nAsOfsrq",
        "outputId": "ae2742f7-f81a-4544-8c8d-c7cf6d457415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR 4: path: No such file or directory\n",
            "No such file or directory: 'path'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -e robots=off -m -np -R .html,.tmp -nH --cut-dirs=3 \\\n",
        "  \"https://ladsweb.modaps.eosdis.nasa.gov/archive/allData/61/MOD01/2021/206/MOD01.A2021206.0510.061.2021217151715.hdf\" \\\n",
        "  --header \"Authorization: Bearer a3JveTAwMDE6YzJOb2JHSnZlVEkyUUdkdFlXbHNMbU52YlE9PToxNjYwMDU0MjQ1OmM3NjNmMmI0ODdiNDY3M2YwZmM0OWM0MGM1ODhhNmM2NzNkYmNhZmM\" \\\n",
        "  -P .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CANw1lUKFYhn",
        "outputId": "80833fbc-7d04-4d4d-c9f6-c23be81b78b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "504 Gateway Time-out\n",
            "Retrying.\n",
            "\n",
            "--2022-08-09 16:46:59--  (try: 4)  https://ladsweb.modaps.eosdis.nasa.gov/archive/allData/61/MOD01/2021/206/MOD01.A2021206.0510.061.2021217151715.hdf\n",
            "Reusing existing connection to ladsweb.modaps.eosdis.nasa.gov:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 574155118 (548M) [application/x-hdf]\n",
            "Saving to: ‘./MOD01/2021/206/MOD01.A2021206.0510.061.2021217151715.hdf’\n",
            "\n",
            "MOD01/2021/206/MOD0 100%[===================>] 547.56M  42.5MB/s    in 6.3s    \n",
            "\n",
            "2022-08-09 16:47:19 (87.5 MB/s) - ‘./MOD01/2021/206/MOD01.A2021206.0510.061.2021217151715.hdf’ saved [574155118/574155118]\n",
            "\n",
            "FINISHED --2022-08-09 16:47:19--\n",
            "Total wall clock time: 3m 26s\n",
            "Downloaded: 1 files, 548M in 6.3s (87.5 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e0qhyZwiMg2"
      },
      "source": [
        "# DOWNLOADING MOD10A.061"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvRYPgILiUz9"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/OUT/data/MOD10A1v6\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9uD7meghBMj"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# ----------------------------------------------------------------------------\n",
        "# NSIDC Data Download Script\n",
        "#\n",
        "# Copyright (c) 2022 Regents of the University of Colorado\n",
        "# Permission is hereby granted, free of charge, to any person obtaining\n",
        "# a copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "# The above copyright notice and this permission notice shall be included\n",
        "# in all copies or substantial portions of the Software.\n",
        "#\n",
        "# Tested in Python 2.7 and Python 3.4, 3.6, 3.7, 3.8, 3.9\n",
        "#\n",
        "# To run the script at a Linux, macOS, or Cygwin command-line terminal:\n",
        "#   $ python nsidc-data-download.py\n",
        "#\n",
        "# On Windows, open Start menu -> Run and type cmd. Then type:\n",
        "#     python nsidc-data-download.py\n",
        "#\n",
        "# The script will first search Earthdata for all matching files.\n",
        "# You will then be prompted for your Earthdata username/password\n",
        "# and the script will download the matching files.\n",
        "#\n",
        "# If you wish, you may store your Earthdata username/password in a .netrc\n",
        "# file in your $HOME directory and the script will automatically attempt to\n",
        "# read this file. The .netrc file should have the following format:\n",
        "#    machine urs.earthdata.nasa.gov login MYUSERNAME password MYPASSWORD\n",
        "# where 'MYUSERNAME' and 'MYPASSWORD' are your Earthdata credentials.\n",
        "#\n",
        "# Instead of a username/password, you may use an Earthdata bearer token.\n",
        "# To construct a bearer token, log into Earthdata and choose \"Generate Token\".\n",
        "# To use the token, when the script prompts for your username,\n",
        "# just press Return (Enter). You will then be prompted for your token.\n",
        "# You can store your bearer token in the .netrc file in the following format:\n",
        "#    machine urs.earthdata.nasa.gov login token password MYBEARERTOKEN\n",
        "# where 'MYBEARERTOKEN' is your Earthdata bearer token.\n",
        "#\n",
        "from __future__ import print_function\n",
        "\n",
        "import base64\n",
        "import getopt\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import netrc\n",
        "import os.path\n",
        "import ssl\n",
        "import sys\n",
        "import time\n",
        "from getpass import getpass\n",
        "\n",
        "try:\n",
        "    from urllib.parse import urlparse\n",
        "    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n",
        "    from urllib.error import HTTPError, URLError\n",
        "except ImportError:\n",
        "    from urlparse import urlparse\n",
        "    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n",
        "\n",
        "short_name = 'MOD10A2'\n",
        "version = '61'\n",
        "time_start = '2000-02-18T00:00:00Z'\n",
        "time_end = '2022-08-04T20:12:39Z'\n",
        "bounding_box = '75.67,31.13,77.84,32.59'\n",
        "polygon = ''\n",
        "filename_filter = ''\n",
        "url_list = []\n",
        "\n",
        "CMR_URL = 'https://cmr.earthdata.nasa.gov'\n",
        "URS_URL = 'https://urs.earthdata.nasa.gov'\n",
        "CMR_PAGE_SIZE = 2000\n",
        "CMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n",
        "                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n",
        "                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n",
        "\n",
        "\n",
        "def get_username():\n",
        "    username = 'kroy0001'\n",
        "    while not username:\n",
        "      # For Python 2/3 compatibility:\n",
        "      try:\n",
        "          do_input = raw_input  # noqa\n",
        "      except NameError:\n",
        "          do_input = input\n",
        "      username = do_input('Earthdata username (or press Return to use a bearer token): ')\n",
        "    return username\n",
        "\n",
        "\n",
        "def get_password():\n",
        "    password = '/#j%kWrPA,8.HRe'\n",
        "    while not password:\n",
        "        password = getpass('password: ')\n",
        "    return password\n",
        "\n",
        "\n",
        "def get_token():\n",
        "    token = ''\n",
        "    while not token:\n",
        "        token = getpass('bearer token: ')\n",
        "    return token\n",
        "\n",
        "\n",
        "def get_login_credentials():\n",
        "    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n",
        "    credentials = None\n",
        "    token = None\n",
        "\n",
        "    try:\n",
        "        info = netrc.netrc()\n",
        "        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n",
        "        if username == 'token':\n",
        "            token = password\n",
        "        else:\n",
        "            credentials = '{0}:{1}'.format(username, password)\n",
        "            credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n",
        "    except Exception:\n",
        "        username = None\n",
        "        password = None\n",
        "\n",
        "    if not username:\n",
        "        username = get_username()\n",
        "        if len(username):\n",
        "            password = get_password()\n",
        "            credentials = '{0}:{1}'.format(username, password)\n",
        "            credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n",
        "        else:\n",
        "            token = get_token()\n",
        "\n",
        "    return credentials, token\n",
        "\n",
        "\n",
        "def build_version_query_params(version):\n",
        "    desired_pad_length = 3\n",
        "    if len(version) > desired_pad_length:\n",
        "        print('Version string too long: \"{0}\"'.format(version))\n",
        "        quit()\n",
        "\n",
        "    version = str(int(version))  # Strip off any leading zeros\n",
        "    query_params = ''\n",
        "\n",
        "    while len(version) <= desired_pad_length:\n",
        "        padded_version = version.zfill(desired_pad_length)\n",
        "        query_params += '&version={0}'.format(padded_version)\n",
        "        desired_pad_length -= 1\n",
        "    return query_params\n",
        "\n",
        "\n",
        "def filter_add_wildcards(filter):\n",
        "    if not filter.startswith('*'):\n",
        "        filter = '*' + filter\n",
        "    if not filter.endswith('*'):\n",
        "        filter = filter + '*'\n",
        "    return filter\n",
        "\n",
        "\n",
        "def build_filename_filter(filename_filter):\n",
        "    filters = filename_filter.split(',')\n",
        "    result = '&options[producer_granule_id][pattern]=true'\n",
        "    for filter in filters:\n",
        "        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n",
        "    return result\n",
        "\n",
        "\n",
        "def build_cmr_query_url(short_name, version, time_start, time_end,\n",
        "                        bounding_box=None, polygon=None,\n",
        "                        filename_filter=None):\n",
        "    params = '&short_name={0}'.format(short_name)\n",
        "    params += build_version_query_params(version)\n",
        "    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n",
        "    if polygon:\n",
        "        params += '&polygon={0}'.format(polygon)\n",
        "    elif bounding_box:\n",
        "        params += '&bounding_box={0}'.format(bounding_box)\n",
        "    if filename_filter:\n",
        "        params += build_filename_filter(filename_filter)\n",
        "    return CMR_FILE_URL + params\n",
        "\n",
        "\n",
        "def get_speed(time_elapsed, chunk_size):\n",
        "    if time_elapsed <= 0:\n",
        "        return ''\n",
        "    speed = chunk_size / time_elapsed\n",
        "    if speed <= 0:\n",
        "        speed = 1\n",
        "    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n",
        "    i = int(math.floor(math.log(speed, 1000)))\n",
        "    p = math.pow(1000, i)\n",
        "    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n",
        "\n",
        "\n",
        "def output_progress(count, total, status='', bar_len=60):\n",
        "    if total <= 0:\n",
        "        return\n",
        "    fraction = min(max(count / float(total), 0), 1)\n",
        "    filled_len = int(round(bar_len * fraction))\n",
        "    percents = int(round(100.0 * fraction))\n",
        "    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n",
        "    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n",
        "    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n",
        "    sys.stdout.write(fmt)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n",
        "    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n",
        "    while True:\n",
        "        data = file_object.read(chunk_size)\n",
        "        if not data:\n",
        "            break\n",
        "        yield data\n",
        "\n",
        "\n",
        "def get_login_response(url, credentials, token):\n",
        "    opener = build_opener(HTTPCookieProcessor())\n",
        "\n",
        "    req = Request(url)\n",
        "    if token:\n",
        "        req.add_header('Authorization', 'Bearer {0}'.format(token))\n",
        "    elif credentials:\n",
        "        try:\n",
        "            response = opener.open(req)\n",
        "            # We have a redirect URL - try again with authorization.\n",
        "            url = response.url\n",
        "        except HTTPError:\n",
        "            # No redirect - just try again with authorization.\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print('Error{0}: {1}'.format(type(e), str(e)))\n",
        "            sys.exit(1)\n",
        "\n",
        "        req = Request(url)\n",
        "        req.add_header('Authorization', 'Basic {0}'.format(credentials))\n",
        "\n",
        "    try:\n",
        "        response = opener.open(req)\n",
        "    except HTTPError as e:\n",
        "        err = 'HTTP error {0}, {1}'.format(e.code, e.reason)\n",
        "        if 'Unauthorized' in e.reason:\n",
        "            if token:\n",
        "                err += ': Check your bearer token'\n",
        "            else:\n",
        "                err += ': Check your username and password'\n",
        "        print(err)\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print('Error{0}: {1}'.format(type(e), str(e)))\n",
        "        sys.exit(1)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "def cmr_download(urls, force=False, quiet=False):\n",
        "    \"\"\"Download files from list of urls.\"\"\"\n",
        "    if not urls:\n",
        "        return\n",
        "\n",
        "    url_count = len(urls)\n",
        "    if not quiet:\n",
        "        print('Downloading {0} files...'.format(url_count))\n",
        "    credentials = None\n",
        "    token = None\n",
        "\n",
        "    for index, url in enumerate(urls, start=1):\n",
        "        if not credentials and not token:\n",
        "            p = urlparse(url)\n",
        "            if p.scheme == 'https':\n",
        "                credentials, token = get_login_credentials()\n",
        "\n",
        "        filename = url.split('/')[-1]\n",
        "        if not quiet:\n",
        "            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n",
        "                                        url_count, filename))\n",
        "\n",
        "        try:\n",
        "            response = get_login_response(url, credentials, token)\n",
        "            length = int(response.headers['content-length'])\n",
        "            try:\n",
        "                if not force and length == os.path.getsize(filename):\n",
        "                    if not quiet:\n",
        "                        print('  File exists, skipping')\n",
        "                    continue\n",
        "            except OSError:\n",
        "                pass\n",
        "            count = 0\n",
        "            chunk_size = min(max(length, 1), 1024 * 1024)\n",
        "            max_chunks = int(math.ceil(length / chunk_size))\n",
        "            time_initial = time.time()\n",
        "            with open(filename, 'wb') as out_file:\n",
        "                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n",
        "                    out_file.write(data)\n",
        "                    if not quiet:\n",
        "                        count = count + 1\n",
        "                        time_elapsed = time.time() - time_initial\n",
        "                        download_speed = get_speed(time_elapsed, count * chunk_size)\n",
        "                        output_progress(count, max_chunks, status=download_speed)\n",
        "            if not quiet:\n",
        "                print()\n",
        "        except HTTPError as e:\n",
        "            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n",
        "        except URLError as e:\n",
        "            print('URL error: {0}'.format(e.reason))\n",
        "        except IOError:\n",
        "            raise\n",
        "\n",
        "\n",
        "def cmr_filter_urls(search_results):\n",
        "    \"\"\"Select only the desired data files from CMR response.\"\"\"\n",
        "    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n",
        "        return []\n",
        "\n",
        "    entries = [e['links']\n",
        "               for e in search_results['feed']['entry']\n",
        "               if 'links' in e]\n",
        "    # Flatten \"entries\" to a simple list of links\n",
        "    links = list(itertools.chain(*entries))\n",
        "\n",
        "    urls = []\n",
        "    unique_filenames = set()\n",
        "    for link in links:\n",
        "        if 'href' not in link:\n",
        "            # Exclude links with nothing to download\n",
        "            continue\n",
        "        if 'inherited' in link and link['inherited'] is True:\n",
        "            # Why are we excluding these links?\n",
        "            continue\n",
        "        if 'rel' in link and 'data#' not in link['rel']:\n",
        "            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n",
        "            continue\n",
        "\n",
        "        if 'title' in link and 'opendap' in link['title'].lower():\n",
        "            # Exclude OPeNDAP links--they are responsible for many duplicates\n",
        "            # This is a hack; when the metadata is updated to properly identify\n",
        "            # non-datapool links, we should be able to do this in a non-hack way\n",
        "            continue\n",
        "\n",
        "        filename = link['href'].split('/')[-1]\n",
        "        if filename in unique_filenames:\n",
        "            # Exclude links with duplicate filenames (they would overwrite)\n",
        "            continue\n",
        "        unique_filenames.add(filename)\n",
        "\n",
        "        urls.append(link['href'])\n",
        "\n",
        "    return urls\n",
        "\n",
        "\n",
        "def cmr_search(short_name, version, time_start, time_end,\n",
        "               bounding_box='', polygon='', filename_filter='', quiet=False):\n",
        "    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n",
        "    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n",
        "                                        time_start=time_start, time_end=time_end,\n",
        "                                        bounding_box=bounding_box,\n",
        "                                        polygon=polygon, filename_filter=filename_filter)\n",
        "    if not quiet:\n",
        "        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n",
        "\n",
        "    cmr_scroll_id = None\n",
        "    ctx = ssl.create_default_context()\n",
        "    ctx.check_hostname = False\n",
        "    ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    urls = []\n",
        "    hits = 0\n",
        "    while True:\n",
        "        req = Request(cmr_query_url)\n",
        "        if cmr_scroll_id:\n",
        "            req.add_header('cmr-scroll-id', cmr_scroll_id)\n",
        "        try:\n",
        "            response = urlopen(req, context=ctx)\n",
        "        except Exception as e:\n",
        "            print('Error: ' + str(e))\n",
        "            sys.exit(1)\n",
        "        if not cmr_scroll_id:\n",
        "            # Python 2 and 3 have different case for the http headers\n",
        "            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n",
        "            cmr_scroll_id = headers['cmr-scroll-id']\n",
        "            hits = int(headers['cmr-hits'])\n",
        "            if not quiet:\n",
        "                if hits > 0:\n",
        "                    print('Found {0} matches.'.format(hits))\n",
        "                else:\n",
        "                    print('Found no matches.')\n",
        "        search_page = response.read()\n",
        "        search_page = json.loads(search_page.decode('utf-8'))\n",
        "        url_scroll_results = cmr_filter_urls(search_page)\n",
        "        if not url_scroll_results:\n",
        "            break\n",
        "        if not quiet and hits > CMR_PAGE_SIZE:\n",
        "            print('.', end='')\n",
        "            sys.stdout.flush()\n",
        "        urls += url_scroll_results\n",
        "\n",
        "    if not quiet and hits > CMR_PAGE_SIZE:\n",
        "        print()\n",
        "    return urls\n",
        "\n",
        "\n",
        "def main(argv=None):\n",
        "    global short_name, version, time_start, time_end, bounding_box, \\\n",
        "        polygon, filename_filter, url_list\n",
        "\n",
        "    if argv is None:\n",
        "        argv = sys.argv[1:]\n",
        "\n",
        "    force = False\n",
        "    quiet = False\n",
        "    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n",
        "\n",
        "    try:\n",
        "        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n",
        "        for opt, _arg in opts:\n",
        "            if opt in ('-f', '--force'):\n",
        "                force = True\n",
        "            elif opt in ('-q', '--quiet'):\n",
        "                quiet = True\n",
        "            elif opt in ('-h', '--help'):\n",
        "                print(usage)\n",
        "                sys.exit(0)\n",
        "    except getopt.GetoptError as e:\n",
        "        print(e.args[0])\n",
        "        print(usage)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Supply some default search parameters, just for testing purposes.\n",
        "    # These are only used if the parameters aren't filled in up above.\n",
        "    if 'short_name' in short_name:\n",
        "        short_name = 'ATL06'\n",
        "        version = '003'\n",
        "        time_start = '2018-10-14T00:00:00Z'\n",
        "        time_end = '2021-01-08T21:48:13Z'\n",
        "        bounding_box = ''\n",
        "        polygon = ''\n",
        "        filename_filter = '*ATL06_2020111121*'\n",
        "        url_list = []\n",
        "\n",
        "    try:\n",
        "        if not url_list:\n",
        "            url_list = cmr_search(short_name, version, time_start, time_end,\n",
        "                                  bounding_box=bounding_box, polygon=polygon,\n",
        "                                  filename_filter=filename_filter, quiet=quiet)\n",
        "\n",
        "        cmr_download(url_list, force=force, quiet=quiet)\n",
        "    except KeyboardInterrupt:\n",
        "        quit()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "#1284 seconds execution time"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NSIDC Data Editor.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}