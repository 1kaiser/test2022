{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjYGwwuSDgvw7xK2BZtQKM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2022/blob/main/MLP_Image_Train_Inference_JAX_v0.4.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "miM9-4pMQ23h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5442925-c572-4bbc-fac8-eb3b96d8d1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RUN** "
      ],
      "metadata": {
        "id": "bwhvgI06xTIu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI1aeBA-dCqC"
      },
      "source": [
        "### Model and training code\n",
        "\n",
        "Our model is a coordinate-based multilayer perceptron. In this example, for each input image coordinate $(x,y)$, the model predicts the associated color $(r,g,b)$ or any $(gray)$.\n",
        "\n",
        "![Network diagram](https://user-images.githubusercontent.com/3310961/85066930-ad444580-b164-11ea-9cc0-17494679e71f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[--xla_force_host_platform_device_count](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html#:~:text=When%20running%20on-,CPU,-you%20can%20always)"
      ],
      "metadata": {
        "id": "Zehuof-K_cZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ\n",
        "import os\n",
        "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
        "import jax\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "rUeQSSu5_Jwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932f72e0-d046-43a0-a7dc-452690ce07e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[CpuDevice(id=0),\n",
              " CpuDevice(id=1),\n",
              " CpuDevice(id=2),\n",
              " CpuDevice(id=3),\n",
              " CpuDevice(id=4),\n",
              " CpuDevice(id=5),\n",
              " CpuDevice(id=6),\n",
              " CpuDevice(id=7)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(inputs):\n",
        "    print(\"positional_encoding start\")\n",
        "    batch_size, _ = inputs.shape;print(inputs.shape)\n",
        "    inputs_freq = jax.vmap(lambda x: inputs * 2.0 ** x)(jnp.arange(positional_encoding_dims));print(inputs_freq.shape)\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)]);print(x.shape)\n",
        "    x = x.swapaxes(0, 2);print(x.shape)\n",
        "    x = x.reshape([batch_size, -1]);print(x.shape)\n",
        "    x = jnp.concatenate([inputs, x], axis=-1);print(x.shape)\n",
        "    print(\"positional_encoding end\")\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "z9aPWpu5iJ1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.config.update('jax_array', True)"
      ],
      "metadata": {
        "id": "1PJXyJuwOk3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX code >>>"
      ],
      "metadata": {
        "id": "jbgY27TvT198"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import functools\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "jax.config.update('jax_array', True)"
      ],
      "metadata": {
        "id": "udenxESQTsJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(jax.local_devices()) < 8:\n",
        "  raise Exception(\"Notebook requires 8 devices to run\")"
      ],
      "metadata": {
        "id": "454IFHaYTvvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import PositionalSharding"
      ],
      "metadata": {
        "id": "jisZICmvT3V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Sharding object to distribute a value across devices:\n",
        "sharding = PositionalSharding(mesh_utils.create_device_mesh((8,)))"
      ],
      "metadata": {
        "id": "5f31r70gT4_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install rich #`visualize_sharding` requires `rich` to be installed.\n",
        "# Create an array of random values:\n",
        "x = jax.random.normal(jax.random.PRNGKey(0), (8192, 8192))\n",
        "# and use jax.device_put to distribute it across devices:\n",
        "y = jax.device_put(x, sharding.reshape(2, 4))\n",
        "y.shape\n",
        "type(y)  #jax.debug.visualize_array_sharding(y)\n",
        "sharding = sharding.reshape(4, 2)\n",
        "print(sharding)"
      ],
      "metadata": {
        "id": "hT6XZ7qYT6pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cec8997-946f-4724-86e6-7ef2edee3fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rich\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 237 kB 21.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich) (4.4.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich) (2.6.1)\n",
            "Installing collected packages: commonmark, rich\n",
            "Successfully installed commonmark-0.9.1 rich-12.6.0\n",
            "PositionalSharding([[{CPU 0} {CPU 1}]\n",
            "                    [{CPU 2} {CPU 3}]\n",
            "                    [{CPU 4} {CPU 5}]\n",
            "                    [{CPU 6} {CPU 7}]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP MODEL\n",
        "Basically, passing input points through a simple Fourier Feature Mapping enables an MLP to learn high-frequency functions (such as an RGB image) in low-dimensional problem domains (such as a 2D coordinate of pixels)."
      ],
      "metadata": {
        "id": "DllYcUtgovO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ\n",
        "!python -m pip install -q -U flax\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import flax\n",
        "import optax\n",
        "from typing import Any\n",
        "\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "num_dense_layers = 8 # Number of dense layers in MLP\n",
        "dense_layer_width = 256 # Dimentionality of dense layers' output space \n",
        "\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "        x = positional_encoding(input_points) if self.apply_positional_encoding else input_points\n",
        "        print(\"network model start\")\n",
        "        print(x.shape)\n",
        "        for i in range(num_dense_layers):\n",
        "            x = nn.Dense(dense_layer_width,dtype=self.dtype,precision=self.precision)(x)\n",
        "            x = nn.relu(x)\n",
        "            x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "            print(x.shape)\n",
        "  \n",
        "        x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "        print(x.shape)\n",
        "        print(\"network model end\")\n",
        "        return x\n",
        "##########################################<< MLP MODEL >>#########################################"
      ],
      "metadata": {
        "id": "VRkotxnvvHrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497a3f88-be64-4a1a-af5b-9f651c4ede0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197 kB 7.9 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 237 kB 46.6 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 154 kB 63.4 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66 kB 5.2 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.3 MB 52.5 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51 kB 7.3 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85 kB 4.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### initialize the module"
      ],
      "metadata": {
        "id": "4o02pjAJqdjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#‚úÖ\n",
        "!python -m pip install -q -U flax\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "\n",
        "def Create_train_state(model, r_key, shape, learning_rate ) -> train_state.TrainState:\n",
        "    print(shape)\n",
        "    variables = model.init(r_key, jnp.ones(shape))  # Initialize the Model with some variables\n",
        "    optimizer = optax.adam(learning_rate) # Create the optimizer taken from Deepmind takes care of optimization\n",
        "    # return a state with a created state actually a pytree >>> may be the idea is to easily augment to other languages or librarys like markdown , xml or yaml or json >>> for conversion to WASM on WEBGPU ??\n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=variables['params']\n",
        "    )\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size_no = 64\n",
        "\n",
        "model = MLPModel() # Instantiate the Model\n",
        "\n",
        "key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "x = jnp.ones(shape=(batch_size_no, 28, 28, 1)) # Dummy Input\n",
        "_, image_height, image_width, channels = x.shape\n",
        "\n",
        "state = Create_train_state( model, rng, (image_height * image_width, channels), learning_rate )\n"
      ],
      "metadata": {
        "id": "rJEuhCl5xuR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb38dcf-340d-499e-c777-c04b1bc4a5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 1)\n",
            "positional_encoding start\n",
            "(784, 1)\n",
            "(6, 784, 1)\n",
            "(2, 6, 784, 1)\n",
            "(784, 6, 2, 1)\n",
            "(784, 12)\n",
            "(784, 13)\n",
            "positional_encoding end\n",
            "network model start\n",
            "(784, 13)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 257)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 256)\n",
            "(784, 1)\n",
            "network model end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_difference_loss(logits, labels):\n",
        "    loss = .5 * jnp.mean((logits - labels) ** 2)\n",
        "    return loss\n",
        "def compute_metrics(*, logits, labels):\n",
        "  loss = image_difference_loss(logits, labels)\n",
        "  loss = lax.pmean(loss, axis_name=\"batch\");print(\"ok4\")\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'logits': logits, #PREDICTED IMAGE\n",
        "      'labels': labels  #ACTUAL IMAGE\n",
        "  }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "f-6Gf-ee-9Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "\n",
        "def train_step(state: train_state.TrainState, batch: jnp.asarray, rng):\n",
        "    print(batch)\n",
        "    image, label = batch\n",
        "    print(image,\"<<<image\")\n",
        "    print(label,\"<<<label\")    \n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({'params': params}, image);print(\"done1\",logits.shape) # apply function applys the input image to the model then it generated the predicted image called \"logits\"\n",
        "        loss =  image_difference_loss(logits, label);print(\"done2\",loss.shape) # loss is calculated by RMSE of logits i.e. predicted image and label i.e. input image\n",
        "        return loss, logits\n",
        "\n",
        "    print(\"ok1really\")\n",
        "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True);print(\"ok1\") # calculated the value and gradient from (loss function) output i.e. loss and logits>>> \n",
        "    (_, logits), grads = gradient_fn(state.params);print(\"ok2\") # takes the old state then applies gradient functions on its parameters to create gradient\n",
        "    grads = lax.pmean(grads,\"batch\");print(\"ok4\")# takes the mean of gradients generated parallelly from other devices\n",
        "    state = state.apply_gradients(grads=grads);print(\"ok5\") # Updating the state weights by applying changes from new gradient\n",
        "    logs = compute_metrics(logits=logits, labels=label);print(\"ok7\")\n",
        "    return state, logs\n",
        "\n",
        "parallel_train_step = jax.pmap(train_step, \"batch\")\n",
        "# parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", in_axes = (0, 0, 0))\n",
        "\n",
        "import jax\n",
        "@jax.jit\n",
        "def eval_step(state, batch):\n",
        "    image, label = batch\n",
        "    logits = state.apply_fn({'params': state.params}, image)\n",
        "    return compute_metrics(logits=logits, labels=label)\n"
      ],
      "metadata": {
        "id": "wskhkyy49MIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train & evaluation function"
      ],
      "metadata": {
        "id": "LvNe_bDIz0Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1UgWEotThxnP-Vh-h83-VcTPMkKWmgCDe #downloading MAP-DEM "
      ],
      "metadata": {
        "id": "BoQeqWgTkX3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://live.staticflickr.com/7492/15677707699_d9d67acf9d_b.jpg -O a.jpg\n",
        "\n",
        "newsize = (150, 150) # /.... 233 * 454\n",
        "from PIL import Image\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def imageGRAY(argv):\n",
        "    im = Image.open(argv).convert('L')\n",
        "    # im = Image.open(argv).convert('L')\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu\n"
      ],
      "metadata": {
        "id": "QjJ1VIiKgs0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fca40c6-924a-40e0-84d2-72e2d8b88b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-21 05:28:23--  https://live.staticflickr.com/7492/15677707699_d9d67acf9d_b.jpg\n",
            "Resolving live.staticflickr.com (live.staticflickr.com)... 99.84.205.159, 2600:9000:201f:ea00:0:5a51:64c9:c681, 2600:9000:201f:b800:0:5a51:64c9:c681, ...\n",
            "Connecting to live.staticflickr.com (live.staticflickr.com)|99.84.205.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [image/jpeg]\n",
            "Saving to: ‚Äòa.jpg‚Äô\n",
            "\n",
            "a.jpg                   [ <=>                ] 230.86K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-12-21 05:28:23 (10.3 MB/s) - ‚Äòa.jpg‚Äô saved [236398]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download [flower dataset](https://www.kaggle.com/datasets/alxmamaev/flowers-recognition?resource=download) from kaggle."
      ],
      "metadata": {
        "id": "go6vwNDjvoSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/archive.zip\" \"/content/\"\n",
        "!unzip /content/archive.zip #unzipping the flower images from archive.."
      ],
      "metadata": {
        "id": "yNgHqCmMRR2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or"
      ],
      "metadata": {
        "id": "p7vs4FdHRQv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1SynswUkxdl6B3c6Uc7Q6MhLG3XirrHd9 # downloading from google drive saved location.. \n",
        "!unzip /content/archive.zip #unzipping the flower images from archive.."
      ],
      "metadata": {
        "id": "F_FW-EGUd5EJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b63595f-768d-4b34-eb9c-5f7a90e667a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1SynswUkxdl6B3c6Uc7Q6MhLG3XirrHd9 \n",
            "\n",
            "unzip:  cannot find or open /content/archive.zip, /content/archive.zip.zip or /content/archive.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/flowers/all\n",
        "!cp /content/flowers/daisy/* /content/flowers/all\n",
        "!cp /content/flowers/dandelion/* /content/flowers/all\n",
        "!cp /content/flowers/rose/* /content/flowers/all\n",
        "!cp /content/flowers/sunflower/* /content/flowers/all\n",
        "!cp /content/flowers/tulip/* /content/flowers/all"
      ],
      "metadata": {
        "id": "-Zi4PcrcKTFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing batching of the dataset frfom the total dataset, by assuming batch size as 8 then running 50 epochs over the batch , then moving processing onto next batch>>>\n",
        "batch_size = jax.device_count()    #///// batchsize depending on number of devices available for processing\n",
        "import os\n",
        "image_dir = r'/content/flowers/tulip/'\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"c\",\".jpg\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.jpg'\n",
        "expression_b2 = bandend[1]\n",
        "total_images =  [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "total_images.sort()\n",
        "total_images_path = [os.path.join(image_dir, i) for i in total_images if i != 'outputs']\n",
        "no_of_batches = int(len(total_images_path)/batch_size)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o724RVHZweXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_folder = '/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/DepthMap'\n",
        "\n",
        "\n",
        "import os\n",
        " \n",
        "# Read images with OpenCV.\n",
        "#images= None\n",
        "image_dir = \"MEDIAPIPEinput/\"\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "image_dir_out = \"annotated_images/\"\n",
        "os.makedirs(image_dir_out, exist_ok=True)"
      ],
      "metadata": {
        "id": "1hMnIi4zKqfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/MEDIAPIPEinput/input1 /content/annotated_images/outputs_leres"
      ],
      "metadata": {
        "id": "4mgsEmD7LG5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "\n",
        "!cp -r {drive_folder}'/MEDIAPIPEinput' {image_dir}\n",
        "!cp -r {drive_folder}'/annotated_images' {image_dir_out}\n",
        "#############################################################################"
      ],
      "metadata": {
        "id": "hEdxwiLmK-vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# testing batching of the dataset frfom the total dataset, by assuming batch size as 8 then running 50 epochs over the batch , then moving processing onto next batch>>>\n",
        "batch_size = jax.device_count()    #///// batchsize depending on number of devices available for processing\n",
        "import os\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"c\",\".png\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.jpg'\n",
        "expression_b2 = bandend[1]\n",
        "image_dir = \"MEDIAPIPEinput/MEDIAPIPEinput\"\n",
        "image_dir_out = \"annotated_images/annotated_images\"\n",
        "\n",
        "total_images =  [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "total_images.sort()\n",
        "total_images_path = [os.path.join(image_dir, i) for i in total_images if i != 'outputs']\n",
        "no_of_batches = int(len(total_images_path)/batch_size)\n",
        "total_images_annotate =  [f for f in os.listdir(image_dir_out) if f.__contains__(expression_b2)]\n",
        "total_images_annotate.sort()\n",
        "total_images_annotate_path = [os.path.join(image_dir_out, i) for i in total_images_annotate if i != 'outputs']\n"
      ],
      "metadata": {
        "id": "ExSeeC70ICw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(total_images)\n",
        "len(total_images_annotate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq8Z7XnxLhd3",
        "outputId": "fc479dca-ef23-4d75-fe6b-087b3bbcbe7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "343"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax.device_count()"
      ],
      "metadata": {
        "id": "uiksWjeoDmWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4900f9bd-3465-4f06-a9cc-25ea45d68ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **tensorboard visualization of loss graph**"
      ],
      "metadata": {
        "id": "h7tUJb9cn4Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/ckpts"
      ],
      "metadata": {
        "id": "64H_wYz6LRGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c953381-ec60-4ee3-9cbd-71689fa92d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/ckpts': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "8CzwuHhg0I7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "logdir = \"runs\"\n",
        "\n",
        "writer = SummaryWriter(logdir)"
      ],
      "metadata": {
        "id": "Ykdz88NxG-N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir={logdir}"
      ],
      "metadata": {
        "id": "w1h6oZge0L7S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "b3b4b31a-ebad-4200-fee9-2a01f8879d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RUN 2**"
      ],
      "metadata": {
        "id": "Ns6S0Bmrxhyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/ckpts/checkpoint_37 /content/"
      ],
      "metadata": {
        "id": "HhiZfhjTCbdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # **üë†HIGH HEELS RUN >>>>>>>>>>>** { vertical-output: true }\n",
        "newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "def batchedimages(image_locations):\n",
        "  ddyss = jnp.asarray((imageRGB(total_images_path[image_locations[0]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[1]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[2]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[3]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[4]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[5]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[6]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[7]])[1]))\n",
        "  ddxss = jnp.asarray((imageGRAY(total_images_annotate_path[image_locations[0]])[1],\n",
        "                      imageGRAY(total_images_annotate_path[image_locations[1]])[1],\n",
        "                      imageGRAY(total_images_annotate_path[image_locations[2]])[1],\n",
        "                      imageGRAY(total_images_annotate_path[image_locations[3]])[1],\n",
        "                      imageGRAY(total_images_annotate_path[image_locations[4]])[1],\n",
        "                      imageGRAY(total_images_annotate_path[image_locations[5]])[1],\n",
        "                      imageGRAY(total_images_annotate_path[image_locations[6]])[1],\n",
        "                      imageGRAY(total_images_annotate_path[image_locations[7]])[1]))\n",
        "  #print(ddyss.shape,\"<<<< ddyss.shape???\",ddxss.shape,\"<<<< ddxss.shape???\") #to check shape \n",
        "  batch_ccc = ddyss, ddxss\n",
        "  return batch_ccc\n",
        "  \n",
        "def data_stream():\n",
        "  key = random.PRNGKey(0)\n",
        "  perm = random.permutation(key, len(total_images_path))\n",
        "  for i in range(no_of_batches):\n",
        "    batch_idx = perm[i * batch_size : (i + 1) * batch_size]; #print(batch_idx)\n",
        "    yield batchedimages(batch_idx)\n",
        "\n",
        "batches = data_stream()  ### this stream will utilize the array of paths of images to a folder, then \"generate\" batches into the variable\n",
        "\n",
        "next(batches)[0].shape ### this command starts initial 8 image  stream, if callled inside a iteration loop then it will get next images for calculations>>>\n",
        "vv, shapea, channels = next(batches)[0].shape # seitting values >>> [0] 8 784 3  [1] 8 784 1 ; RGB & GRAYSCALE versions 8 images each converted to 1-D array\n",
        "print(vv, shapea, channels)\n",
        "######################<<< summary writer for tensor board\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# logdir = \"runs\"\n",
        "# writer = SummaryWriter(logdir)\n",
        "######################\n",
        "######################\n",
        "rng = jax.random.PRNGKey(0)\n",
        "# dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "######################\n",
        "#################################<<< checking if checkpoint already available\n",
        "import os # importing os module\n",
        "import re # to find file using regular expression\n",
        "checkpoint_available = 0\n",
        "pattern = re.compile(\"checkpoint_\\d+\")   # to search for \"checkpoint_*munerical value*\" numerical value of any length is denoted by regular expression \"\\d+\"\n",
        "dir = \"/content/ckpts/\"\n",
        "isFile = os.path.isdir(dir)\n",
        "if isFile:\n",
        "  for filepath in os.listdir(dir):\n",
        "      if pattern.match(filepath):\n",
        "          checkpoint_available = 1\n",
        "#################################\n",
        "##########################################<<< loading checkpoint by checking the Flag available\n",
        "from flax.training import checkpoints\n",
        "if checkpoint_available:\n",
        "  CKPT_DIR = 'ckpts'\n",
        "  restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "  #state = flax.jax_utils.replicate(restored_state)\n",
        "  print(\"true <<< File loaded for and replicated to all devices\")\n",
        "##########################################\n",
        "######################<<<< initiating train state\n",
        "count = 0\n",
        "if count == 0 :\n",
        "  state = Create_train_state( model, rng, (shapea, channels), learning_rate ) \n",
        "  count = 1\n",
        "state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "######################\n",
        "from flax.training import checkpoints\n",
        "import time\n",
        "total_epochs = 50\n",
        "for epochs in range(total_epochs):   # EPOCHS for training & updating the initiated state, metrics may show the loss in each epochs or iteration\n",
        "  start_time = time.time()\n",
        "  batches = data_stream()  ### this stream will utilize the array of paths of images to a folder, then \"generate\" batches into the variable\n",
        "  if checkpoint_available:\n",
        "    CKPT_DIR = 'ckpts'\n",
        "    restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "    state = restored_state\n",
        "    checkpoint_available = 0 # << Flag updated >>> to stop loading the same checkpoint in the next iteration then remove the checkpoint directory\n",
        "    !rm -r /content/ckpts\n",
        "  for bbb in range(no_of_batches-5):\n",
        "    print(bbb,\"of total number of batches\",no_of_batches)\n",
        "    state, metrics = parallel_train_step(state, next(batches), dropout_rngs)\n",
        "    print(\"<<‚úÖ‚úÖ‚úÖepoc : \",epochs,\" complete‚úÖ‚úÖ‚úÖ>>\\n\",metrics['loss'][0]) #printing loss 1 out of 8 processed in 8 devices\n",
        "    \n",
        "    #############################################<<< output visualization üëçüèª\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    import numpy as np        # source : https://www.folkstalk.com/tech/how-to-convert-numpy-array-to-cv2-image-with-code-examples/\n",
        "    \n",
        "    print(\"logits shape ‚ö°‚ö°‚ö°‚ö°\", metrics['logits'][0].shape)\n",
        "    L1 = metrics['logits'][0]\n",
        "    # predicted_image = np.array(L1,  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "    # cv2img = cv2_imshow(predicted_image) # This work the same as passing an image\n",
        "    for i in range(0,metrics['labels'].shape[0]):\n",
        "      print(i)\n",
        "      predicted_image = np.array(metrics['logits'][i],  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "      cv2_imshow(predicted_image)\n",
        "\n",
        "      print(\"labels shape ‚ö°‚ö°‚ö°‚ö°\", metrics['labels'][0].shape)\n",
        "      L2 = metrics['labels'][0]\n",
        "      actual_image = np.array(metrics['labels'][i],  dtype=np.uint8).reshape(newsize) # This would be your image array newsize = image_width , image_height\n",
        "      #cv2img = cv2_imshow(actual_image) # This work the same as passing an image\n",
        "    #############################################<<< output visualization üëçüèª\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    writer.add_scalar('Loss', int(metrics['loss'][0]), epochs)\n",
        "  epoch_time = time.time() - start_time\n",
        "  print(f\"Epoch {epochs} in {epoch_time:0.2f} sec\")\n",
        "  ##################################################<<< model saving mechanism for flax model state as checkpoints for each epochs,\"checkpoint\" is a terminology that means all the model weights and biases during the calculation till the completion of 1 epoch were being updated, then this final set of weights and biases including their placement inside the model will be saves as a (schema+weight values) saved as checkpoint in the mentioned <<CKPT_DIR = 'ckpts'>> mentioned folder.  \n",
        "  CKPT_DIR = 'ckpts'\n",
        "  checkpoints.save_checkpoint(ckpt_dir=CKPT_DIR, target=state, step= epochs)     # naming of the checkpoint is \"checkpoint_*\"  where \"*\" => value of the steps variable, i.e. 'epochs'\n",
        "  restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state) # using to get the checkpoint loaded , it can be latest one , or if already available as checkpoint in the \"CKPT_DIR\" directory then take the file from directory then save in >> restored_checkpoints\n",
        "  ##################################################\n",
        "  # images = total_images_path[batch_idx]\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "y2IWaBMaWUkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import numpy        # source : https://www.folkstalk.com/tech/how-to-convert-numpy-array-to-cv2-image-with-code-examples/\n",
        "predicted_image = np.array(L1,  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "cv2img = cv2_imshow(predicted_image) # This work the same as passing an image\n",
        "\n",
        "\n",
        "actual_image = np.array(L2,  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "cv2img = cv2_imshow(actual_image) # This work the same as passing an image\n",
        "\n"
      ],
      "metadata": {
        "id": "CH6UCd6E-pLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RUN 3**"
      ],
      "metadata": {
        "id": "8NnOKa3cxn5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SETING UP INFERENCE** üñºÔ∏èü•Ω."
      ],
      "metadata": {
        "id": "uW052m0EdsrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batchedimages(image_locations):\n",
        "  ddyss = jnp.asarray((imageRGB(image_locations)[1],\n",
        "                      imageRGB(image_locations)[1],\n",
        "                      imageRGB(image_locations)[1],\n",
        "                      imageRGB(image_locations)[1],\n",
        "                      imageRGB(image_locations)[1],\n",
        "                      imageRGB(image_locations)[1],\n",
        "                      imageRGB(image_locations)[1],\n",
        "                      imageRGB(image_locations)[1]))\n",
        "  ddxss = jnp.asarray((imageGRAY(image_locations)[1],\n",
        "                      imageGRAY(image_locations)[1],\n",
        "                      imageGRAY(image_locations)[1],\n",
        "                      imageGRAY(image_locations)[1],\n",
        "                      imageGRAY(image_locations)[1],\n",
        "                      imageGRAY(image_locations)[1],\n",
        "                      imageGRAY(image_locations)[1],\n",
        "                      imageGRAY(image_locations)[1]))\n",
        "  #print(ddyss.shape,\"<<<< ddyss.shape???\",ddxss.shape,\"<<<< ddxss.shape???\") #to check shape \n",
        "  batch_ccc = ddyss, ddxss\n",
        "  return batch_ccc\n",
        "\n",
        "IMAGE_PATHS = \"/content/a.jpg\"\n",
        "batches = batchedimages(IMAGE_PATHS)  ### this stream will utilize the array of paths of images to a folder, then \"generate\" batches into the variable\n",
        "\n",
        "batches[0].shape ### this command starts initial 8 image  stream, if callled inside a iteration loop then it will get next images for calculations>>>\n",
        "vv, shapea, channels = batches[0].shape # seitting values >>> [0] 8 784 3  [1] 8 784 1 ; RGB & GRAYSCALE versions 8 images each converted to 1-D array\n",
        "print(vv, shapea, channels)\n",
        "\n",
        "##################################################<<< model saving mechanism for flax model state as checkpoints for each epochs,\"checkpoint\" is a terminology that means all the model weights and biases during the calculation till the completion of 1 epoch were being updated, then this final set of weights and biases including their placement inside the model will be saves as a (schema+weight values) saved as checkpoint in the mentioned <<CKPT_DIR = 'ckpts'>> mentioned folder.  \n",
        "state = Create_train_state( model, rng, (shapea, channels), learning_rate ) \n",
        "state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "\n",
        "CKPT_DIR = 'ckpts'\n",
        "restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state) # using to get the checkpoint loaded , it can be latest one , or if already available as checkpoint in the \"CKPT_DIR\" directory then take the file from directory then save in >> restored_checkpoints\n",
        "##################################################\n",
        "\n",
        "state, metrics = parallel_train_step(restored_state, batches, dropout_rngs)\n",
        "print(\"<<Calculations complete‚úÖ‚úÖ‚úÖ loss: >>\",metrics['loss'][0]) #printing loss 1 out of 8 processed in 8 devices\n",
        "\n",
        "#############################################<<< output visualization üëçüèª\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np       # source : https://www.folkstalk.com/tech/how-to-convert-numpy-array-to-cv2-image-with-code-examples/\n",
        "\n",
        "print(\"logits shape ‚ö°‚ö°‚ö°‚ö°\", metrics['logits'][0].shape)\n",
        "L1 = metrics['logits'][0]\n",
        "predicted_image = np.array(L1,  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "cv2img = cv2_imshow(predicted_image) # This work the same as passing an image\n",
        "\n",
        "print(\"labels shape ‚ö°‚ö°‚ö°‚ö°\", metrics['labels'][0].shape)\n",
        "L2 = metrics['labels'][0]\n",
        "actual_image = np.array(L2,  dtype=np.uint8).reshape(newsize) # This would be your image array newsize = image_width , image_height\n",
        "cv2img = cv2_imshow(actual_image) # This work the same as passing an image\n",
        "#############################################<<< output visualization üëçüèª\n"
      ],
      "metadata": {
        "id": "oeUR-giJdwqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics['labels'].shape[0]\n",
        "images = []\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "for i in range(0,metrics['labels'].shape[0]):\n",
        "  print(i)\n",
        "  predicted_image = np.array(metrics['labels'][i], dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "  cv2img = cv2_imshow(predicted_image)\n",
        "\n",
        "#  cv2img = cv2_imshow(predicted_image) # This work the same as passing an image\n",
        "# cv2img = cv2_imshow(images)\n",
        "len(images)\n",
        "print(predicted_image)"
      ],
      "metadata": {
        "id": "kgSvOfE9w6zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GeoTiff to Image & Imahe to GeoTiff Conversion**"
      ],
      "metadata": {
        "id": "8ZkMG_PTunBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1-6Mxg3hroKxXFGBn3y6wIEHK1L6GdgwJ #downloading DEM-MAP https://drive.google.com/file/d/1-6Mxg3hroKxXFGBn3y6wIEHK1L6GdgwJ/view?usp=share_link\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "gJDjAlnav2T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os.path\n",
        "import re\n",
        "\n",
        "from osgeo import gdal\n",
        "from osgeo import gdal_array\n",
        "from osgeo import osr\n",
        "\n",
        "def get_gain_band(input_file):\n",
        "    \"\"\"get GAIN_BAND from meta file (*.tif.txt)\"\"\"\n",
        "     # define file name of *.tif.txt\n",
        "    ifile_txt = re.sub(r'.tif', '.tif.txt', input_file)\n",
        "    ld = open(ifile_txt)\n",
        "    lines = ld.readlines()\n",
        "    ld.close()\n",
        "    \n",
        "    gain_band = []\n",
        "    for line in lines:\n",
        "        if line.find(\"GAIN_BAND\") >= 0:\n",
        "             gain_band.append(float((re.split(' ', line)[1]).strip()))\n",
        "    return gain_band\n",
        "\n",
        "def tif2array(input_file, calc_gain=True):\n",
        "    \"\"\"\n",
        "    read GeoTiff and convert to numpy.ndarray.\n",
        "    Inputs:\n",
        "        input_file (str) : the name of input GeoTiff file.\n",
        "        calc_gain (bool) : wheter calc GAIN to DN  or not (defaul:True).\n",
        "    return:\n",
        "        image(np.array) : image for each bands\n",
        "        dataset : for gdal's data drive.\n",
        "    \"\"\"\n",
        "    dataset = gdal.Open(input_file, gdal.GA_ReadOnly)\n",
        "    # Allocate our array using the first band's datatype\n",
        "    image_datatype = dataset.GetRasterBand(1).DataType\n",
        "    image = np.zeros((dataset.RasterYSize, dataset.RasterXSize, dataset.RasterCount),\n",
        "                     dtype=float)\n",
        "    \n",
        "    if calc_gain == True:\n",
        "        # get gain\n",
        "        gain = get_gain_band(input_file)\n",
        "    \n",
        "    # Loop over all bands in dataset\n",
        "    for b in range(dataset.RasterCount):\n",
        "        # Remember, GDAL index is on 1, but Python is on 0 -- so we add 1 for our GDAL calls\n",
        "        band = dataset.GetRasterBand(b + 1)\n",
        "        # Read in the band's data into the third dimension of our array\n",
        "        if calc_gain == True:\n",
        "            # calc gain value for each bands\n",
        "            image[:, :, b] = band.ReadAsArray() * gain[b]\n",
        "        else:\n",
        "            image[:, :, b] = band.ReadAsArray()\n",
        "    return image, dataset\n",
        "\n",
        "def array2raster(newRasterfn, dataset, array, dtype):\n",
        "    \"\"\"\n",
        "    save GTiff file from numpy.array\n",
        "    input:\n",
        "        newRasterfn: save file name\n",
        "        dataset : original tif file\n",
        "        array : numpy.array\n",
        "        dtype: Byte or Float32.\n",
        "    \"\"\"\n",
        "    cols = array.shape[1]\n",
        "    rows = array.shape[0]\n",
        "    originX, pixelWidth, b, originY, d, pixelHeight = dataset.GetGeoTransform() \n",
        "\n",
        "    driver = gdal.GetDriverByName('GTiff')\n",
        "\n",
        "    # set data type to save.\n",
        "    GDT_dtype = gdal.GDT_Unknown\n",
        "    if dtype == \"Byte\": \n",
        "        GDT_dtype = gdal.GDT_Byte\n",
        "    elif dtype == \"Float32\":\n",
        "        GDT_dtype = gdal.GDT_Float32\n",
        "    else:\n",
        "        print(\"Not supported data type.\")\n",
        "\n",
        "    # set number of band.\n",
        "    if array.ndim == 2:\n",
        "        band_num = 1\n",
        "    else:\n",
        "        band_num = array.shape[2]\n",
        "\n",
        "    outRaster = driver.Create(newRasterfn, cols, rows, band_num, GDT_dtype)\n",
        "    outRaster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n",
        "\n",
        "    # # Loop over all bands.\n",
        "    # for b in range(band_num):\n",
        "    #     outband = outRaster.GetRasterBand(b + 1)\n",
        "    #     # Read in the band's data into the third dimension of our array\n",
        "    #     if band_num == 1:\n",
        "    #         outband.WriteArray(array)\n",
        "    #     else:\n",
        "    #         outband.WriteArray(array[:,:,b])\n",
        "    outband = outRaster.GetRasterBand(1)\n",
        "    outband.WriteArray(array.reshape(rows, cols))\n",
        "    # setting srs from input tif file.\n",
        "    prj=dataset.GetProjection()\n",
        "    outRasterSRS = osr.SpatialReference(wkt=prj)\n",
        "    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
        "    outband.FlushCache()\n",
        "    return newRasterfn"
      ],
      "metadata": {
        "id": "gnDwbyWAi4dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT.ndim\n",
        "INPUT.reshape(INPUT.shape[0],INPUT.shape[1]).shape"
      ],
      "metadata": {
        "id": "PyHqZZoCOccL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_PATH = \"/content/MOD09A1.061_sur_refl_b01_doy2000065_aid0001.tif\"\n",
        "INPUT,Datasetq = tif2array(INPUT_PATH,0) #<<<<<<<<<<<<<<<<<< input\n",
        "type(INPUT)\n",
        "print(INPUT,Datasetq)\n",
        "len(INPUT)\n",
        "INPUT[0].shape\n",
        "dataset = gdal.Open(INPUT_PATH, gdal.GA_ReadOnly)\n",
        "originX, pixelWidth, b, originY, d, pixelHeight = dataset.GetGeoTransform() \n",
        "print(originX, pixelWidth, b, originY, d, pixelHeight)\n",
        "print(INPUT.shape)\n",
        "OUTPUT = array2raster(\"geo1.tif\", Datasetq, INPUT, \"Float32\") #<<<<<<<<<<<<< output\n",
        "\n",
        "type(OUTPUT)\n",
        "OUTPUT"
      ],
      "metadata": {
        "id": "R0BbR6CHwPxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(INPUT.shape)\n",
        "INPUT[0][:,:].shape\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2img = cv2_imshow(OUTPUT)\n"
      ],
      "metadata": {
        "id": "BSAC4bI5C_7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# image"
      ],
      "metadata": {
        "id": "HZOFzoYMOpTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsize = (260, 260) # /.... 233 * 454\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "def batchedimages(image_locations):\n",
        "  ddyss = jnp.asarray((tif2array((total_images_path[image_locations[0]]),0)[1],\n",
        "                      imageRGB(total_images_path[image_locations[1]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[2]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[3]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[4]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[5]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[6]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[7]])[1]))\n",
        "  ddxss = jnp.asarray((imageGRAY(total_images_path[image_locations[0]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[1]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[2]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[3]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[4]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[5]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[6]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[7]])[1]))\n",
        "  #print(ddyss.shape,\"<<<< ddyss.shape???\",ddxss.shape,\"<<<< ddxss.shape???\") #to check shape \n",
        "  batch_ccc = ddyss, ddxss\n",
        "  return batch_ccc\n",
        "  \n",
        "def data_stream():\n",
        "  key = random.PRNGKey(0)\n",
        "  perm = random.permutation(key, len(total_images_path))\n",
        "  for i in range(no_of_batches):\n",
        "    batch_idx = perm[i * batch_size : (i + 1) * batch_size]; #print(batch_idx)\n",
        "    yield batchedimages(batch_idx)\n",
        "\n",
        "batches = data_stream() \n",
        "print(next(batches)[0].shape,next(batches)[1].shape)\n"
      ],
      "metadata": {
        "id": "_hc_sz4COoTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Yub1ZwEDPniA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **setting up google drive images connection**"
      ],
      "metadata": {
        "id": "p1wwSpndwd35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text { vertical-output: true }\n",
        "\n",
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b1 = prefix+bandend[0]\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b3 = prefix+bandend[2]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b5 = prefix+bandend[4]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "expression_b7 = prefix+bandend[6]\n",
        "\n",
        "temp_dir = r'/content/'\n",
        "import os\n",
        "\n",
        "imgs_list_b2 = [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)] # since the string path item date is same, we just need to change the band number in the string to get other bands, ie. we are doing it by storing image_paths <<< then image_path_b2[i].replace(expression_b2, expression_b4) << to get the b4 band , silimarly bands b1 to b7 image paths can be generated assuming all bands present per image ‚úÖ\n",
        "imgs_list_b2.sort(reverse=True)                     #<<<< to start file streaming from the last date 2022 >> 2021 >> 2020 ....\n",
        "\n",
        "imgs_path_b2 = [os.path.join(image_dir, i) for i in imgs_list_b2 if i != 'outputs']\n",
        "\n",
        "print(len(imgs_path_b2))\n",
        "print(imgs_path_b2[5])\n",
        "\n",
        "print(imgs_path_b2[5].replace(expression_b2, expression_b4))\n",
        "\n",
        "batch_size = 8\n",
        "no_of_batches = int(len(imgs_list_b2)/batch_size)\n"
      ],
      "metadata": {
        "id": "uRjgnXp5Plcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(imgs_list_b2)\n",
        "len(imgs_list_b2)"
      ],
      "metadata": {
        "id": "eoTjSXSgTa_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_list_b2[1]\n",
        "os.path.join(image_dir, imgs_list_b2[1])"
      ],
      "metadata": {
        "id": "KbxeSMCWXWiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"b01\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.tif'\n",
        "expression_b1 = prefix+bandend[0]\n",
        "expression_b2 = prefix+bandend[1]\n",
        "expression_b3 = prefix+bandend[2]\n",
        "expression_b4 = prefix+bandend[3]\n",
        "expression_b5 = prefix+bandend[4]\n",
        "expression_b6 = prefix+bandend[5]\n",
        "expression_b7 = prefix+bandend[6]"
      ],
      "metadata": {
        "id": "mkrODo-WZ4ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bandend)\n",
        "for i in bandend:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "EMepQIw1avUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "merging bands into 1 file ie (width, height, 7 bands) <<< file"
      ],
      "metadata": {
        "id": "-jHSpnziw48I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = r'/content/drive/MyDrive/OUT/data/MOD09A1061/files/'\n",
        "path = os.path.join(image_dir, imgs_list_b2[1])\n",
        "v1 = tif2array(path.replace(expression_b2, expression_b1),0)[0]\n",
        "v2 = jax.numpy.append(v1, tif2array(path.replace(expression_b2, expression_b2),0)[0] , axis =2)\n",
        "v3 = jax.numpy.append(v2, tif2array(path.replace(expression_b2, expression_b3),0)[0] , axis =2)\n",
        "v4 = jax.numpy.append(v3, tif2array(path.replace(expression_b2, expression_b4),0)[0] , axis =2)\n",
        "v5 = jax.numpy.append(v4, tif2array(path.replace(expression_b2, expression_b5),0)[0] , axis =2)\n",
        "v6 = jax.numpy.append(v5, tif2array(path.replace(expression_b2, expression_b6),0)[0] , axis =2)\n",
        "v7 = jax.numpy.append(v6, tif2array(path.replace(expression_b2, expression_b7),0)[0] , axis =2)\n",
        "print(v1.shape,v2.shape,v7.shape)\n",
        "type(v2)"
      ],
      "metadata": {
        "id": "Y-dPea1ZXC88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v8 = v7.reshape(v7.shape[0]*v7.shape[1], v7.shape[2]).shape\n",
        "v8"
      ],
      "metadata": {
        "id": "t8OD26_Le-AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np     \n",
        "predicted_image = np.array(v7[:,:,2],  dtype=np.float32) # This would be your image array\n",
        "cv2img = cv2_imshow(predicted_image) # This work the same as passing an image"
      ],
      "metadata": {
        "id": "WxePQFzrdvC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clipping into parts "
      ],
      "metadata": {
        "id": "99-i4UgexIdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from osgeo import gdal\n",
        "\n",
        "dset = gdal.Open(path)\n",
        "\n",
        "width = dset.RasterXSize\n",
        "height = dset.RasterYSize\n",
        "\n",
        "print(width, 'x', height)\n",
        "\n",
        "tilesize = 200\n",
        "\n",
        "for i in range(0, width, tilesize):\n",
        "    for j in range(0, height, tilesize):\n",
        "        w = min(i+tilesize, width) - i\n",
        "        h = min(j+tilesize, height) - j\n",
        "        gdaltranString = \"gdal_translate -of GTIFF -srcwin \"+str(i)+\", \"+str(j)+\", \"+str(w)+\", \" \\\n",
        "            +str(h)+\" \" + path + \" \" + \"/content/\" + \"_\"+str(i)+\"_\"+str(j)+\".tif\"\n",
        "        os.system(gdaltranString)"
      ],
      "metadata": {
        "id": "wNKSkAmmkFuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = tif2array(\"/content/_200_200.tif\",0)[0]\n",
        "predicted_image = np.array(c1,  dtype=np.float32) # This would be your image array\n",
        "cv2img = cv2_imshow(predicted_image)"
      ],
      "metadata": {
        "id": "F_849rgIlAcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing  clip centre location >> "
      ],
      "metadata": {
        "id": "qKqd3l2HeA0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape =[454, 233]\n",
        "clip_s = [200, 200]\n",
        "list = []\n",
        "col = int(shape[0]/clip_s[0])+1\n",
        "row = int(shape[1]/clip_s[1])+1\n",
        "in_col = int(((shape[0]-clip_s[0]/2)-(clip_s[0]/2))/(col -1))+1\n",
        "in_row = int(((shape[1]-clip_s[1]/2)-(clip_s[1]/2))/(col -1))+1\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "f = plt.figure()\n",
        "f.set_figwidth(shape[0]/50)\n",
        "f.set_figheight(shape[1]/50)\n",
        "plt.plot(x, y)\n",
        "import matplotlib.patches as patches\n",
        "ax = plt.gca()\n",
        "for i in range(0, col):\n",
        "  for j in range(0, row):\n",
        "    if(i==0):\n",
        "      x.append(int(clip_s[0]/2 ))\n",
        "      y.append(int(j*in_row + clip_s[1]/2 ))\n",
        "    if(i==col-1):\n",
        "      x.append(int(i*in_col + clip_s[0]/2 ))\n",
        "    if(j==0):\n",
        "      y.append(int(j*in_row + clip_s[1]/2 ))\n",
        "    if(j==row-1):\n",
        "      y.append(int(j*in_row + clip_s[1]/2 ))\n",
        "    else:\n",
        "      x.append(int(i*in_col + clip_s[0]/2 ))\n",
        "      y.append(int(j*in_row + clip_s[1]/2 ))\n",
        "    print(x, y)\n",
        "    rect = patches.Rectangle((x[i]-clip_s[0]/2, y[j]-clip_s[1]/2), clip_s[0], clip_s[1], linewidth=1, edgecolor='r', facecolor='none')\n",
        "    print((x[i]-clip_s[0]/2, y[j]-clip_s[1]/2))\n",
        "    ax.add_patch(rect)\n",
        "plt.plot(x, y)\n",
        "\n",
        "# Create a Rectangle patch\n",
        "\n",
        "\n",
        "# Add the patch to the Axes\n",
        "# plt.grid()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "54_HApDicgDV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}